

# 注意力机制

按照顺序看参考地址：

[参考地址1](https://zhuanlan.zhihu.com/p/67909876)

[参考地址2](https://zhuanlan.zhihu.com/p/53682800)

## 一、Attention机制剖析

### 1、为什么要引入Attention机制？

根据通用近似定理，前馈网络和循环网络都有很强的能力。但为什么还要引入注意力机制呢？

- **计算能力的限制**：当要记住很多“信息“，模型就要变得更复杂，然而目前计算能力依然是限制神经网络发展的瓶颈。
- **优化算法的限制**：虽然局部连接、权重共享以及pooling等优化操作可以让神经网络变得简单一些，有效缓解模型复杂度和表达能力之间的矛盾；但是，如循环神经网络中的长距离以来问题，信息“记忆”能力并不高。

### 2、Attention机制有哪些？（怎么分类？）

当用神经网络来处理大量的输入信息时，也可以借鉴人脑的注意力机制，只 选择一些关键的信息输入进行处理，来提高神经网络的效率。按照认知神经学中的注意力，可以总体上分为两类：

- **聚焦式（focus）注意力**：自上而下的有意识的注意力，主动注意——是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力；
- **显著性（saliency-based）注意力**：自下而上的有意识的注意力，被动注意——基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关；可以将**max-pooling和门控（gating）机制**来近似地看作是自下而上的基于显著性的注意力机制。

在人工神经网络中，注意力机制一般就特指聚焦式注意力。

### 3、Attention机制的计算流程是怎样的？

这个看参考地址1和参考地址2互相印证。

**Attention机制的实质其实就是一个寻址（addressing）的过程**，如上图所示：给定一个和任务相关的查询**Query**向量 **q**，通过计算与**Key**的注意力分布并附加在**Value**上，从而计算**Attention Value**，这个过程实际上是**Attention机制缓解神经网络模型复杂度的体现**：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。

![preview](https://pic1.zhimg.com/v2-54fe529ded98721f35277a5bfa79febc_r.jpg)

> ***注意力机制可以分为三步：一是信息输入；二是计算注意力分布α；三是根据注意力分布α 来计算输入信息的加权平均。\***

**step1-信息输入**：用**X** = [x1, · · · , xN ]表示N 个输入信息；（这个step-1的信息输入就是参考二中的输入的隐藏层状态 $\bar{h}$）$q$ 即为$\bar{h}$

**step2-注意力分布计算**：令**Key**=**Value**=**X**，则可以给出注意力分布(step2在参考一中并非计算得出value，step2则是计算$h_{t}$ 与$\bar{h}_{i}$的相关性得出value)

![[公式]](https://raw.githubusercontent.com/jiutiananshu/Picture/master/img/softmax.png)

我们将 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_i) 称之为注意力分布（概率分布）， ![[公式]](https://www.zhihu.com/equation?tex=s(X_i%2Cq)) 为注意力打分机制，有几种打分机制：

![img](https://pic3.zhimg.com/80/v2-981a0c9ab01531c7139e4701574cb056_720w.jpg)

**step3-信息加权平均**：注意力分布 ![[公式]](https://www.zhihu.com/equation?tex=\alpha_i) 可以解释为在上下文查询**q**时，第i个信息受关注的程度，采用一种“软性”的信息选择机制对输入信息**X**进行编码为：

![[公式]](https://www.zhihu.com/equation?tex=att(q%2CX)%3D\sum_{i%3D1}^{N}{\alpha_iX_i})

这种编码方式为**软性注意力机制（soft Attention）**，软性注意力机制有两种：普通模式（**Key**=**Value**=**X**）和键值对模式（**Key！**=**Value**）。

![img](https://pic3.zhimg.com/80/v2-aa371755dc73b7137149b8d2905fc4ba_720w.jpg)

### 4、Attention机制的变种有哪些？

与普通的Attention机制（上图左）相比，Attention机制有哪些变种呢？

- **变种1-硬性注意力：**之前提到的注意力是软性注意力，其选择的信息是所有输入信息在注意力 分布下的期望。还有一种注意力是只关注到某一个位置上的信息，叫做硬性注意力（hard attention）。硬性注意力有两种实现方式：（1）一种是选取最高概率的输入信息；（2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。硬性注意力模型的缺点：

> 硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习来进行训练。——[《神经网络与深度学习》](https://link.zhihu.com/?target=https%3A//nndl.github.io/)

- **变种2-键值对注意力：**即上图右边的键值对模式，此时Key！=Value，注意力函数变为：

![img](https://pic3.zhimg.com/80/v2-49a8acda3757ea21218b2f7ecca6e9ae_720w.jpg)

- **变种3-多头注意力：**多头注意力（multi-head attention）是利用多个查询Q = [q1, · · · , qM]，来平行地计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分，然后再进行拼接：

![img](https://pic4.zhimg.com/80/v2-27673fff36241d6ef163c9ac1cedcce7_720w.png)

### 5、一种强大的Attention机制：为什么自注意力模型（self-Attention model）在长距离序列中如此强大？

**（1）卷积或循环神经网络难道不能处理长距离序列吗？**

当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如图所示：

![img](https://pic3.zhimg.com/80/v2-8b369281a66bea6920962f45660a9f0a_720w.jpg)

从上图可以看出，无论卷积还是循环神经网络其实都是对变长序列的一种“**局部编码**”：卷积神经网络显然是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。

**（2）要解决这种短距离依赖的“局部编码”问题，从而对输入序列建立长距离依赖关系，有哪些办法呢？**

> 如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一 种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互，另一种方法是使用全连接网络。 ——[《神经网络与深度学习》](https://link.zhihu.com/?target=https%3A//nndl.github.io/)

![img](https://pic1.zhimg.com/80/v2-cd2d7f0961c669d983b73db4e93ccbdc_720w.jpg)

由上图可以看出，全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。

这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（self-attention model）。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。

总体来说，**为什么自注意力模型（self-Attention model）如此强大**：**利用注意力机制来“动态”地生成不同连接的权重，从而处理变长的信息序列。**

**（3）自注意力模型（self-Attention model）具体的计算流程是怎样的呢?**

同样，给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到为查询向量序列，键向量序列和值向量序列：

![img](https://pic1.zhimg.com/80/v2-ab400406cf423842e4274527dc5a7074_720w.png)

上面的公式可以看出，**self-Attention中的Q是对自身（self）输入的变换，而在传统的Attention中，Q来自于外部。**

![img](https://pic4.zhimg.com/80/v2-fcc2df696966a9c6700d1476690cff9f_720w.jpg)

注意力计算公式为：

![img](https://pic2.zhimg.com/80/v2-72093f153e59cfdc851e2ac1fbf5c03d_720w.jpg)

自注意力模型（self-Attention model）中，通常使用缩放点积来作为注意力打分函数，输出向量序列可以写为：

![img](https://pic2.zhimg.com/80/v2-2f76af60c24ba75e37f2f5df8edfdb71_720w.jpg)

[参考地址3](https://zhuanlan.zhihu.com/p/265108616)

参考地址3对上图的详细分析。

## 二、Transformer（Attention Is All You Need）详解

**1、Transformer的整体架构是怎样的？由哪些部分组成？**

<img src="https://pic1.zhimg.com/80/v2-7f8b460cd617fedc822064c4230302b0_720w.jpg" alt="Transformer模型架构"  />

Transformer其实这就是一个Seq2Seq模型，左边一个encoder把输入读进去，右边一个decoder得到输出：

![img](https://pic4.zhimg.com/80/v2-846cf91009c44c6e479bada42bfc437f_720w.jpg)

**Transformer=Transformer Encoder+Transformer Decoder**

**（1）Transformer Encoder（N=6层，每层包括2个sub-layers）：**

![img](https://pic3.zhimg.com/80/v2-3b97d37951078856097069778293230a_720w.jpg)

- **sub-layer-1**：**multi-head self-attention mechanism**，用来进行self-attention。
- **sub-layer-2**：**Position-wise Feed-forward Networks**，简单的全连接网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出（输入输出层的维度都为512，中间层为2048）：

![img](https://pic3.zhimg.com/80/v2-5236351e3efd93d567ac1fceea7716ee_720w.png)

每个sub-layer都使用了残差网络： ![[公式]](https://www.zhihu.com/equation?tex=LayerNorm%28X%2Bsublayer%28X%29%29)

**（2）Transformer Decoder（N=6层，每层包括3个sub-layers）：**

![img](https://pic1.zhimg.com/80/v2-4dc71fe78c4752645de1f1ba8dd762a4_720w.jpg)Transformer Decoder

- **sub-layer-1**：**Masked multi-head self-attention mechanism**，用来进行self-attention，与Encoder不同：由于是序列生成过程，所以在时刻 i 的时候，大于 i 的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做**Mask**。
- **sub-layer-2**：**Position-wise Feed-forward Networks**，同Encoder。
- **sub-layer-3**：**Encoder-Decoder attention计算**。

**2、Transformer Encoder 与 Transformer Decoder 有哪些不同？**

（1）multi-head self-attention mechanism不同，Encoder中不需要使用Masked，而Decoder中需要使用Masked；

（2）Decoder中多了一层Encoder-Decoder attention，这与 self-attention mechanism不同。

**3、Encoder-Decoder attention 与self-attention mechanism有哪些不同？**

它们都是用了 multi-head计算，不过Encoder-Decoder attention采用传统的attention机制，其中的Query是self-attention mechanism已经计算出的上一时间i处的编码值，Key和Value都是Encoder的输出，这与self-attention mechanism不同。代码中具体体现：

```text
 ## Multihead Attention ( self-attention)
            self.dec = multihead_attention(queries=self.dec,
                                           keys=self.dec,
                                           num_units=hp.hidden_units,
                                           num_heads=hp.num_heads,
                                           dropout_rate=hp.dropout_rate,
                                           is_training=is_training,
                                           causality=True,
                                           scope="self_attention")

## Multihead Attention ( Encoder-Decoder attention)
            self.dec = multihead_attention(queries=self.dec,
                                           keys=self.enc,
                                           num_units=hp.hidden_units,
                                           num_heads=hp.num_heads,
                                           dropout_rate=hp.dropout_rate,
                                           is_training=is_training,
                                           causality=False,
                                           scope="vanilla_attention")
```

**4、multi-head self-attention mechanism具体的计算过程是怎样的？**

![img](https://pic3.zhimg.com/80/v2-392692c19c57f5bfa116f7b505dfde7a_720w.jpg)multi-head self-attention mechanism计算过程

Transformer中的Attention机制由**Scaled Dot-Product Attention**和**Multi-Head Attention**组成，上图给出了整体流程。下面具体介绍各个环节：

- **Expand**：实际上是经过线性变换，生成Q、K、V三个向量；
- **Split heads**: 进行分头操作，在原文中将原来每个位置512维度分成8个head，每个head维度变为64；
- **Self Attention**：对每个head进行Self Attention，具体过程和第一部分介绍的一致；
- **Concat heads**：对进行完Self Attention每个head进行拼接；

上述过程公式为：

![img](https://pic3.zhimg.com/80/v2-c7100e268bcefaa7ff0a1344acc15e7e_720w.jpg)

**5、Transformer在GPT和Bert等词向量预训练模型中具体是怎么应用的？有什么变化？**

- GPT中训练的是单向语言模型，其实就是直接应用**Transformer Decoder**；
- Bert中训练的是双向语言模型，应用了**Transformer Encoder**部分，不过在Encoder基础上还做了**Masked操作**；

BERT Transformer 使用双向self-attention，而GPT Transformer 使用受限制的self-attention，其中每个token只能处理其左侧的上下文。双向 Transformer 通常被称为“Transformer encoder”，而左侧上下文被称为“Transformer decoder”，decoder是不能获要预测的信息的。

[参考地址4](https://zhuanlan.zhihu.com/p/48508221)

参考地址4详细解释了transformer

## 三、transformer分割解释

[参考地址5](https://github.com/aespresso/a_journey_into_math_of_ml)

transformer模型主要分为**两大部分**, 分别是**编码器**和**解码器**, **编码器**负责把自然语言序列映射成为**隐藏层**(下图中**第2步**用九宫格比喻的部分), 含有自然语言序列的数学表达. 然后解码器把隐藏层再映射为自然语言序列, 从而使我们可以解决各种问题, 如情感分类, 命名实体识别, 语义关系抽取, 摘要生成, 机器翻译等等, 下面我们简单说一下下图的每一步都做了什么:

1. 输入自然语言序列到编码器: Why do we work?(为什么要工作);
2. 编码器输出的隐藏层, 再输入到解码器;
3. 输入$start$(起始)符号到解码器;
4. 得到第一个字"为";
5. 将得到的第一个字"为"落下来再输入到解码器;
6. 得到第二个字"什";
7. 将得到的第二字再落下来, 直到解码器输出$end$(终止符), 即序列生成完成.

![模型分解](https://raw.githubusercontent.com/jiutiananshu/Picture/master/img/intuition.jpg)

**编码器**部分, 即把**自然语言序列映射为隐藏层的数学表达的过程**。

![详解编码器](https://raw.githubusercontent.com/jiutiananshu/Picture/master/img/encoder.jpg)

**Transformer Block结构图, 注意: 为方便查看, 下面的内容分别对应着上图第1, 2, 3, 4个方框的序号:**

### 1. positional encoding, 即**位置嵌入**(或位置编码);

由于transformer模型**没有**循环神经网络的迭代操作, 所以我们必须提供每个字的位置信息给transformer, 才能识别出语言中的顺序关系.
		现在定义一个位置嵌入的概念, 也就是positional encoding, 位置嵌入的维度为[max sequence length, embedding dimension], 嵌入的维度同词向量的维度, max sequence length属于超参数, 指的是限定的**最大单个句长**.
注意, 我们一般以字为单位训练transformer模型, 也就是说我们不用分词了, 首先我们要初始化字向量为[vocab size, embedding dimension], vocab size为总共的字库数量, embedding dimension为字向量的维度, 也是每个字的数学表达.
在这里论文中使用了sine和cosine函数的线性变换来提供给模型位置信息:

$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}}) \quad PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})\tag{eq.1}$$

 上式中$pos$指的是句中字的位置, 取值范围是[0, max sequence length],  $i $指的是词向量的维度, 取值范围是[0, embedding dimension]), 上面有$sin$和$cos$一组公式, 也就是对应着embedding dimension维度的一组奇数和偶数的序号的维度, 例如0,10,1一组, 2,32,3一组, 分别用上面的$sin$和$cos$函数做处理, 从而产生不同的周期性变化, 而位置嵌入在embedding dimension维度上随着维度序号增大, 周期变化会越来越慢, 而产生一种包含位置信息的纹理, 就像论文原文中第六页讲的, 位置嵌入函数的周期从2π到0000∗2π变化, 而每一个位置在embedding dimension维度上都会得到不同周期的$sin$和$cos$函数的取值组合, 从而产生独一的纹理位置信息, 模型从而学到位置之间的依赖关系和自然语言的时序特性.
下面画一下位置嵌入, 可见纵向观察, 随着embedding dimension增大, 位置嵌入函数呈现不同的周期变化.

```
def get_positional_encoding(max_seq_len, embed_dim):
    # 初始化一个positional encoding
    # embed_dim: 字嵌入的维度
    # max_seq_len: 最大的序列长度
    positional_encoding = np.array([
        [pos / np.power(10000, 2 * i / embed_dim) for i in range(embed_dim)]
        if pos != 0 else np.zeros(embed_dim) for pos in range(max_seq_len)])
    positional_encoding[1:, 0::2] = np.sin(positional_encoding[1:, 0::2])  # dim 2i 偶数
    positional_encoding[1:, 1::2] = np.cos(positional_encoding[1:, 1::2])  # dim 2i+1 奇数
    # 归一化, 用位置嵌入的每一行除以它的模长
    # denominator = np.sqrt(np.sum(position_enc**2, axis=1, keepdims=True))
    # position_enc = position_enc / (denominator + 1e-8)
    return positional_encoding
positional_encoding = get_positional_encoding(max_seq_len=100, embed_dim=128)
```

### 2. self attention mechanism, **自注意力机制**;

![img](https://gitee.com/enisya66/a_journey_into_math_of_ml/raw/master/03_transformer_tutorial_1st_part/imgs/attention_0.jpg)

这一块的代码可以[参考](https://gitee.com/rover912/a_journey_into_math_of_ml/blob/master/04_transformer_tutorial_2nd_part/BERT_tutorial/models/bert_model.py)

查看X[batch, sequence length]通过[embedding lokeup](https://zhuanlan.zhihu.com/p/341176854)

注意, 在上面self attentionself attention的计算过程中, 我们通常使用mini batchmini batch来计算, 也就是一次计算多句话, 也就是XX的维度是[batch size, sequence length][batch size, sequence length], sequence lengthsequence length是句长, 而一个mini batchmini batch是由多个不等长的句子组成的, 我们就需要按照这个mini batchmini batch中最大的句长对剩余的句子进行补齐长度, 我们一般用00来进行填充, 这个过程叫做paddingpadding.
但这时在进行softmaxsoftmax的时候就会产生问题, 回顾softmaxsoftmax函数$\sigma (\mathbf {z} )*{i}={\frac {e^{z*{i}}}{\sum *{j=1}^{K}e^{z*{j}}}}$,$e^0$是1,是有值的,这样的话是1,是有值的,这样的话softmax中被中被padding的部分就参与了运算,就等于是让无效的部分参与了运算,会产生很大隐患,这时就需要做一个的部分就参与了运算,就等于是让无效的部分参与了运算,会产生很大隐患,这时就需要做一个mask让这些无效区域不参与运算,我们一般给无效区域加一个很大的负数的偏置,也就是:让这些无效区域不参与运算,我们一般给无效区域加一个很大的负数的偏置,也就是:$z_{illegal}=z_{illegal}+bias_{illegal} \ \ b_{illegal}→−∞ \  \ e^{z_{illegal}}→0 $ 经过上式的masking我们使无效区域经过softmax计算之后还几乎为计算之后还几乎为0, 这样就避免了无效区域参与计算.

### 3. Layer Normalization和残差连接.

1). **残差连接**:防止梯度衰退

我们在上一步得到了经过注意力矩阵加权之后的VV, 也就是Attention(Q, K, V)Attention(Q, K, V), 我们对它进行一下转置, 使其和XembeddingXembedding的维度一致, 也就是[batch size, sequence length, embedding dimension][batch size, sequence length, embedding dimension], 然后把他们加起来做残差连接, 直接进行元素相加, 因为他们的维度一致:

$X_{embedding}+Attention(Q, K, V)$

在之后的运算里, 每经过一个模块的运算, 都要把运算之前的值和运算之后的值相加, 从而得到残差连接, 训练的时候可以使梯度直接走捷径反传到最初始层:

$X+SubLayer(X)$

2). LayerNorm:[讲解](https://zhuanlan.zhihu.com/p/33173246)，LN 针对单个训练样本进行，不依赖于其他数据。

$$\mu_{i}=\frac{1}{m} \sum^{m}*{i=1}x*{ij}$$

求行的均值，

 $$\sigma^{2}*{j}=\frac{1}{m} \sum^{m}*{i=1} (x_{ij}-\mu_{j})^{2}$$

行的方差。

$$LayerNorm(x)=\alpha \odot \frac{x_{ij}-\mu_{i}} {\sqrt{\sigma^{2}_{i}+\epsilon}} + \beta \tag{eq.6}$$ 

然后用**每一行**的**每一个元素**减去**这行的均值**, 再除以**这行的标准差**, 从而得到归一化后的数值, ϵ是为了防止除0;
之后引入两个可训练参数α,, β来弥补归一化的过程中损失掉的信息, 注意⊙表示元素相乘而不是点积, 我们一般初始化α为全1, 而β为全0.

## 4. transformer encoder整体结构.

经过上面3个步骤, 我们已经基本了解到来transformer编码器的主要构成部分, 我们下面用公式把一个transformer  block的计算过程整理一下:
1). 字向量与位置编码:
$$
X= EmbeddingLookup (X)+ PositionalEncoding  + \beta \tag{eq.2}\\ 

X \in \mathbb{R}^{\text {batch size } * \text { seq. len. } * \text { embed. dim }}
$$
2). 自注意力机制
$$
Q=Linear(X)=X W_{Q} + \beta \tag {eq.3}
$$

$$
K=Linear(X)=X W_{K} + \beta \tag {eq.4}
$$

$$
V=Linear(X)=X W_{V} + \beta \tag {eq.5}
$$

$$
 X_{attention }= { SelfAttention }(Q, K, V) + \beta \tag{eq.6}
$$

3). 残差连接与Layer Normalization
$$
X_{attention }=X+X_{attention } + \beta \tag{eq.7}
$$

$$
X_{ attention }= LayerNor(X_{attention }) \beta \tag{eq.8}
$$

4). 下面进行transformer block结构图中的**第4部分**, 也就是FeedForward, 其实就是两层线性映射并用激活函数激活, 比如说ReLU:
$$
X_{hidden} = Activete(Linear(Linear(X_{attention})))   
$$
5). 重复3):
$$
X_{\text {hidden }}=X_{\text {attention }}+X_{\text {hidden }} \\
X_{\text {hidden }}= LayerNorm \left(X_{\text {hidden }}\right) \\
X_{\text {hidden }} \in \mathbb{R}^{\text {batch size } * \text { seq. len. } * \text { embed. } d i m}
$$


**小结:**

我们到现在位置已经讲完了transformer的编码器的部分, 了解到了transformer是怎样获得自然语言的位置信息的, 注意力机制是怎样的, 其实举个语言情感分类的例子, 我们已经知道, 经过自注意力机制, 一句话中的每个字都含有这句话中其他所有字的信息, 那么我们可不可以添加一个空白字符到句子最前面, 然后让句子中的所有信息向这个空白字符汇总, 然后再映射成想要分的类别呢? 
	在**BERT**的预训练中, 我们给每句话的句头加一个特殊字符, 然后句末再加一个特殊字符, 之后模型预训练完毕之后, 我们就可以用句头的特殊字符的hidden state完成一些分类任务了.



## 四、BERT模型

[参考地址](https://github.com/aespresso/a_journey_into_math_of_ml/blob/master/04_transformer_tutorial_2nd_part/BERT_tutorial/transformer_2_tutorial.ipynb)

### 1.初始化bert模型的参数

```
class BertConfig(object):
    """Configuration class to store the configuration of a `BertModel`.
    """
    def __init__(self,
                 vocab_size, # 字典字数
                 hidden_size=384, # 隐藏层维度也就是字向量维度-embedding dimension
                 num_hidden_layers=6, # transformer block 的个数
                 num_attention_heads=12, # 注意力机制"头"的个数
                 intermediate_size=384*4, # feedforward层线性映射的维度
                 hidden_act="gelu", # 激活函数
                 hidden_dropout_prob=0.4, # dropout的概率  
                 # 丢弃部分数据防止过拟合   https://blog.csdn.net/junbaba_/article/details/105673998
                 attention_probs_dropout_prob=0.4,
                 max_position_embeddings=512*2,
                 type_vocab_size=256, # 用来做next sentence预测,
                 # 这里预留了256个分类, 其实我们目前用到的只有0和1
                 initializer_range=0.02 # 用来初始化模型参数的标准差
                 ):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.hidden_act = hidden_act
        self.intermediate_size = intermediate_size
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.max_position_embeddings = max_position_embeddings
        self.type_vocab_size = type_vocab_size
        self.initializer_range = initializer_range
```

### 2. 设置 embedding 函数

```
class BertEmbeddings(nn.Module):
    """LayerNorm层, 见Transformer(一), 讲编码器(encoder)的第1部分"""
    """Construct the embeddings from word, position and token_type embeddings.
    """
    def __init__(self, config):
        super(BertEmbeddings, self).__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        # embedding矩阵初始化
        nn.init.orthogonal_(self.word_embeddings.weight)
        nn.init.orthogonal_(self.token_type_embeddings.weight)

        # embedding矩阵进行归一化
        epsilon = 1e-8
        self.word_embeddings.weight.data = \
            self.word_embeddings.weight.data.div(torch.norm(self.word_embeddings.weight, p=2, dim=1, keepdim=True).data + epsilon)
        self.token_type_embeddings.weight.data = \
            self.token_type_embeddings.weight.data.div(torch.norm(self.token_type_embeddings.weight, p=2, dim=1, keepdim=True).data + epsilon)

        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
        # any TensorFlow checkpoint file
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, positional_enc, token_type_ids=None):
        """
        :param input_ids: 维度 [batch_size, sequence_length]
        :param positional_enc: 位置编码 [sequence_length, embedding_dimension]
        :param token_type_ids: BERT训练的时候, 第一句是0, 第二句是1
        :return: 维度 [batch_size, sequence_length, embedding_dimension]
        """
        # 字向量查表
        words_embeddings = self.word_embeddings(input_ids)

        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)

        embeddings = words_embeddings + positional_enc + token_type_embeddings
        # embeddings: [batch_size, sequence_length, embedding_dimension]
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

```

### 3. 设置Layer Normalization和残差连接

```
class BertLayerNorm(nn.Module):
    """LayerNorm层, 见Transformer(一), 讲编码器(encoder)的第3部分"""
    def __init__(self, hidden_size, eps=1e-12):
        """Construct a layernorm module in the TF style (epsilon inside the square root).
        """
        super(BertLayerNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.variance_epsilon = eps
        
	def forward(self, x):
        u = x.mean(-1, keepdim=True)
        s = (x - u).pow(2).mean(-1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.variance_epsilon)
        return self.weight * x + self.bias

```

  ### 4. 用layer nor 和残差连接处理selfAttention的输出

```
class BertSelfOutput(nn.Module):
    # 封装的LayerNorm和残差连接, 用于处理SelfAttention的输出
    def __init__(self, config):
        super(BertSelfOutput, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states

```

### 5. 封装多头注意力机制

```
class BertAttention(nn.Module):
    # 封装的多头注意力机制部分, 包括LayerNorm和残差连接
    def __init__(self, config):
        super(BertAttention, self).__init__()
        self.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)

    def forward(self, input_tensor, attention_mask, get_attention_matrices=False):
        self_output, attention_matrices = self.self(input_tensor, attention_mask, get_attention_matrices=get_attention_matrices)
        attention_output = self.output(self_output, input_tensor)
        return attention_output, attention_matrices
```







#### 这个提到了mask attention参与的步骤

[参考地址](https://zhuanlan.zhihu.com/p/340149804)

这个提到了mask attention参与的步骤

即：

**Step2：** ![[公式]](https://www.zhihu.com/equation?tex=Q%5ET%5Ccdot+K) 得到 Attention矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A%5Cin+R_%7BN%2CN%7D) ，此时先不急于做softmax的操作，而是先于一个 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BMask%7D%5Cin+R_%7BN%2CN%7D) 矩阵相乘，使得attention矩阵的有些位置 归0，得到Masked Attention矩阵 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BMask+Attention%7D%5Cin+R_%7BN%2CN%7D) 。 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BMask%7D%5Cin+R_%7BN%2CN%7D) 矩阵是个下三角矩阵，为什么这样设计？是因为想在计算 ![[公式]](https://www.zhihu.com/equation?tex=Z) 矩阵的某一行时，只考虑它前面token的作用。即：在计算 ![[公式]](https://www.zhihu.com/equation?tex=Z) 的第一行时，刻意地把 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BAttention%7D) 矩阵第一行的后面几个元素屏蔽掉，只考虑 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BAttention%7D_%7B0%2C0%7D) 。在产生have这个单词时，只考虑 I，不考虑之后的have a cat，即只会attend on已经产生的sequence，这个很合理，因为还没有产生出来的东西不存在，就无法做attention。