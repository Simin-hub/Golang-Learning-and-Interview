# BERT模型蒸馏完全指南

[参考地址](https://zhuanlan.zhihu.com/p/273378905)

知识蒸馏（Knowledge Distillation）的概念，旨在把一个大模型或者多个模型ensemble学到的知识迁移到另一个轻量级单模型上，方便部署。简单的说就是用小模型去学习大模型的预测结果，而不是直接学习训练集中的label。



# FastBERT：又快又稳的推理提速方法

