# NLP方法总结——预处理以及词表示（词向量）

[TOC]

## 一、分词

### 各类分词方法（分词 – Tokenization）

分词就是将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作。

为什么要分词？

**1.将复杂问题转化为数学问题**

文本都是一些「非结构化数据」，我们需要先将这些数据转化为「结构化数据」，结构化数据就可以转化为数学问题了，而分词就是转化的第一步。

**2.词是一个比较合适的粒度**

词是表达完整含义的最小单位。

字的粒度太小，无法表达完整含义，比如”鼠“可以是”老鼠“，也可以是”鼠标“。

而句子的粒度太大，承载的信息量多，很难复用。比如”传统方法要分词，一个重要原因是传统方法对远距离依赖的建模能力较弱。”

**3. 深度学习时代，部分任务中也可以「分字」**

#### 中英文分词的3个典型的区别 

**区别1：分词方式不同，中文更难**

英文有天然的空格作为分隔符，但是中文没有。所以如何切分是一个难点，再加上中文里一词多意的情况非常多，导致很容易出现歧义。下文中难点部分会详细说明。

**区别2：英文单词有多种形态**

英文单词存在丰富的变形变换。为了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为词形还原（Lemmatization）和词干提取（Stemming）。中文则不需要

词性还原：does，done，doing，did 需要通过词性还原恢复成 do。

词干提取：cities，children，teeth 这些词，需要转换为 city，child，tooth”这些基本形态2+胖

**区别3：中文分词需要考虑粒度问题**	

例如「中国科学技术大学」就有很多种分法：

- 中国科学技术大学
- 中国 \ 科学技术 \ 大学
- 中国 \ 科学 \ 技术 \ 大学

粒度越大，表达的意思就越准确，但是也会导致召回比较少。所以中文需要不同的场景和要求选择不同的粒度。这个在英文中是没有的。

#### 中文分词的3大难点

**难点 1：没有统一的标准**

目前中文分词没有统一的标准，也没有公认的规范。不同的公司和组织各有各的方法和规则。

**难点 2：歧义词如何切分**

例如「兵乓球拍卖完了」就有2种分词方式表达了2种不同的含义：

- 乒乓球 \ 拍卖 \ 完了
- 乒乓 \ 球拍 \ 卖 \ 完了

**难点 3：新词的识别**

信息爆炸的时代，三天两头就会冒出来一堆新词，如何快速的识别出这些新词是一大难点。比如当年「蓝瘦香菇」大火，就需要快速识别。

#### 3种典型的分词方法

分词的方法大致分为 3 类：

1. 基于词典匹配
2. 基于统计
3. 基于深度学习

**给予词典匹配的分词方式**

优点：速度快、成本低

缺点：适应性不强，不同领域效果差异大

基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于正向最大匹配和基于逆向最大匹配及双向匹配法。

**基于统计的分词方法**

优点：适应性较强

缺点：成本较高，速度较慢

这类目前常用的是算法是 **HMM、CRF、SVM、深度学习** 等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。

**基于深度学习**

优点：准确率高、适应性强



缺点：成本高，速度慢

例如有人员尝试使用双向LSTM+CRF实现分词器，其本质上是序列标注，所以有通用性，命名实体识别等都可以使用该模型，据报道其分词器字符准确率可高达97.5%。

**常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。**

#### 中文分词工具

下面排名根据 GitHub 上的 star 数排名：

1. Hanlp
2. Stanford 分词
3. ansj 分词器
4. 哈工大 LTP
5. KCWS分词器
6. jieba
7. IK
8. 清华大学THULAC
9. ICTCLAS

#### 英文分词工具

1. Keras
2. Spacy
3. Gensim
4. NLTK

#### 总结

分词就是将句子、段落、文章这种长文本，分解为以字词为单位的数据结构，方便后续的处理分析工作。

**分词的原因：**

1. 将复杂问题转化为数学问题
2. 词是一个比较合适的粒度
3. 深度学习时代，部分任务中也可以「分字」

**中英文分词的3个典型区别：**

1. 分词方式不同，中文更难
2. 英文单词有多种形态，需要词性还原和词干提取
3. 中文分词需要考虑粒度问题

**中文分词的3大难点**

1. 没有统一的标准
2. 歧义词如何切分
3. 新词的识别

**3个典型的分词方式：**

1. 基于词典匹配
2. 基于统计
3. 基于深度学习

**[介绍分词原因](https://easyai.tech/ai-definition/tokenization/)**

**[介绍分词的方法](https://zhuanlan.zhihu.com/p/50444885)**

**[介绍一些中文分词库](https://www.zhihu.com/question/19578687/answer/190569700)**

分词一般都是直接调用上面提到的这些库

## 二、清洗

数据清洗的内容：数据重复处理、数据错误处理、数据缺失处理、数据异常处理
		对于我们使用的数据（包括文本、图像、数字信息等）以上操作是必不可少的，但是在文本清洗过程中还需要一些和文本相关的清洗操作，如下：无用信息的清理、特殊文本的清理、停用词的处理
		1.无用信息的清理：我们得到的数据可能带有html或者url标签，这些标签往往都是无用数据，在我们的数据中可能有一些长文本或者长字符串，例如电话号码等也可能是无用信息，其中还有许多对于模型训练没有任何作用的文本信息，例如某些广告等也可能是无用信息。
		2.特殊文本的清理：变形词是一些符号不同，但意义相同的的，这类词增加了涉黄涉政等特殊的文本分类场景下的分类难度，如下：特殊符号替换（对于有实际意义的可以替换成相应的语义，无太大实际意义也可以直接去掉（认为成停用词））同音近音近型替换（going、went、go->go）（有一种说法是往往英语需要，而中文一般不需要，可自行了解一下～）（实际上的实现是有一定难度的，清洗后面的标准化也是讲解这一方面的～）
简繁体替换等（我愛中國->我爱中国）
		3.停用词的处理：停用词是一些不包含或包含极少语义的词，另外标点符号和其他特殊符号也可以被认为是一种停用词。NLP学习中我们常常把停用词或字出现频率很低的词语给过滤掉。因为停用词和出现频率特别低的词汇对于分析往往作用不大，所以一般去掉。将停用词和出现频率特别低的词汇去掉后就可以获得一个词典—所有分词词汇的集合（无停用词和低频词）。

[清洗参考地址](https://www.cnblogs.com/ywjfx/p/11019689.html)

## 三、词的标准化

词形标准化的相关内容：在清洗过程中，我们说到了一个处理，就是同音近音近型替换也就是词形规范化，这里就讲述两种实现的方法，如下

词干提取（Stemming）：基于语言的规则，抽取词的词干或词根形式（不一定能够表达完整语义），方法较为简单。

词性还原 （Lemmatisation）：基于字典的映射，把一个词汇还原为一般形式（能表达完整语义），方法较为复杂。

词干提取：在py中也有相应的库（nltk.stem.porter），很容易实现，主要有：

基于Porter词干提取算法，PorterStemmer()方法

基于Lancaster 词干提取算法，LancasterStemmer()方法

基于Snowball 词干提取算法，SnowballStemmer()方法

词性还原 ：在py中也有相应的库（nltk.stem.porter），很容易调用实现实现，最常见的就是WordNetLemmatizer()方法

以下将从各种方面对两者进行对比。

从目的上来说：是一致的，词干提取和词形还原的目标均是将词的其他分支形态或者派生形态归并为原始形态，都是一种对词的不同形态的统一归并的过程。

从实现方法上说：词干提取是基于语言的规则的，利用规则变化进行词缀的去除和缩减，从而达到词的简化效果，而词性还原是基于字典的映射，利用字典进行词形变化和原形的映射，还原成为原始单词。

从原理上来说：词干提取是采取缩减的方法将词转换为词干（boys->boy），而词性还原是采取转换的方法将词转换为其原型（made->make）。

从复杂性上来说：词干提取相对容易，而词性还原相对复杂。

从结果上来说：词干提取得到的结果有可能并不是具有意义的词，而只是词的一部分（例如revival->reviv），而词性还原得到的结果一定是完整的单词。

从应用领域上来说：词干提取用于信息检索领域较多，而词性还原更适合文本挖掘、自然语言处理等更精确的领域。

**[两种标准化的区别](https://easyai.tech/ai-definition/stemming-lemmatisation/)**

## 四、拼写纠错和停用词

**[拼写纠错参考地址1](https://www.jianshu.com/p/df27693d9de2)**

**[拼写纠错参考地址2](https://blog.csdn.net/weixin_45422335/article/details/105638224)**

拼音纠错在一般的文本任务中使用较少。

出现频率特别高的和频率特别低的词对于文本分析帮助不大，一般在预处理阶段会过滤掉。 在英文里，经典的停用词为 “The”, “an”…

停用词也是直接调库使用。停用词的使用这和上面的分词使用结合。

## 五、独热编码表示

One-hot encoding（独热编码）通常在传统的数据分析，机器学习中用的较多，例如：一个特征为属性为文字：性别：男、女

独热编码可以自己用函数，或者直接调库

**[参考地址](https://blog.csdn.net/Hunter_Murphy/article/details/107310675?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-0&spm=1001.2101.3001.4242)**

六、TF-IDF与相似度

TF-IDF计算文章相似度。

TF-IDF的优点是计算简单，利于理解，性价比极高。但是它也有缺陷，首先单纯依据文章中的TF来衡重要性，忽略了位置信息。如段⾸首，句首一般权重更高；其次，有的文章可能关键词只出现1-2次，但可能通篇都是围绕其进行阐述和解释，所以单纯靠TF仍然不能解决所有的情况。

**[参考地址](https://www.cnblogs.com/liangjf/p/8283519.html)**

**[参考地址2](https://zhuanlan.zhihu.com/p/32826433)**

## 七、文本特征提取（分布式表示与词向量）

文本特征提取的方法：

[CountVectorizer](https://blog.csdn.net/weixin_38278334/article/details/82320307)是通过统计词汇出现的次数，并用词汇出现的次数的**稀疏矩阵**来表示文本的特征。它会统计所有出现的词汇，每个词汇出现了多少次，最后得到的稀疏矩阵的列就是词汇的数量（每个词汇就是一个特征/维度）

[TfidfVectorizer](https://zhuanlan.zhihu.com/p/67883024)提取的特征是：在一个文本中各个有效词汇对应的TFIDF值是多少，同时，每个文本特征向量会自动进行normalization（归一化）操作.

[HashingVectorizer](https://zhuanlan.zhihu.com/p/268886634)就是--- CountVectorizer省略了vocabulary这个映射（不管是CountVectorizer还是TfidfVectorizer，都是在内存中有一个word2id的映射。），而直接使用Hash的方式来映射。无论多少个词汇，都可以设定为固定维度，这么做节省了内存，但会有存在冲突的可能了，因为可能存在多个词汇共用一个id的情形。

词嵌入：	

​	[Word2vec（词向量）](https://zhuanlan.zhihu.com/p/26306795),词向量的用法很多，能够用于分类、预测等任务，在来斯惟的那篇博士论文有介绍。计算词向量的方法也有很多种。在深度学习的框架中体现为[Embedding](https://zhuanlan.zhihu.com/p/53194407).词向量有很多的模型实现。

​	[GloVe](https://nlp.stanford.edu/projects/glove/)和词向量都是词嵌入的一种。

词向量的评估和可视化，来斯惟的博士论文有具体的介绍，即三大任务，八个项目。[l另外一篇参考地址](https://blog.csdn.net/Datawhale/article/details/107194225)

[Doc2vec](https://zhuanlan.zhihu.com/p/36886191)：将一个句子甚至一篇短文也用一个向量来表示。

[参考地址1](http://blog.sina.com.cn/s/blog_b8effd230102zu8f.html)

[参考地址2-具体用法](https://zhuanlan.zhihu.com/p/163256631)

[介绍词向量](https://easyai.tech/ai-definition/word2vec/)

[词嵌入介绍](https://easyai.tech/ai-definition/word-embedding/)

## 八、词向量的具体介绍

词向量分为静态词向量和动态(上下文)词向量，静态词向量指的是**一个单词不管上下文如何变化只有一个唯一的词向量表示**，所以它最大的缺点是无法表达多意性，动态词向量指的会根据上下文动态适应性的调整词向量，可以一定程度上解决单词多意性。

一般而言，动态词向量会归类为预训练模型，例如elmo，bert模型。但是其实严格来说，不管静态还是动态词向量本身都是一种预训练的思想， 即第一阶段训练词向量，第二阶段根据使用预训练的词向量并根据需要是否进行fine-tuning。但是为了突出预训练的重要性，我们这里也只单独说静态词向量，后面有文章专门说基于预训练的动态词向量。这里说这么一段废话是希望读者能意识到这两块内容是有深刻的历史沿革的。

**静态词向量：**

word2vec来源于2013年的论文《Efficient Estimation of Word Representation in Vector Space》，它的核心思想是通过词的上下文得到词的向量化表示，有两种方法：CBOW（通过附近词预测中心词）、Skip-gram（通过中心词预测附近的词）。

**[word2vec](https://zhuanlan.zhihu.com/p/66289400)**部分有太多的资料了，这里推荐[word2vec的数学原理详解](https://link.zhihu.com/?target=https%3A//blog.csdn.net/itplus/article/details/37969519)，多看两遍，否则容易陷入繁乱的数学推导中。

word2vec的优点是克服了基于矩阵分解的部分问题：

- 语料库可以动态的变化，可以在线学习
- 不需要进行矩阵分解这种重量级的运算

缺点：

- 每次更新只使用少语料上下文信息，缺少全局上下文的大局观。
- 无法适应一词多义（静态词向量的痛点）

**Glove（Global Vectors for Word Representation）**融合了两者的优点，利用词共现矩阵充分利用了统计信息，高效地对语义进行编码。

**[动态词向量](https://zhuanlan.zhihu.com/p/73668860)：**

word embedding则不需要管下游是什么任务与模型，第一阶段训练词向量，第二阶段根据使用预训练的词向量并根据需要是否进行fine-tuning。归根结底他们思想都是：**复用底层特征**。

**Elmo(Embedding from Language Models):**

它的模型架构就是一个双向LSTM的LM模型.

elmo是为了解决一次多意性，对于不同的上下文可以给出不同的词向量，对于下游任务有增强作用。

它也是采用两阶段模式，第一阶段使用大的通用语料库训练一个emlo模型Elmo_A，然后使用这个预训练的模型做下游任务。步骤如下

1. 产生pre-trained biLM模型。模型由两层bi-LSTM组成。
2. 在任务语料上(注意是语料，忽略label)fine tuning上一步得到的biLM模型。可以把这一步看为biLM的domain transfer。
3. 利用ELMo的word embedding来对任务进行训练。通常的做法是把它们作为输入加到已有的模型中，一般能够明显的提高原模型的表现。

elmo的精髓其实在论文题目中体现完全了，两个词，deep和contextualized可以概括一切。

- contextualized：这是一个语言模型，其双向LSTM产生的词向量会包含左侧上文信息和右侧下文信息，所以称之为contextualized
- deep：句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些，这是所谓的deep。

elmo效果当然是很好的，并且如果站在当年的时间线上，它几乎是完美的。但是如果我们已未来者来审视这个模型，其最大的缺点有两个：

- 特征提取器是LSTM，不够强
- 双向LSTM直接concat，没有进行深度融合。

**GPT & Bert**

GPT是“Generative Pre-Training”的简称。GPT和Elmo有两个区别：

- 使用Transformer代替LSTM
- 使用单向语言模型

在现在看来这两个区别，一个是好的一个是坏的，好的证明了Transformer确实是由于LSTM的特征提取器，它在未来逐步变为NLP中一个基本的Block。坏的自然是使用单向语言模型，那么逻辑上说不通啊，Elmo都用了双向到GPT没有理由开历史的倒车啊！问题在于Transformer中的encoder本身，如果它变成双向会导致在高层的信息泄露，所以不采用任何特殊trick的话，使用Transformer的LM只有两个选择，要么单向，要么单层，显然GPT为了deep放弃了bi-direction。

那么到Bert这里，NLP的预训练模型基本确定了其江湖地位，大厦已成，但是还有两点小问题：

- [自回归语言模型 VS 自编码语言模型](https://github.com/jiutiananshu/Learning-Notes/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/hugging%20face/summary%20of%20model.md)： Bert实际上是自编码语言模型，而其使用的Masked LM正是训练这种模型的技术，但是这必然会导致预训练和Fine-tuning阶段的不一致（尽管Bert使用了各种“百分之”将其降得很低）。尤其是在Generation任务上，Generation任务就是根据已生成的单词以及一些外部知识，生成下一个单词，这和自回归语言模型天然匹配，但是和Bert的自编码语言模型天然矛盾。
- Transformer模型限制：Transfer需要把文本裁剪为固定长度，导致无法获得更长时间的依赖，以及....固定长度想想都觉得是一个很不好的设定好伐，变大白小变长变短才是我们想要的。

**XLNet**

[参考地址](https://zhuanlan.zhihu.com/p/70257427)

