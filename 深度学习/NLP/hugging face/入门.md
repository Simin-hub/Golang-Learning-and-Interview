# 入门教程

## 一、能做什么

1.该框架功能：情感分析、文本生成（英语）、命名实体（NER）、问答、填空、总结摘要、翻译、特征提取。

例如：

```python
# 引入库，设置管道
from transformers import pipeline
# 管道任务名称，使用默认的分类模型
classifier = pipeline('sentiment-analysis')

# 进行预测
classifier('We are very happy to show you the 🤗 Transformers library.')

# 选择其他的模型 可以从model hu（https://huggingface.co/models）上面选择 例如选择nlptown/bert-base-multilingual-uncased-sentiment。也可以用本地文件代替
classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
```

```
# 需要实例化两个对象，AutoTokenizer：是对文本进行分词，分词模型和分类模型需要一一对应。
# AutoModelForSequenceClassification：用于文本分类
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 选择模型
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
# 下载并实例分类模型
model = AutoModelForSequenceClassification.from_pretrained(model_name)
# 下载并实例分词模型
tokenizer = AutoTokenizer.from_pretrained(model_name)
# 输入到管道内
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
```

## 2.分析预训练模型

```
# 同上
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

分词器（tokenizer）是对文本进行预处理，对文本进行切分（词、标点、符号）称为分词。

```
inputs = tokenizer("We are very happy to show you the 🤗 Transformers library.")
inputs
# {'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
# 返回的是一个字典，input_ids：每个单词在字典中的id， attention_mask：表示是否MASKLE 1 表示没有
```















## 4.基本概念

该框架仅仅需要实例化三个类，其他对象都被封装。这些类通过统一的from_pretrained()方法实现。

在这三个类的三层，提供了pipeline()用于快速建立模型，`Trainer()`/`TFTrainer()`快速训练模型。

**Model classes** 提供了各个模型的各种方法。

**Configuration classes** 存储了模型所需要的参数



## 5.tokenizer and mask_attened

**input_ids**

在同一个tensor进行tokenizer时中，短序列需要补齐（或者长序列截断）。

```
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
sequence_a = "This is a short sequence."
sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."
encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
encoded_sequence_b = tokenizer(sequence_b)["input_ids"]
```

```
len(encoded_sequence_a), len(encoded_sequence_b)
（8，19）
padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)
padded_sequences["input_ids"]
输入：
[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]
```

**attention_mask**表示注不注意到该词，1表示注意，不用mask标记该词。表示不注意到该词

```
padded_sequences["attention_mask"]
输出：
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]
```

**token_type_ids**表示哪一段序列开始和结束。

```
encoded_dict['token_type_ids']
输出：
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

**position_ids**（可选参数）表示token所在的位置

**Labels**（可选参数）用于分类计算loss。用途有：

​	在文本分类任务中，对应的shape是batch_size，每个预测值对应真实标签。

​	在分词分类任务中，对应的shape是batch_size * seq_length，在每次预测出的token_id对应真实的

​	在mask语言模型中，对应的shape是batch_size * seq_length，在每次预测mask对应的token_id和真实的（简而言之，根据上下文预测词）

​	在序列到序列的任务中，对应的shape是batch_size * tgt_seq_length，每个值对应于与每个输入序列相关联的目标序列

**decoder_input_ids**：这个输入是特定于编码器-解码器模型的，并且包含将被输入到解码器的输入id。这些输入应该用于序列到序列任务，例如翻译或摘要，并且通常以特定于每个模型的方式构建。