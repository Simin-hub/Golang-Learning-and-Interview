# Activation

深度学习的基本原理是基于人工神经网络，信号从一个神经元进入，经过**非线性的**activation function，传入到下一层神经元；再经过该层神经元的activate，继续往下传递，如此循环往复，直到输出层。正是由于这些非线性函数的反复叠加，才使得神经网络有足够的capacity来抓取复杂的pattern，在各个领域取得state-of-the-art的结果。显而易见，activation function在深度学习中举足轻重，也是很活跃的研究领域之一。目前来讲，选择怎样的activation function不在于它能否模拟真正的神经元，而在于能否便于优化整个深度神经网络。下面我们简单聊一下各类函数的特点以及为什么现在优先推荐ReLU函数。

激活函数确实是很好宽广的点，它对于**提高模型鲁棒性，非线性表达能力，缓解梯度消失问题，将特征图映射到新的特征空间从何更有利于训练，加速模型收敛等问题都有很好的帮助**