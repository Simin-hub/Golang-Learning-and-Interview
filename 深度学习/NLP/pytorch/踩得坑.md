# 踩的坑

#### 1. 输入和模型在不同的设备（CPU\GPU）

通过判断模型model的参数是否在cuda上来判定模型是否在gpu上。

```
print('Is model on gpu: ', next(model.parameters()).is_cuda)
```

输出若是True，则model在gpu上；若是False，则model在cpu上。

输出数据data的device字段。

```
print('data device: ', data.device)
```

输出gpu则在gpu上，输出cpu则在cpu上。

#### 2. 卷积层输入通道为1时，需要4-dim

```
self.convs = nn.ModuleList(
        [nn.Conv2d(1, config['num_filter'], (k, config['emb_dim'])) for k in config['filter_sizes']]
    )
    
# 增加一维通道数 batch_size * seq_len * embed_dim >> batch_size * channels * seq_len * embed_dim
embed_output = embed_output.unsqueeze(1)
outputs = model(embed_output)
```

#### 3. np拼接不能用[]

进行拼接再分离时不能为了方便直接使用[]，这样都后面都不能单独取出其中的数组

[]操作只能一直在列表中，列表和数组转化操作中不能出现[]

```
x1 = [[1,2,3,4,5],[7,8,9,10,12]]
x2 = [6,49,57,78]
x3 = np.array([x1,x2]) # 这样是不行的

# 正确操作
x1 = np.array([[1,2,3,4,5],[7,8,9,10,12]])
x2 = np.array([6,49,57,78]).reshape(-1,1)
x3 = np.concatenate((x1,x2),axis=1)
```

#### 4. bert模型

bert模型限制一句话的输入最长为130.

#### 5.  关于损失函数计算

labels.shape必须是[batch_size]。不能变成[batch_size * 1]

```
loss = F.cross_entropy(outputs, labels.cuda())
```

