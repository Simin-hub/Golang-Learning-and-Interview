# 模型训练、验证、预测

## 1. model.train()和model.eval()用法和区别

### 1.1 model.train()

model.train()的作用是**启用 Batch Normalization 和 Dropout。**

如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到**每一批数据的均值和方差**。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。

### 1.2 model.eval()

model.eval()的作用是**不启用 Batch Normalization 和 Dropout。**

如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用**全部训练数据的均值和方差**，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。

训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。

在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点。

### 1.3 分析原因

使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval。model.eval()时，框架会自动把BN和Dropout固定住，不会取平均，而是用训练好的值，不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大！

为什么PyTorch会关注我们是训练还是评估模型？最大的原因是dropout和BN层（以dropout为例）。这项技术在训练中随机去除神经元。

![img](https://raw.githubusercontent.com/jiutiananshu/Picture/master/img/v2-d5bb3153e0189a2ce04728c5b068e5b1_720w.png?token=AKS6BK6WFLM2KRF4JXM6CSTBUNTIU)dropout

想象一下，如果右边被删除的神经元（叉号）是唯一促成正确结果的神经元。一旦我们移除了被删除的神经元，它就迫使其他神经元训练和学习如何在没有被删除神经元的情况下保持准确。这种dropout提高了最终测试的性能，但它对训练期间的性能产生了负面影响，因为网络是不全的。

**下面我们看一个我们写代码的时候常遇见的错误写法：**

在这个特定的例子中，似乎每50次迭代就会降低准确度。

如果我们检查一下代码， 我们看到确实在train函数中设置了训练模式。

```python3
def train(model, optimizer, epoch, train_loader, validation_loader):
    model.train()  # 错误的位置
    for batch_idx, (data, target) in experiment.batch_loop(iterable=train_loader):
    	# model.train() # 正确的位置，保证每一个batch都能进入model.train()的模式
        data, target = Variable(data), Variable(target)
        # Inference
        output = model(data)
        loss_t = F.nll_loss(output, target)
        # The iconic grad-back-step trio
        optimizer.zero_grad()
        loss_t.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            train_loss = loss_t.item()
            train_accuracy = get_correct_count(output, target) * 100.0 / len(target)
            experiment.add_metric(LOSS_METRIC, train_loss)
            experiment.add_metric(ACC_METRIC, train_accuracy)
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx, len(train_loader),
                100. * batch_idx / len(train_loader), train_loss))
            with experiment.validation():
            	# test是另外的函数
                val_loss, val_accuracy = test(model, validation_loader)
                experiment.add_metric(LOSS_METRIC, val_loss)
                experiment.add_metric(ACC_METRIC, val_accuracy)
```

这个问题不太容易注意到，在循环中我们调用了test函数。

```python3
def test(model, test_loader):
    model.eval()
    # ...
```

在test函数内部，我们将模式设置为eval。这意味着，如果我们在训练过程中调用了test函数，我们就会进eval模式，直到下一次train函数被调用。这就导致了每一个epoch中只有一个batch使用了dropout ，这就导致了我们看到的性能下降。

修复很简单我们将model.train() 向下移动一行，让其在训练循环中。理想的模式设置是尽可能接近推理步骤，以避免忘记设置它。修正后，我们的训练过程看起来更合理，没有中间的峰值出现。

## 2. model.eval()和torch.no_grad()的区别

在PyTorch中进行validation/test时，会使用model.eval()切换到测试模式，在该模式下：

1. 主要用于通知dropout层和BN层在train和validation/test模式间切换：

- 在train模式下，dropout网络层会按照设定的参数p设置保留激活单元的概率（保留概率=p); BN层会继续计算数据的mean和var等参数并更新。
- 在eval模式下，dropout层会让所有的激活单元都通过，而BN层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值。

2. 该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播（backpropagation)。

而with torch.no_grad()则主要是用于**停止autograd模块**的工作，以起到加速和节省显存的作用。它的作用是将该with语句包裹起来的部分停止梯度的更新，从而节省了GPU算力和显存，但是并不会影响dropout和BN层的行为。

如果不在意显存大小和计算时间的话，仅仅使用model.eval()已足够得到正确的validation/test的结果；而with torch.no_grad()则是更进一步加速和节省gpu空间（因为不用计算和存储梯度），从而可以更快计算，也可以跑更大的batch来测试。

## 3. torch.no_grad()

**在验证和测试阶段取消掉梯度（no_grad）**

一般来说，我们在进行模型训练的过程中，因为要监控模型的性能，在跑完若干个`epoch`训练之后，需要进行一次在**验证集**上的性能验证。一般来说，在验证或者是测试阶段，因为只是需要跑个前向传播(forward)就足够了，因此不需要保存变量的梯度。**保存梯度**是需要额外**显存或者内存**进行保存的，占用了空间，有时候还会在验证阶段导致**OOM**(**O**ut **O**f **M**emory)错误，**因此我们在验证和测试阶段，最好显式地取消掉模型变量的梯度。** 在`pytroch 0.4`及其以后的版本中，用`torch.no_grad()`这个上下文管理器就可以了，例子如下：

> 显式地取消掉模型变量的梯度：

```python3
model.train()
# here train the model, just skip the codes
model.eval() # here we start to evaluate the model
with torch.no_grad():
 for each in eval_data:
  data, label = each
  logit = model(data)
  ... # here we just skip the codes
```

如上，我们只需要在加上上下文管理器就可以很方便的取消掉梯度。这个功能在`pytorch`以前的版本中，通过设置`volatile=True`生效，不过现在这个用法已经被抛弃了。

**显式指定`model.train()`和`model.eval()`**

我们的模型中经常会有一些**子模型**，其在训练时候和测试时候的参数是不同的，比如`dropout`中的丢弃率和`Batch Normalization`中的![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma)和![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta)等，这个时候我们就需要显式地指定不同的阶段（训练或者测试），在`pytorch`中我们通过`model.train()`和`model.eval()`进行显式指定，具体如：

> **训练阶段和测试阶段必须显式指定train()和eval()：**

```python3
model = CNNNet(params)
# here we start the training
model.train()
for each in train_data:
 data, label = each
 logit = model(data)
 loss = criterion(logit, label)
 ... # just skip
# here we start the evaluation

model.eval() 
with torch.no_grad(): # we dont need grad in eval phase
 for each in eval_data:
  data, label = each
  logit = model(data)
  loss = criterion(logit, label)
  ... # just skip
```

**注意，在模型中有BN层或者dropout层时，在训练阶段和测试阶段必须显式指定train()和eval()**。

## 4. model.parameters()与model.state_dict()

*model.parameters()*与*model.state_dict()*是Pytorch中用于查看网络参数的方法。一般来说，**前者多见于优化器的初始化**，例如：

![img](https://pic4.zhimg.com/80/v2-5c9bbd19ac058c725550d6a800ca19b7_720w.jpg)

**后者多见于模型的保存**，如：

![img](https://pic1.zhimg.com/80/v2-a52f44627d28ae6339adae1950a0de34_720w.jpg)

当我们对网络调参或者查看网络的参数是否具有可复现性时，可能会查看网络的参数。

model.state_dict()方法

pytorch 中的 state_dict 是一个简单的python的字典对象,将每一层与它的对应参数建立映射关系.(如model的每一层的weights及偏置等等)

注意：

（1）只有那些参数可以训练的layer才会被保存到模型的state_dict中,如卷积层,线性层等等，像什么池化层、BN层这些本身没有参数的层是没有在这个字典中的；

（2）这个方法的作用一方面是方便查看某一个层的权值和偏置数据，另一方面更多的是在模型保存的时候使用。

model.parameters()方法

这个方法也会获得模型的参数信息，如下：

print(type(model.parameters())) # 返回的是一个generator

从这里可以看出，其实这个state_dict方法所得到结果差不多，不同的是，model.parameters()方法返回的是一个生成器generator，每一个元素是从开头到结尾的参数，parameters没有对应的key名称，是一个由纯参数组成的generator，而state_dict是一个字典，包含了一个key。

其实Module还有一个与parameters类似的函数，named_parameters，而且parameters正是通过named_parameters来实现的，

## 5. torch.optim

### 5.1 model.zero_grad()和optimizer.zero_grad()

```python3
model.zero_grad()
optimizer.zero_grad()
```

首先，这两种方式都是把模型中参数的梯度设为0

当optimizer = optim.Optimizer(model.parameters())时，二者等效，其中Optimizer可以是Adam、SGD等优化器

```text
def zero_grad(self):
        """Sets gradients of all model parameters to zero."""
        for p in self.parameters():
            if p.grad is not None:
                p.grad.data.zero_()
```

### 5.2 torch.optim讲解

[参考地址](https://zhuanlan.zhihu.com/p/346205754)

#### 1. 优化器 Optimizer

**1.0 基本用法**

- 优化器主要是在模型训练阶段对模型可学习参数进行更新, 常用优化器有 SGD，RMSprop，Adam等
- 优化器初始化时传入传入模型的可学习参数，以及其他超参数如 `lr`，`momentum`等
- 在训练过程中先调用 `optimizer.zero_grad()` 清空梯度，再调用 `loss.backward()` 反向传播，最后调用 `optimizer.step()`更新模型参数

简单使用示例如下所示：

```python
import torch
import numpy as np
import warnings
warnings.filterwarnings('ignore') #ignore warnings

x = torch.linspace(-np.pi, np.pi, 2000)
y = torch.sin(x)

p = torch.tensor([1, 2, 3])
xx = x.unsqueeze(-1).pow(p)

model = torch.nn.Sequential(
    torch.nn.Linear(3, 1),
    torch.nn.Flatten(0, 1)
)
loss_fn = torch.nn.MSELoss(reduction='sum')

learning_rate = 1e-3
optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)
for t in range(1, 1001):
    y_pred = model(xx)
    loss = loss_fn(y_pred, y)
    if t % 100 == 0:
        print('No.{: 5d}, loss: {:.6f}'.format(t, loss.item()))
    optimizer.zero_grad() # 梯度清零
    loss.backward() # 反向传播计算梯度
    optimizer.step() # 梯度下降法更新参数
No.  100, loss: 26215.714844
    No.  200, loss: 11672.815430
    No.  300, loss: 4627.826172
    No.  400, loss: 1609.388062
    No.  500, loss: 677.805115
    No.  600, loss: 473.932159
    No.  700, loss: 384.862396
    No.  800, loss: 305.365143
    No.  900, loss: 229.774719
    No. 1000, loss: 161.483841
```

**1.2.1 初始化 Optimizer**

初始化优化器只需要将模型的可学习参数(params)和超参数(defaults)分别传入优化器的构造函数.

**1.2.2 add_param_group**

该方法在初始化函数中用到，主要用来向 `self.param_groups`添加不同分组的模型参数

利用 add_param_group 函数功能，可以对模型不同的可学习参数组设定不同的超参数，初始化优化器可传入元素是 dict 的 list，每个 dict 中的 key 是 `params` 或者其他超参数的名字如 `lr`，下面是一个实用的例子：对模型的`fc`层参数设置不同的学习率

```python
from torch.optim import SGD
from torch import nn

class DummyModel(nn.Module):
    def __init__(self, class_num=10):
        super(DummyModel, self).__init__() 
        self.base = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
        )
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(128, class_num)

    def forward(self, x):
        x = self.base(x)
        x = self.gap(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)
        return x

model = DummyModel().cuda()

optimizer = SGD([
                {'params': model.base.parameters()}, 
                {'params': model.fc.parameters(), 'lr': 1e-3} # 对 fc的参数设置不同的学习率
            ], lr=1e-2, momentum=0.9)
```

**1.2.3 step**

此方法主要完成一次模型参数的更新.

- step 方法可传入闭包函数 closure，主要目的是为了实现如`Conjugate Gradient`和`LBFGS`等优化算法，这些算法需要对模型进行多次评估
- Python 中闭包概念：在一个内部函数中，对外部作用域的变量进行引用(并且一般外部函数的返回值为内部函数)，那么内部函数就被认为是闭包

**1.2.4 zero_grad**

- 在反向传播计算梯度之前对上一次迭代时记录的梯度清零，参数`set_to_none` 设置为 `True` 时会直接将参数梯度设置为 `None`，从而减小内存使用, 但通常情况下不建议设置这个参数，因为梯度设置为 `None` 和 `0` 在 PyTorch 中处理逻辑会不一样。

**1.2.5 state_dict() 和 load_state_dict**

这两个方法实现序列化和反序列化功能。

- state_dict(): 将优化器管理的参数和其状态信息以 dict 形式返回
- load_state_dict(state_dict): 加载之前返回的 dict，更新参数和其状态
- 两个方法可用来实现模型训练中断后继续训练功能

**1.3 常见优化器简介**

**1.3.1 SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)**

实现带`momentum`和`dampening`的 SGD，公式如下：

![[公式]](https://www.zhihu.com/equation?tex=v_%7Bt%2B1%7D+%3D+%5Cmu+%2A+v_%7Bt%7D+%2B+g_%7Bt%2B1%7D+%5C+p_%7Bt%2B1%7D++%3D+p_%7Bt%7D+-+%5Ctext%7Blr%7D+%2A+v_%7Bt%2B1%7D+)

**1.3.2 Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)**

自适应学习率，考虑历史所有梯度信息, 公式如下:

![[公式]](https://www.zhihu.com/equation?tex=p_%7Bt%7D+%3D+p_%7Bt-1%7D++-+%5Ctext%7Blr%7D+%2A+%5Cfrac%7Bg_t%7D%7B%5Csqrt%7B%5Csum%5E%7Bi%3Dt%7D_%7Bi%3D1%7D+g_t%5E2+%2B+eps%7D%29%7D+)

**1.3.3 RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)**

加权考虑历史梯度和当前梯度，历史梯度系数是 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)，当前梯度系数是 ![[公式]](https://www.zhihu.com/equation?tex=%281-%5Calpha%29)

![[公式]](https://www.zhihu.com/equation?tex=E_t+%3D+%5Calpha+%2A+E_%7Bt-1%7D+%2B+%281+-+%5Calpha%29+%2A+g%5E2_t+%5C+p_%7Bt%7D+%3D+p_%7Bt-1%7D++-+%5Ctext%7Blr%7D+%2A+%5Cfrac%7Bg_t%7D%7B%5Csqrt%7BE_%7Bt%7D+%2B+eps%7D%7D+)

**1.3.4 Adam(params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False)**

实现了自适应学习率有优化器, Adam 是 Momentum 和 RMSprop 的结合 主要超参数有 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_1) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_2) `，eps`。 公式如下：

其中， ![[公式]](https://www.zhihu.com/equation?tex=mt) 、![[公式]](https://www.zhihu.com/equation?tex=vt) 分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望 E[gt]、E[g_t^2] 的近似； ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm%7D%7Bt%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bv%7D%7Bt%7D) 是校正，这样可以近似为对期望的无偏估计

![[公式]](https://www.zhihu.com/equation?tex=+m_%7Bt%7D+%3D+%5Cbeta_%7B1%7D+%2A+m_%7Bt-1%7D+%2B+%281-%5Cbeta_%7B1%7D%29+%2A+g_%7Bt%7D+%5C+v_%7Bt%7D+%3D++%5Cbeta_%7B2%7D+%2A+v_%7Bt-1%7D+%2B+%281-%5Cbeta_%7B2%7D%29+%2A+g%5E2_%7Bt%7D++%5C+%5Chat%7Bm%7D%7Bt%7D+%3D+%5Cfrac%7Bm_t%7D+%7B1+-+%5Cbeta%5Et%7B1%7D%7D+%5C+%5Chat%7Bv%7D%7Bt%7D+%3D+%5Cfrac%7Bv_t%7D+%7B1+-+%5Cbeta%5Et%7B2%7D%7D+%5C+p_%7Bt%7D+%3D+p_%7Bt%7D+-+%5Ctext%7Blr%7D+%2A+%5Cfrac%7B%5Chat%7Bm%7Dt%7D+%7B%5Csqrt%7B%5Chat%7Bv%7D%7Bt%7D+%2B+%5Cepsilon%7D%7D+)

#### 2.1 基类: `_LRScheduler`

学习率调整类主要的逻辑功能就是每个 epoch 计算参数组的学习率，更新 `optimizer`对应参数组中的`lr`值，从而应用在`optimizer`里可学习参数的梯度更新。所有的学习率调整策略类的父类是`torch.optim.lr_scheduler._LRScheduler`，基类 `_LRScheduler` 定义了如下方法:

- step(epoch=None): 子类公用
- get_lr(): 子类需要实现
- get_last_lr(): 子类公用
- print_lr(is_verbose, group, lr, epoch=None): 显示 lr 调整信息
- state_dict(): 子类可能会重写
- load_state_dict(state_dict): 子类可能会重写

**2.1.1 初始化**

基类的初始化函数可传入两个参数, 第一是`optimizer`就是之前我们讲过的优化器的实例，第二个参数`last_epoch`是最后一次 epoch 的 index，默认值是 -1，代表初次训练模型，此时会对`optimizer`里的各参数组设置初始学习率 `initial_lr`。若`last_epoch`传入值大于 -1，则代表从某个 epoch 开始继续上次训练，此时要求`optimizer`的参数组中有`initial_lr`初始学习率信息。初始化函数内部的 `with_counter` 函数主要是为了确保`lr_scheduler.step()`是在`optimizer.step()`之后调用的 (PyTorch=1.1 发生变化). 注意在`__init__`函数最后一步调用了`self.step()`，即`_LRScheduler`在初始化时已经调用过一次`step()`方法。

**2.1.2 step**

当模型完成一个 epoch 训练时，需要调用`step()`方法，该方法里对`last_epoch`自增之后，在内部上下文管理器类里调用子类实现的`get_lr()`方法获得各参数组在此次 epoch 时的学习率，并更新到 `optimizer`的`param_groups`属性之中，最后记录下最后一次调整的学习率到`self._last_lr`，此属性将在`get_last_lr()`方法中返回。在这个方法中用到了上下文管理功能的内部类 `_enable_get_lr_call`，实例对象添加了`_get_lr_called_within_step`属性，这个属性可在子类中使用。此外，需要注意的是，`step`方法中的参数`epoch`已经废弃了，在使用时可以直接忽略这个参数。

**2.1.3 get_last_lr、get_lr和print_lr**

- `get_last_lr()`方法比较简单，就是`step()`方法调用后，记录的最后一次 `optimizer`各参数组里更新后的学习率信息
- `get_lr()` 方法是抽象方法，定义了更新学习率策略的接口，不同子类继承后会有不同的实现.其返回值是[lr1, lr2, ...]结构
- `print_lr(is_verbose, group, lr, epoch=None))`: 该方法提供了显示 lr 调整信息的功能

**2.1.4 state_dict 和 load_state_dict**

这两个方法和`Optimizer`里的方法功能是一样的，就是为了保存和重新加载状态信息，需要注意的是，这里不会重复记录`self.optimizer`属性的状态信息，因为 `Optimizer` 有自己实现的对应方法。

- `state_dict()`: 以字典 dict 形式返回当前实例除 `self.optimizer` 之外的其他所有属性信息
- `load_state_dict(state_dict)`: 重新载入之前保存的状态信息

#### 2.2 学习率调整策略示例

**2.2.1 StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)**

`StepLR`是根据 epoch 的等间隔学习率调整策略，实现了`get_lr()`方法。初始化函数须传入优化器，epoch 间隔 `step_size，gamma`是学习率的衰减系数，默认是 0.1。

```python
## 可视化学习率
from torch.optim import lr_scheduler
from matplotlib import pyplot as plt
%matplotlib inline

def create_optimizer():
    return SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)

def plot_lr(scheduler, title='', labels=['base'], nrof_epoch=100):
    lr_li = [[] for _ in range(len(labels))]
    epoch_li = list(range(nrof_epoch))
    for epoch in epoch_li:
        scheduler.step()  # 调用step()方法,计算和更新optimizer管理的参数基于当前epoch的学习率
        lr = scheduler.get_last_lr()  # 获取当前epoch的学习率
        for i in range(len(labels)):
            lr_li[i].append(lr[i])
    for lr, label in zip(lr_li, labels):
        plt.plot(epoch_li, lr, label=label)
    plt.grid()
    plt.xlabel('epoch')
    plt.ylabel('lr')
    plt.title(title)
    plt.legend()
    plt.show()
## StepLR 可视化学习率
optimizer = create_optimizer()
scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
plot_lr(scheduler, title='StepLR')
```

**2.2.2 MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)**

多阶段学习率调整策略，参数 `milestones` 是包含多个学习率调整点列表

```python
optimizer = create_optimizer()
scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[20, 35, 45], gamma=0.5)
plot_lr(scheduler, title='MultiStepLR')
```

![img](https://pic3.zhimg.com/80/v2-93e2ce394fa89a9e18ab496d2cc58d9a_720w.jpg)

**2.2.3 MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)**

乘法调整策略实现了学习率的衰减系数 `gamma`可变，即在每个调整节点，可对各参数组的学习率乘上一个不同的衰减率`gamma`，初始化函数中`lr_lambda`参数可以是一个`lambda`函数，也可是`lambda`函数列表，每个`lambda`函数输入是 epoch，输出是`gamma`。

```python
optimizer = SGD([
                {'params': model.base.parameters()}, 
                {'params': model.fc.parameters(), 'lr': 0.05} # 对 fc的参数设置不同的学习率
            ], lr=0.1, momentum=0.9)
lambda_base = lambda epoch:  0.5 if epoch % 10 == 0 else 1
lambda_fc = lambda epoch: 0.8 if epoch % 10 == 0 else 1
scheduler = lr_scheduler.MultiplicativeLR(optimizer, [lambda_base, lambda_fc])
plot_lr(scheduler, title='MultiplicativeLR', labels=['base', 'fc'])
```

![img](https://pic4.zhimg.com/80/v2-9950a2c3b3674ca6b683e12f612c224b_720w.jpg)

**2.2.4 LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)**

该策略可传入自定义的`lambda`函数， `lambda`函数参数为`epoch`，返回值为学习率。

```python
# LamdbdaLR调用示例
def lambda_foo(epoch):
    if epoch < 10:
        return (epoch+1) * 1e-3
    elif epoch < 40:
        return 1e-2
    else:
        return 1e-3

optimizer = create_optimizer()
scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_foo)
plot_lr(scheduler, title='LambdaLR')
```

![img](https://pic1.zhimg.com/80/v2-8e226cece1e9a94846612528b3488f88_720w.jpg)

**2.2.5 ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False)**

指数衰减学习率调整策略

```python
optimizer = create_optimizer()
scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
plot_lr(scheduler, title='ExponentialLR')
```

![img](https://pic2.zhimg.com/80/v2-4d2604b2dabc8554ebc9cbf6a1e6ff1d_720w.jpg)

**2.2.6 CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)**

余弦退火调整策略，`T_max`是最大迭代次数， `eta_min`是最小学习率值，其公式如下，eta_max为初始学习率，T_cur 是自重新启动后的 epoch 数

![[公式]](https://www.zhihu.com/equation?tex=+%5Cbegin%7Baligned%7D+%5Ceta_t+%3D+%5Ceta_%7Bmin%7D+%2B+%5Cfrac%7B1%7D%7B2%7D%28%5Ceta_%7Bmax%7D+-+%5Ceta_%7Bmin%7D%29%5Cleft%281+%2B+++++++++%5Ccos%5Cleft%28%5Cfrac%7BT_%7Bcur%7D%7D%7BT_%7Bmax%7D%7D%5Cpi%5Cright%29%5Cright%29+%5Cend%7Baligned%7D+)

```python
optimizer = create_optimizer()
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 10, 1e-5)
plot_lr(scheduler, title='CosineAnnealingLR')
```

![img](https://pic3.zhimg.com/80/v2-c4b7db894ebcb0cdc4c9c3853587e1d6_720w.jpg)

**2.2.7 CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False)**

在 SGDR([Stochastic Gradient Descent with Warm Restarts](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1608.03983))中提出:

- `T_0`: 第一次启动时的迭代数
- `T_mult`: 启动后，改变周期 T 的因子
- `eta_min`: 学习率下限

```python
optimizer = create_optimizer()
scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2)
plot_lr(scheduler, title='CosineAnnealingWarmRestarts')
```

![img](https://pic3.zhimg.com/80/v2-43f57b03de26aeab5b46a8903ca0e3d6_720w.jpg)

**2.2.8 CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', ...)**

类似三角波形状的学习率调整策略，以下是几个重要初始化参数:

- `base_lr`: 基准学习率，也是最小的学习率
- `max_lr`: 学习率上限
- `step_size_up`: 一个周期里上升阶段 epoch 数
- `step_size_down`: 一个周期里下降阶段 epoch 数

```python
optimizer = create_optimizer()
scheduler = lr_scheduler.CyclicLR(optimizer, 0.01, 0.1, step_size_up=25, step_size_down=10)
plot_lr(scheduler, title='CyclicLR')
```

![img](https://pic3.zhimg.com/80/v2-a24bb1fc8823ae535e8e1a402a8807d2_720w.jpg)

**2.2.9 OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3,...)**

只有 1 次循环的学习率调整策略

- `max_lr`: float/list, 学习率调整的上限
- `total_steps`: int 循环中的总步数

```python
optimizer = create_optimizer()
scheduler = lr_scheduler.OneCycleLR(optimizer, 0.1, total_steps=100)
plot_lr(scheduler, title='OneCycleLR')
```

![img](https://pic3.zhimg.com/80/v2-0efd180ba5b1f0936161f4b2249fb3e2_720w.jpg)

**2.2.10 ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', ...)**

自适应学习率调整策略，比如只有当 loss 在几个 epoch 里都不发生下降时，才调整学习率。注意在调用时，需要在其 `step()` 方法中传入对应的参考变量，例如: `scheduler.step(val_loss)`

- `mode`: 评价模型训练质量的模式, 传入值为`min` 或`max`
- `factor`: 学习率衰减因子, 类似`gamma`
- `patience`: 控制何时调整学习率

示例:

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
scheduler = ReduceLROnPlateau(optimizer, 'min')
for epoch in range(100):
    train(...)
    val_loss = validate(...)
    scheduler.step(val_loss)
```

#### 3 swa_utils里SWA相关类和函数

该模块中只有 2 个类和一个函数:

- AveragedModel: 实现 SWA 算法的权重平均模型
- SWALR: 与`AverageModel`配合使用的学习率调整策略
- update_bn: 更新模型中的 bn

#### 3.0 SWA 简介

随机权重平均(SWA)是一种优化算法，在SWA 论文的结果证明，取 SGD 轨迹的多点简单平均值，以一个周期或者不变的学习率，会比传统训练有更好的泛化效果。论文的结果同样了证明了，随机权重平均 (SWA) 可以找到更广的最优值域。

#### 3.1 AveragedModel

- 该类实现 SWA 算法的权重平均模型，初始化时传入模型 `model` 和参数平均化函数 `avg_fn`，然后在初始化函数中对 `model`的参数进行深拷贝, 注册模型计数器。
- 在`update_parameters(self, model)`方法中再次传入模型后，根据参数`avg_fn`对模型参数进行平均后更新 `swa` 模型参数。

#### **3.2 update_bn**

该函数主要是通过传入的某个训练时刻的模型`model` 和 `dataloader`，来允许 swa 模型计算和更新 bn

#### 3.3 SWALR

`SWALR`类继承`_LRScheduler`基类，实现了供 swa 模型的学习率调整策略



## 6. 可视化（TensorboardX）

[参考地址](https://zhuanlan.zhihu.com/p/54947519)

[参考地址二](https://medium.com/looka-engineering/how-to-use-tensorboard-with-pytorch-in-google-colab-1f76a938bc34)

**tensorboadX**

这相当于一个辅助工具，可以把Pytorch中的参数传递到Tensorboad上面，那么如何进行安装呢？分为三个步骤：

> pip install tensorboardX
>
> pip install tensorboard
>
> pip install tensorflow

**网络训练（Cifar10）**

首先，我使用了非官方的代码对Cifar10进行训练，类似于ResNet, 由于Cifar10中的图片尺寸都很小，大约32x32，所以我们对传统的resnet进行了修改，其网络结构如下：
参考于官方的ResNet18并做如下修改：

- 由于像素太小，修改第一个卷积核步长为1，不进行下采样
- 修改通道，让通道变小些
- 删除layer4，不用再继续降采样了

```text
# ResNet
class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes = 10):   # block为残差模块
        super(ResNet, self).__init__()
        self.in_channels = 16
        self.conv = conv3x3(3, 16)
        self.bn = nn.BatchNorm2d(16)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self.make_layer(block, 16, layers[0], 1)   # 残差模块
        self.layer2 = self.make_layer(block, 32, layers[1], 2)
        self.layer3 = self.make_layer(block, 64, layers[2], 2)
        self.avg_pool = nn.AvgPool2d(8)    # kernel_size = 8
        self.fc = nn.Linear(64, num_classes)

    def make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if (stride != 1) or (self.in_channels != out_channels):
            downsample = nn.Sequential(
                conv3x3(self.in_channels, out_channels, stride=stride),
                nn.BatchNorm2d(out_channels)
            )
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels
        for i in range(1, blocks):
            layers.append(block(out_channels, out_channels))
        return nn.Sequential(*layers)    # 把一个列表变成一个层的函数

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        out = self.relu(out)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.avg_pool(out)
        out = out.reshape(out.size(0), -1)   # 1*1*64 features
        out = self.fc(out)
        return out
```

然后进行模型训练，训练代码如下：

```text
# 训练整个网络
total_step = len(train_loader)
curr_lr = learning_rate
for epoch in range(num_epoches):
    for i, (images, labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)

        # 正向传播
        outputs = model(images)
        loss = criterion(outputs, labels)

        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (i+1) % 100 == 0:
            print(f'Epoch {epoch+1}/{num_epoches}, Step {i+1}/{total_step}, {loss.item()}')   # 不要忘了item()
torch.save(model.state_dict(), 'ResnetCifar10.pt')
```

最后我们得到模型，精度为：0.78左右，没有一直进行训练，这无所谓！！

#### **TensorBoardX可视化**

参考了一开始的那个代码库，我把可视化的工作分成了如下几个：

- 卷积核的可视化
- feature map的可视化
- 模型图的可视化
- 标量变化情况的可视化
- 权重的直方图的可视化（可以知道权重的数据分布）

那么，话不多说，我们来看看如何进行可视化的把，首先把tensorboardX导入并进行初始化:

```text
from tensorboardX import SummaryWriter
# 定义Summary_Writer
writer = SummaryWriter('./Result')   # 数据存放在这个文件夹
```

**第一个任务：标量可视化与权重直方图**

> add_scalar(tag, scalar_value, global_step=None, walltime=None) 记录标量的变化
>
> add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None) 记录多个标量的值
>
> add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None) 绘制直方图

这个权重直方图，非常的关键，特别是寻找模型训练loss出现的各种问题，由于我是在测试时画的图，所以只有一幅，但一般我们在训练阶段使用，用于寻找模型的问题~~

任务代码：

```text
# 显示每个layer的权重
loss = 10   # 第0层
for i, (name, param) in enumerate(model.named_parameters()):
    if 'bn' not in name:
        writer.add_histogram(name, param, 0)
        writer.add_scalar('loss', loss, i)
        loss = loss*0.5
```

效果图：

![img](https://pic3.zhimg.com/80/v2-81eb3a7f2f323f026e93fec55c5dcef6_720w.jpg)

![img](https://pic1.zhimg.com/80/v2-1e6bffefb6cd859b7c10c30d24c544e0_720w.jpg)

**第二个任务：feature map的可视化和卷积核的可视化**

- add_image(tag, img_tensor, global_step=None, walltime=None) 绘制图片
- torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0) 制作网格用于显示

我们把每个卷积层后的feature map来进行可视化，一共有4个卷积层，所以我们将要绘制4个阶段的feature map，由于我们的模型已经训练好了，所以可视化出来的feature map可以很好地解释我们训练出来的模型到底是什么，每个卷积层提取的特征是什么？当然只能部分解释~

任务代码：

```text
img_grid = vutils.make_grid(x, normalize=True, scale_each=True, nrow=2)
# 绘制原始图像
writer.add_image('raw img', img_grid, global_step=666)  # j 表示feature map数
print(x.size())

model.eval()
for name, layer in model._modules.items():

    # 为fc层预处理x
    x = x.view(x.size(0), -1) if "fc" in name else x
    print(x.size())

    x = layer(x)
    print(f'{name}')

    # 第一个卷积没有进行relu，要记得加上
    x = F.relu(x) if 'conv' in name else x
    if  'layer' in name or 'conv' in name:
        x1 = x.transpose(0, 1)  # C，B, H, W  ---> B，C, H, W
        img_grid = vutils.make_grid(x1, normalize=True, scale_each=True, nrow=4)  # normalize进行归一化处理
        writer.add_image(f'{name}_feature_maps', img_grid, global_step=0)
```

效果图：

![img](https://pic4.zhimg.com/80/v2-61a43a44d782939c7a0ce5c2f6b43413_720w.jpg)Cifar 10里面的一张原图

![img](https://pic2.zhimg.com/80/v2-03017b35be8cd5bcca03c664d06fea7d_720w.jpg)conv/layer 1/layer 2/layer 3四个层提取的feature map

我们可以清晰的看到，在网络的底层，我们可以看到船的边缘特征，由于归一化了，所以色彩特征不太明显，当然也可以不进行归一化，但是layer3这种高层特征我们就能以解释了，它代表更多的是语义特征，很抽象！

卷积核的可视化与这个类似，其实质也就是权重可视化，代码如下：

```text
# 可视化卷积核
#  in model.named_parameters():
if 'conv' in name and 'weight' in name:
in_channels = param.size()[1]
out_channels = param.size()[0]   # 输出通道，表示卷积核的个数

k_w, k_h = param.size()[3], param.size()[2]   # 卷积核的尺寸
kernel_all = param.view(-1, 1, k_w, k_h)  # 每个通道的卷积核
kernel_grid = vutils.make_grid(kernel_all, normalize=True, scale_each=True, nrow=in_channels)
writer.add_image(f'{name}_all', kernel_grid, global_step=0) 
```

效果图（conv层和layer 1层）：

![img](https://pic2.zhimg.com/80/v2-d081f6858c95f07564c690cfb8912d49_720w.jpg)

**第三个任务：feature map的可视化和卷积核的可视化**

这个不知道为什么，我使用Pytorch自带的网络模型就没有问题，但是自己定义的网络进行模型图显示就会出现一些莫名其妙的问题，**有大佬知道的请告知原因啊！**这里面我们使用ResNet18的结构来进行可视化，很简单，就一句话：

```text
model = torchvision.models.resnet18(False)
writer.add_graph(model, torch.rand([1,3,224,224]))   # 自己定义的网络有时显示错误
```

效果图：

![img](https://pic2.zhimg.com/80/v2-19c5cbf5488cd59f51770df4a09b8f0d_720w.jpg)

**步骤：**

**开始前，先确保安装成功**

```
pip install tensorboardX
pip install tensorboard
pip install tensorflow
```

***第一步import tensorboardX （如何使用tensorboardX请参考reference）：***

```
from tensorboardX import SummaryWriter
```

***第二步初始化一个 SummaryWriter实例***：

```
writer = SummaryWriter()
```

***第三步：***

```
%load_ext tensorboard.notebook

or (版本差异，自行选择)
%load_ext tensorboard
```

***第四步把你模型需要在tensorboard中可视化的参数写入writer，每次写入 图像名称，loss/accracy数值， n_iteration***

```
writer.add_scalar('Train/Loss', train_loss, epoch)
writer.add_scalar('Train/Accuracy', train_acc, epoch) 
writer.add_scalar('Test/Loss', test_loss, epoch)
writer.add_scalar('Test/Accuracy', test_acc, epoch)
```

 ***第五步：***

```
%tensorboard --logdir ./
```

## 7. 保存节点

[参考地址](https://zhuanlan.zhihu.com/p/148159709)



```
# checkpoint
checkpoint = {"model_state_dict": model.state_dict(),
				"optimizer_state_dict": optimizer.state_dict(),
				"epoch": epoch}
path_checkpoint = config.log_path+"/checkpoint_{}_epoch.pkl".format(epoch)
torch.save(checkpoint, path_checkpoint)

# 恢复
path_checkpoint = "./checkpoint_4_epoch.pkl"
checkpoint = torch.load(path_checkpoint)

net.load_state_dict(checkpoint['model_state_dict'])

optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

start_epoch = checkpoint['epoch']

scheduler.last_epoch = start_epoch
```

## 8. 调参

[参考地址](https://tsinghua-gongjing.github.io/posts/DL-tricks.html)

- [目录](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#目录)
- [调参](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#调参)
- [大方向](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#大方向)
- 数据
  - [预处理](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#预处理)
- 模型本身
  - [参数初始化方法](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#参数初始化方法)
  - [隐藏层的层数](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#隐藏层的层数)
  - [节点数目](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#节点数目)
  - [filter](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#filter)
  - [激活函数的选取](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#激活函数的选取)
  - [dropout](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#dropout)
- [损失函数](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#损失函数)
- 训练相关
  - [学习速率](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#学习速率)
  - [batch size大小](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#batch-size大小)
  - [momentum大小](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#momentum大小)
  - [迭代次数](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#迭代次数)
  - [优化器](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#优化器)
  - [残差块和BN](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#残差块和bn)
- [案例：人脸特征点检测](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#案例人脸特征点检测)
- [参考](https://tsinghua-gongjing.github.io/posts/DL-tricks.html#参考)