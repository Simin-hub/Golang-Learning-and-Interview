# 项目相关问题

## 业务相关

### 禁止多次投票

三种方法：

1. 投票后把对方IP记住，然后再投的时候就去查询一下，如果有他的记录就提示不能投了。（即用数据库进行记录，提高效率，可以使用组合索引）
2. 使用Session，每个用户在登陆的时候都有一个SessID,对方投票后，给他一个新的session变量，证明他投票过了，那么下次投就不行了。
3. 使用cookie，对方投票后，就写入他电脑的cookie，程序去读取判断他投票没有

弊端：

1. 要频繁读去数据库，可能有时候存在不合理性。 但这个方法最合理
2. 对方关掉IE后重开一次再投，还能投，所以不是特别完善。
3. 清空Cookie后又能投

### 重复登录验证

登录的时候request会带一个sessionId过去，这是每个单独的访问特有的（我理解的是每个浏览器相当于单独的会话，如果你在这个浏览器登录了，再开别的标签页系统也是会认为你已经登录过了，原因是sessionid是一致的），那么我们需要保存sessionid和当前登录的用户信息就可以实现了。

大体的思路是这样的：

1. 在登录的接口中，用当前登录的userid去获取缓存中的sessionId，如果和当前访问的sessionId一致，那么放行。如果不一致，删除原缓存中的sessionId，换上新的sessionId。
2. 在登录拦截器中，获取用户信息，（如果获取不到，跳转到登录页，这个是常规操作）通过用户信息(userid) 在缓存中获得sessionId，比较获取的sessionId和本次拦截的sessionId，如果不一致，拦截请求，提示用户账号被顶替了。
3. 提示页跳转至登录页面（过一遍后台走一步登出流程）

### 实现单点登录

实现单点登录的思路很简单，就是**一个账号对应的token同一时间只有一个生效**，也就是说每次用户登录除了生成一个token保存起来，还要删除掉之前的token。

1. 用户登录，服务器生成token保存至 `redis` (设置有效时间)，并将token返回给前端，用户之后的每次请求需要携带token。
2. 在拦截器中校验请求头中的token，判断token是否有效，每一次有效的请求都刷新token的有效时间。
3. 同一个账号再次被登录时，删除之前的token，并生成新的token重复1操作。

步骤看起来都很简单，但有一个问题需要注意，每一次校验token是需要去`redis`中查询的，也就是说设置的key应该为token，value为`userId`(或者其它唯一标识)，那么这个时候如何在登录时做到删除当前的token呢？如果只是有一个token-id的对应关系好像确实没办法获取到该账号当前token，所以还需要一个id-token的对应关系，可以直接通过id拿到token。

### 评论回复功能

[参考](https://blog.csdn.net/ztchun/article/details/71106117)

#### 数据表设计

###### 1 一问一答模式

**(1)需求分析**

大部分APP采用简单的评论设计即可，即是一问一答模式，比如微信朋友圈的评论功能的设计。如：

```xml
A：今天天气真好！
B @ A :今天天气确实不错！12
```

这种设计简单、直接，也满足了用户评论、回复的基本要求，对于没有大量用户评论的APP需求足够。

**（2）数据库设计**
这种场景下一般评论较少，评论不活跃，可以不区分评论和回复，统一看成评论。区别是，有些评论是直接评论主题，而有些是@其他用户，使用一张表就可以达到效果，评论表设计如下：

| 表字段     | 字段说明       |
| ---------- | -------------- |
| id         | 主键           |
| topic_id   | 主题id         |
| topic_type | 主题类型       |
| content    | 评论内容       |
| from_uid   | 评论用户id     |
| to_uid     | 评论目标用户id |

topic_type：为了能复用评论模块，我们引入这个字段来区分主题的类别。

from_uid：表示评论人的id，通过该id我们可以检索到评论人的相关信息。

to_uid 是评论目标人的id，如果没有目标人，则该字段为空

出于性能的考虑，往往我们会冗余评人的相关信息到评论表中，比如评论人的nick、头像，目标用户也是如此。 这样一来我们就只用查询单表就可以达到显示的效果

有时，目标用户有多个，那么可以将to_uid字段修改为to_uids，保存时用分隔符来分割用户id，而目标用户的信息再去查询缓存或者数据库。也可以简单的将多个目标用户的信息一起存成json格式，可以应付简单的展现需求。

###### 2 评论为主模式

**(1)需求分析**

如果以评论为主的显示模式，类似于下面的CSDN的评论显示模式：
![这里写图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20170503021558563)

这里将评论分为评论和回复，所有评论均挂在评论下面，类似于**树状结构**。

**（2）数据库设计**

在以评论为主的树形显示情况下，数据库的设计十分灵活，可以使用单表，添加一个parent_id字段来指向父评论，需要嵌套查询。

同时也可以**将评论拆分为评论表和回复表**，评论挂在各种主题下面，而回复挂在评论下面。

评论表设计如下：

| 表字段     | 字段说明   |
| ---------- | ---------- |
| id         | 主键       |
| topic_id   | 主题id     |
| topic_type | 主题类型   |
| content    | 评论内容   |
| from_uid   | 评论用户id |

回复表设计：

| 表字段     | 字段说明   |
| ---------- | ---------- |
| id         | 主键       |
| comment_id | 评论id     |
| reply_id   | 回复目标id |
| reply_type | 回复类型   |
| content    | 回复内容   |
| from_uid   | 回复用户id |
| to_uid     | 目标用户id |

由于我们拆分了评论和回复，那么评论表就不再需要目标用户字段了，因为评论均是用户对主题的评论，评论表的设计更佳简洁了。

回复表添加了一个comment_id字段来表示该回复挂在的**根评论id**，这样设计也是出于性能方面的考虑，我们可以直接通过评论id一次性的找出该评论下的所有回复，然后通过程序来编排回复的显示结构。 通过适当的冗余来提高性能也是常用的优化手段之一。

reply_type：表示回复的类型，因为回复可以是针对评论的回复(comment)，也可以是针对回复的回复(reply)， 通过这个字段来区分两种情景。

reply_id：表示回复目标的id，如果reply_type是comment的话，那么reply_id＝commit_id，如果reply_type是reply的话，这表示这条回复的父回复。

###### 3 网易新闻盖楼模式

**（1）需求分析**

这种场景中评论和回复是同级显示的，回复不在显示结构上不用挂在一个评论下面。 双表的设计在这里就不太合适了，因为涉及到评论和回复的混排，使用双表则会导致查询的逻辑过于复杂。 所以建议还是采用单表的设计，**不区分评论和回复会简化应用层的逻辑**。 我们统一都看成评论，而有些评论是可以引用其他评论的。

**（2）数据库设计**

本人推荐采用闭包表的设计，例如：

comment表设计：

| 表字段     | 字段说明   |
| ---------- | ---------- |
| id         | 主键       |
| topic_id   | 主题id     |
| topic_type | 主题类型   |
| content    | 评论内容   |
| from_uid   | 评论用户id |

parent_child表：

| 表字段    | 字段说明 |
| --------- | -------- |
| id        | 主键     |
| parent_id | 父id     |
| child_id  | 子id     |

comment表保存所有评论内容，而**parent_children表则记录评论表中各个评论的父子关系**。

查询时往往会按照时间排序，我们可以直接按id或者创建时间降序排列查询comment表即可。 如果用户想查询一条评论的完整引用，则可以通过parent_children来找到对应的路径。

闭包表在查询时非常方便，但是插入的性能稍差，因为除了插入评论表以外，还需要把该条评论所有的父子关系插入到父子关系表中。 插入性能会随着评论层级的加深而线性下降。

#### 数据库优化

如果你的系统每天都会拥有成千上万条评论，那么单表的设计肯定是不行，优化的方式有以下几种思路。

（1）**分库分表**。 分库分表是最为常用也最有效的优化方式，建议**按照主题来分库分表**。 这样同一个主题下面的评论就会落到同一张表里，避免了跨表查询。

（2）**适当的数据冗余**。 如果你需要显示评论人的相关信息，那么在插入评论时就把这些信息写入评论表中，避免多次查询。 实际上，如果是纪录数据，都可以冗余对应的数据信息，因为它们的数据的实时行和一致性要求并不高。

（3）**附加幂。数据只允许单项操作**。 因为从幂性的要求来说，每个赞全都是一条记录。 评论的赞数如果都从点赞表中统计得出，那么性能开销会十分巨大，而且点赞如此轻量级的一个操作一定会加剧点赞表的竞争操作。 所以建议直接在评论表中添加一个like_count的计数器，该字段只增不减。客户端，可以设置取消效果。

（4）**热门评论加缓存**。 类似于网易新闻的热门评论，读取频度非常高，可以专门开接口做缓存。

### 请求量这么高该如何优化

[参考](https://juejin.cn/post/7113730074595541023)

#### 本地缓存

当我们**遇到极端热点数据查询的时候，这个时候就要考虑本地缓存了**。热点本地缓存主要部署在应用服务器的代码中，用于阻挡热点查询对于Redis等分布式缓存或者数据库的压力。

### 如何处理每秒上万次的下单请求

[参考](https://juejin.cn/post/7114845947607121956)

#### 处理热点数据

秒杀的数据通常都是热点数据，处理热点数据一般有几种思路：一是优化，二是限制，三是隔离。

##### 优化

优化热点数据最有效的办法就是**缓存热点数据**，我们可以把热点数据缓存到内存缓存中。

##### 限制

限制更多的是一种保护机制，当秒杀开始后用户就会不断地刷新页面获取数据，这时候我们可以**限制单用户的请求次数**，比如一秒钟只能请求一次，超过限制直接返回错误，返回的错误尽量对用户友好，比如 "店小二正在忙" 等友好提示。

##### 隔离

秒杀系统设计的第一个原则就是**将这种热点数据隔离出来**，不要让1%的请求影响到另外的99%，隔离出来后也更方便对这1%的请求做针对性的优化。具体到实现上，我们需要做服务隔离，即秒杀功能独立为一个服务，通知要做数据隔离，秒杀所调用的大部分是热点数据，我们需要**使用单独的Redis集群和单独的Mysql**，目的也是不想让1%的数据有机会影响99%的数据。

#### 流量削峰

针对秒杀场景，它的特点是在秒杀开始那一刹那瞬间涌入大量的请求，这就会导致一个特别高的流量峰值。但最终能够抢到商品的人数是固定的，也就是不管是100人还是10000000人发起请求的结果都是一样的，并发度越高，无效的请求也就越多。但是从业务角度来说，秒杀活动是希望有更多的人来参与的，也就是秒杀开始的时候希望有更多的人来刷新页面，但是真正开始下单时，请求并不是越多越好。因此我们可以设计一些规则，让并发请求更多的延缓，甚至可以过滤掉一些无效的请求。

**削峰本质上是要更多的延缓用户请求的发出，以便减少和过滤掉一些无效的请求，它遵从请求数要尽量少的原则**。我们最容易想到的解决方案是用消息队列来缓冲瞬时的流量，把同步的直接调用转换成异步的间接推送，中间通过一个队列在一端承接瞬时的流量洪峰，在另一端平滑的将消息推送出去，如下图所示：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50232db2fd08447eaebf6b3067af3869~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

采用消息队列异步处理后，那么秒杀的结果是不太好同步返回的，所以我们的思路是当用户发起秒杀请求后，同步返回响应用户 "秒杀结果正在计算中..." 的提示信息，当计算完之后我们如何返回结果给用户呢？其实也是有多种方案的。

- 一是**在页面中采用轮询的方式定时主动去服务端查询结果**，例如每秒请求一次服务端看看有没有处理结果，这种方式的缺点是服务端的请求数会增加不少。
- 二是**主动push的方式，这种就要求服务端和客户端保持长连接**了，服务端处理完请求后主动push给客户端，这种方式的缺点是服务端的连接数会比较多。

还有一个问题就是如果异步的请求失败了该怎么办？我觉得**对于秒杀场景来说，失败了就直接丢弃就好**了，最坏的结果就是这个用户没有抢到而已。如果想要尽量的保证公平的话，那么失败了以后也可以做重试。

#### 如何保证消息只被消费一次

**kafka是能够保证"At Least Once"的机制的，即消息不会丢失，但有可能会导致重复消费**，消息一旦被重复消费那么就会造成业务逻辑处理的错误，那么我们如何避免消息的重复消费呢？

我们只要保证即使消费到了重复的消息，从消费的最终结果来看和只消费一次的结果等同就好了，也就是保证在消息的生产和消费的过程是幂等的。什么是幂等呢？如果我们消费一条消息的时候，要给现有的库存数量减1，那么如果消费两条相同的消息就给库存的数量减2，这就不是幂等的。而如果消费一条消息后处理逻辑是将库存的数量设置为0，或者是如果当前库存的数量为10时则减1，这样在消费多条消息时所得到的结果就是相同的，这就是幂等的。说白了就是一件事无论你做多少次和做一次产生的结果都是一样的，那么这就是幂等性。

我们可以在消息被消费后，**把唯一id存储在数据库中，这里的唯一id可以使用用户id和商品id的组合**，在处理下一条消息之前先从数据库中查询这个id看是否被消费过，如果消费过就放弃。伪代码如下：

```go
isConsume := getByID(id)
if isConsume {
  return  
} 
process(message)
save(id)
```

还有一种方式是通过数据库中的唯一索引来保证幂等性，不过这个要看具体的业务，在这里不再赘述。

### 极致优化秒杀性能

[参考](https://juejin.cn/post/7116303230841126926)

#### 批量数据聚合

在**SeckillOrder**这个方法中，每来一次秒杀抢购请求都往往Kafka中发送一条消息。假如这个时候有一千万的用户同时来抢购，就算我们做了各种限流策略，一瞬间还是可能会有上百万的消息会发到Kafka，会产生大量的网络IO和磁盘IO成本，大家都知道Kafka是基于日志的消息系统，写消息虽然大多情况下都是顺序IO，但当海量的消息同时写入的时候还是可能会扛不住。

那怎么解决这个问题呢？答案是做**消息的聚合**。**之前发送一条消息就会产生一次网络IO和一次磁盘IO，我们做消息聚合后，比如聚合100条消息后再发送给Kafka，这个时候100条消息才会产生一次网络IO和磁盘IO，对整个Kafka的吞吐和性能是一个非常大的提升**。其实这就是一种小包聚合的思想，或者叫Batch或者批量的思想。这种思想也随处可见，比如我们使用Mysql插入批量数据的时候，可以通过一条SQL语句执行而不是循环的一条一条插入，还有Redis的Pipeline操作等等。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/328a9518f3544f22aef1ea32085e5c2b~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

那怎么来聚合呢，聚合策略是啥呢？**聚合策略有两个维度分别是聚合消息条数和聚合时间**，比如聚合消息达到100条我们就往Kafka发送一次，这个条数是可以配置的，那如果一直也达不到100条消息怎么办呢？通过聚合时间来兜底，这个聚合时间也是可以配置的，比如配置聚合时间为1秒钟，也就是无论目前聚合了多少条消息只要聚合时间达到1秒，那么就往Kafka发送一次数据。聚合条数和聚合时间是或的关系，也就是只要有一个条件满足就触发。

#### 降低消息的消费延迟

通过批量消息处理的思想，我们提供了Batcher工具，提升了性能，但这主要是针对生产端而言的。当我们消费到批量的数据后，还是需要串行的一条条的处理数据，那有没有办法能加速消费从而降低消费消息的延迟呢？有两种方案分别是：

- **增加消费者的数量**
- **在一个消费者中增加消息处理的并行度**

因为在Kafka中，一个Topci可以配置多个Partition，数据会被平均或者按照生产者指定的方式写入到多个分区中，那么在消费的时候，Kafka约定一个分区只能被一个消费者消费，为什么要这么设计呢？我理解的是如果有多个Consumer同时消费一个分区的数据，那么在操作这个消费进度的时候就需要加锁，对性能影响比较大。所以说当消费者数量小于分区数量的时候，我们可以增加消费者的数量来增加消息处理能力，但当消费者数量大于分区的时候再继续增加消费者数量就没有意义了。

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a9023773ad74840a35b45b66522c83b~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

不能增加Consumer的时候，可以在同一个Consumer中提升处理消息的并行度，即通过多个goroutine来并行的消费数据，我们一起来看看如何通过多个goroutine来消费消息。

#### 怎么保证不会超卖

当秒杀活动开始后，大量用户点击商品详情页上的秒杀按钮，会产生大量的并发请求查询库存，一旦某个请求查询到有库存，紧接着系统就会进行库存的扣减。然后，系统生成实际的订单，并进行后续的处理。如果请求查不到库存，就会返回，用户通常会继续点击秒杀按钮，继续查询库存。简单来说，这**个阶段的操作就是三个：检查库存，库存扣减、和订单处**理。因为**每个秒杀请求都会查询库存**，而请求只有查到库存有余量后，后续的库存扣减和订单处理才会被执行，所以，这个阶段中**最大的并发压力都在库存检查操作**上。

为了支撑大量高并发的库存检查请求，我们需要使用Redis单独保存库存量。那么，库存扣减和订单处理是否都可以交给Mysql来处理呢？其实，订单的处理是可以在数据库中执行的，但库存扣减操作不能交给Mysql直接处理。因为到了实际的订单处理环节，请求的压力已经不大了，数据库完全可以支撑这些订单处理请求。那为什么库存扣减不能直接在数据库中执行呢？这是因为，一旦请求查到有库存，就意味着该请求获得购买资格，紧接着就会进行下单操作，同时库存量会减一，这个时候如果直接操作数据库来扣减库存可能就会导致超卖问题。

直接操作数据库扣减库存为什么会导致超卖呢？由于**数据库的处理速度较慢，不能及时更新库存余量，这就会导致大量的查询库存的请求读取到旧的库存值，并进行下单**，此时就会出现下单数量大于实际的库存量，导致超卖。所以，就需要**直接在Redis中进行库存扣减**，具体的操作是，当库存检查完后，一旦库存有余量，我们就立即在Redis中扣减库存，同时，为了避免请求查询到旧的库存值，**库存检查和库存扣减这两个操作需要保证原子性**。

### 怎么判别淘宝刷单？怎么检验手段效果？

**商家角度：**

- 营业额远高于历史平均数据
- 营业额远高于行业平均数据
- 下单的用户很多均为存在异常行为被监控的账户

**顾客角度（账号维度）**：判断是否存在异常行为

- 单次交易行为：是否精准搜索、货比三家、页面停留时间等
- 近期购物成功率：远高于历史时期平均购物成功率
- 是否可能为垫付：短时间支付宝账户内收到与下单金额相同的金额
- 下单的店铺很多均为存在异常行为被监控的账户

**商家与顾客：**

- 存在相近或者共同的网络环境：如ip、wifi等

### 如何把一个文件快速下发到 100w 个服务器？

**采用p2p网络形式**，比如树状形式，网状形式，**单个节点既可以从其他节点接收服务又可以向其他节点提供服务**。

对于树状传递，在100W台服务器这种量级上，可能存在两个问题：

- 如果树上的某一个节点坏掉了，那么从这个节点往下的所有服务器全部宕机。
- 如果树中的某条路径，传递时间太长了（网络中，两个节点间的传递速度受很多因素的影响，可能相差成百上千倍），使得传递效率退化。

**改进**：**100W台服务器相当于有100W个节点的连通图。那么我们可以在图里生成多颗不同的生成树，在进行数据下发时，同时按照多颗不同的树去传递数据**。这样就可以避免某个中间节点宕机，影响到后续的节点。同时这种传递方法实际上是一种依据时间的广度优先遍历，可以避免某条路径过长造成的效率低下。

## 海量数据处理

[参考](https://blog.csdn.net/v_july_v/article/details/6685962)

### TOP K问题

[参考](https://blog.csdn.net/WB18002337539/article/details/120468027)

#### 10亿个数据中找出最大的10000个？——最小堆

**最小堆法**

> 1. 先拿10000个数建堆
> 2. 然后逐个添加剩余元素
> 3. 如果大于堆顶的数（10000中最小的），将这个数替换堆顶，并调整结构使之仍然是一个最小堆
> 4. 遍历完后，堆中的10000个数就是所需的最大的10000个。
>
> - **复杂度分析**：时间复杂度是O（mlogm），算法的时间复杂度为O（nmlogm）（n为10亿，m为10000）。

**优化方法**

- 如果内存受限：可以直接**在内存总使用Hash方法将数据划分成n个partition，每个partition交给一个线程处理，线程的处理逻辑可以采用最小堆，最后一个线程将结果归并**。

  > 进一步优化：
  > 该方法存在一个瓶颈会明显影响效率，即数据倾斜。每个线程的处理速度可能不同，快的线程需要等待慢的线程，最终的处理速度取决于慢的线程。而针对此问题
  > **解决的方法**是，**将数据划分成c×n个partition（c>1），每个线程处理完当前partition后主动取下一个partition继续处理，直到所有数据处理完毕，最后由一个线程进行归并**。

- **如果含较多重复值**：先用hash / 依图法去重，可大大节省运算量

------

#### 有几台机器存储着几亿淘宝搜索日志，你只有一台 2g 的电脑，怎么选出搜索热度最高的十个？

针对top k类文本问题，通常比较好的方案是【分治+trie树/hash+小顶堆】，即**先将数据集按照hash方法分解成多个小数据集，然后使用trie树（前缀树或字典树）或者hash统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出频率最高的前K个数，最后在所有top K中求出最终的top K**。

- **拆分成n多个文件**：**以首字母区分，不同首字母放在不同文件，长度仍过长的继续按照次首字母进行拆分**。这样一来，每个文件的每个数据长度相同且首字母尾字母也相同，就能保证数据被独立的分为了n个文件，且各个文件中不存在关键词的交集。
- **分别词频统计**：对于每个文件，使用`hash或者Trie树`进行进行词频统计
- **小顶堆排序**：依次处理每个文件，并逐渐更新最大的十个词

#### 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

方案1：

1. 顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的)。
2. 找一台内存在2G左右的机器，依次对文件内容用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件。
3. 对这10个文件进行归并排序（内排序与外排序相结合)。

方案2：

  一般query的总量是有限的，只是重复的次数比较多而已，可能**对于所有的query，一次性就可以加入到内存**了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了

  （**读者反馈**@店小二：原文第二个例子中：“找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。”由于query会重复，作为key的话，应该使用hash_multimap 。hash_map 不允许key重复。@hywangw:店小二所述的肯定是错的，hash_map(query,query_count)是用来统计每个query的出现次数 又不是存储他们的值 出现一次 把count+1 就行了 用multimap干什么？多谢hywangw）。

方案3：

  与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。

#### 提取某日访问次数最多的IP地址

将去进行 `%1024` 取模散列到1024个文件中**（划分子文件方法之一）**, 采用hashmap对其进行次数统计 最后用排序算法进行排序。

------

#### 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

  方案1：顺序读文件中，对于每个词x，使用hash映射，然后存到5000个小文件中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并**取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序)的过程了**。

#### 数据太多，内存不够，找到最热门的K个数据

首先对数据进行预处理，采用hash表统计次数 然后再排序

1. 进行预处理，删除不必要的数据
2. 采用 hash 映射进行分割成多个小文件（进一步优化则使用多线程处理，在排序阶段使用一个线程进行归并）
3. 然后使用 hashmap/tried tree 进行统计
4. 使用排序算法（堆/快速排序）进行排序

### 海量数据排序、压缩问题？

[参考](https://blog.csdn.net/WB18002337539/article/details/120468027)

#### 重要方法——位图法 Bitmap

- **位图的基本概念**：**用一个位（bit）来标记某个数据的存放状态**。例如，有`{2, 4, 5, 6, 67, 5}`这么几个整数，我维护一个 `00…0000（共67位）`的`0/1`字符串，1表示该索引（=数据值）处存在数，0则表示不存在。
- **应用**：位图法可以**用于海量数据排序，海量数据去重，海量数据压缩**
- **优点**：**针对于稠密的数据集可以很好体现出位图法的优势**，内存消耗少，速度较快
- **缺点**：**不适用于稀疏数据集**，比如我们有一个长度为10的序列，最大值为20亿，则构造位串的内存消耗将相当大250M，而实际却只需要40个字节，此外位图法还存在可读性差等缺点。

------

#### 非重复排序

假设我们有一个不重复的整型序列`{n1， n2， ... ,nn}`，假设最大值为`nmax`，则我们可以维护一个长度为`nmax`的位串。主要过程就是2步：

- 第一遍遍历整个序列，将出现的数字在位串（java中可以用数组实现）中对应的位置置为1；
- 第二遍遍历位图，依次输出值为1的位对应的数字，这些1所在的位串中的位置的索引代表序列数据，1出现的先后位置则代表序列的大小关系。

------

#### 重复排序——保留 / 不保留重复值

- **保留重复值**：同上，只是子串中不只存在`0/1`，实际数量为多少，则值为多少.输出时，值为多少则输出多少遍
- **不保留**：略。

#### 在2.5亿个整数中找出不重复的整数，内存不足以容纳这2.5亿个整数。

  方案1：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32*2bit=1GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。

  方案2：也可采用上题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。

### 数据压缩

**前提**：数据中存在大量的冗余值

基本思路就是使用某个子串存储原数据中的海量值



#### 如何用redis存储统计1亿用户一年的登陆情况，并快速检索任意时间窗口内的活跃用户数量

在redis 2.2.0版本之后，新增了一个位图数据。redis单独对bitmap提供了一套命令。可以对任意一位进行设置和读取。所以可以在位图中使用1表示活跃。

bitmap的核心命令：

- `SETBIT`：设置某位为1
  语法：`SETBIT key offset value`
  例如：

  ```java
  setbit abc 5 1 ----> 00001
  setbit abc 2 1 ----> 00101
  12
  ```

- `GETBIT`：获取某位的值
  语法：`GETBIT key offset`
  例如：

  ```java
  getbit abc 5 ----> 1
  getbit abc 1 ----> 0
  12
  ```

bitmap的其他命令还有`bitcount，bitpos，bitop`等命令。都是对位的操作。

- 获取某一天id为88000的用户是否活跃：`getbit 2020-01-01 88000 [时间复杂度为O(1)]`
- 统计某一天的所有的活跃用户数：`bitcount 2019-01-01 [时间复杂度为O(N)]`
- 统计某一段时间内的活跃用户数，需要用到bitop命令。这个命令提供四种位运算，AND(与)，(OR)或，XOR(亦或)，NOT(非)。**我们可以对某一段时间内的所有key进行OR(或)操作，或操作出来的位图是0的就代表这段时间内一次都没有登陆的用户**。那只要我们求出1的个数就可以了

以下例子求出了2019-01-01到2019-01-05这段时间内的活跃用户数。

```java
bitop or result 2019-01-01 2019-01-02 2019-01-03 2019-01-04 2019-01-05 [时间复杂度为O(N)]
bitcount result
12
```

从时间复杂度上说，无论是统计某一天，还是统计一段时间。在实际测试时，基本上都是秒出的。符合我们的预期。

### 查找

#### 1. 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

 方案1：可以估计每个文件安的大小为50G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

1. 遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件。这样每个小文件的大约为300M。
2. 遍历文件b，采取和a相同的方式将url分别存储到1000小文件中。这样处理后，所有可能相同的url都在对应的小文件中(相同的URL hash 映射相同)，不对应的小文件不可能有相同的url。然后我们只要求出**1000对小文件**中相同的url即可。
3. 求每对小文件中相同的url时，可以**把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中**，如果是，那么就是共同的url，存到文件里面就可以了。

  方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。

  **读者反馈**@crowgns：

1. **hash后要判断每个文件大小，如果hash分的不均衡有文件较大，还应继续hash分文件，换个hash算法第二次再分较大的文件，一直分到没有较大的文件为止**。这样文件标号可以用A1-2表示（第一次hash编号为1，文件较大所以参加第二次hash，编号为2）
2. 由于1存在，第一次hash如果有大文件，不能用直接set的方法。建议对每个文件都先用字符串自然顺序排序，然后具有相同hash编号的（如都是1-3，而不能a编号是1，b编号是1-1和1-2），可以直接从头到尾比较一遍。对于层级不一致的，如a1，b有1-1，1-2-1，1-2-2，层级浅的要和层级深的每个文件都比较一次，才能确认每个相同的uri。

####  一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数中的中数？

  方案1：先大体估计一下这些数的范围，比如这里假设这些数都是32位无符号整数（共有2^32个）。我们把0到2^32-1的整数**划分为N个范围段**，每个段包含（2^32）/N个整数。比如，第一个段位0到2^32/N-1，第二段为（2^32）/N到（2^32）/N-1，…，第N个段为（2^32）（N-1）/N到2^32-1。然后，扫描每个机器上的N个数，把属于第一个区段的数放到第一个机器上，属于第二个区段的数放到第二个机器上，…，属于第N个区段的数放到第N个机器上。注意这个过程每个机器上存储的数应该是O(N)的。**下面我们依次统计每个机器上数的个数，一次累加，直到找到第k个机器，在该机器上累加的数大于或等于（N^2）/2，而在第k-1个机器上的累加数小于（N^2）/2，并把这个数记为x**。那么我们要找的中位数在第k个机器中，排在第（N^2）/2-x位。然后我们对第k个机器的数排序，并找出第（N^2）/2-x个数，即为所求的中位数的复杂度是O（N^2）的。

  方案2：先对**每台机器上的数进行排序。排好序后，我们采用归并排序的思想，将这N个机器上的数归并起来得到最终的排序**。找到第（N^2）/2个便是所求。复杂度是O（N^2*lgN^2）的。

### 海量文本去重——simhash法

[参考文章](https://cloud.tencent.com/developer/article/1379302?from=14588)

#### simhash计算

给定一篇文章内容，利用simhash算法可以计算出一个哈希值（64位整形）。

判别两篇文章是相似的方法，就是两个simhash值的距离<=3，这里距离计算采用**汉明距离，也就是2个simhash做一下异或运算，数一下比特位=1的有N位，那么距离就是N**。

## 资源 vs 请求问题

[参考](https://blog.csdn.net/WB18002337539/article/details/120468027)

### 如果一个外卖配送单子要发布，现在有200个骑手都想要接这一单，如何保证只有一个骑手接到单子？

1. 采用redis，zookeeper分布式锁加锁。
2. 消息队列 实现幂等接口

### 多个微信用户抢红包

类似于秒杀系统

1. 数据库加乐观锁、悲观锁
2. 在逻辑处理界面加分布式锁
3. 消息队列

------

### ❓1000个任务分给10个人做

全局队列，每一个人都从一个队列中取

分成10个队列对应每一个人