# 分布式

## 一致性哈希

[参考](https://segmentfault.com/a/1190000015336117)、[参考](https://segmentfault.com/a/1190000040373119)

国际惯例，先上源码 [https://github.com/manerfan/c...](https://link.segmentfault.com/?enc=TzTNCZlpIaNSmOPeBmu5Eg%3D%3D.a%2BCe%2B2b6HtSTVbX6qiogX30rkNudwn8ugaWl5AKFqfGD6LKPXNBk9Kd%2Bbdim4iS8)

1. 可自定义节点数据类型
2. 可自定义hash函数

### 原理

一致性哈希可应用于**负载均衡、分布式缓存**(缓存分片)等场景中，以下以分布式缓存为例

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040373121)

#### 传统方式

所以本质来讲：我们需要一个可以将**输入值“压缩”并转成更小的值，这个值通常状况下是唯一、格式极其紧凑的，比如uint64**：

- 幂等：每次用同一个值去计算 hash 必须保证都能得到同一个值

这个就是 `hash` 算法完成的。

如，现有N个缓存实例，将一个对象object映射到某一个缓存上可以采取取模方式 `hash(object) % N`
我们称之为简单hash算法。一般，简单hash算法确实能够比较均匀地实现分布式映射，但如果考虑缓存实例变动（增删）的情况：

1. 某一缓存实例宕机，需要将该实例从集群中摘除，则映射公式变为 `hash(object) % (N - 1)`
2. 增加一台缓存实例，将该实例加入集群，则映射公式变为 `hash(object) % (N + 1)`

对于以上情况，无论新增还是移除，大部分object所映射的缓存实例均会改变，缓存命中率大幅度降低从而回源到服务器，短时间内造成缓存雪崩现象

此时就需要引入 `consistent hash` 算法了。

### 一致性哈希

一致性 Hash 算法简单的说，**在移除/添加一个缓存实例时**，**尽可能小的改变已存在key映射关系**，尽可能的满足单调性的要求。

#### 1. 环形空间

通常的hash算法都是将一个value映射到一个32位的key值，我们可以将这个[0, 2^32-1]空间想象成一个首尾相接的环形队列

![consistent_hashing.001.jpeg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbcu8S)

#### 2. 将对象映射到hash空间

通过hash函数计算对象hash值，将对象映射到hash环形空间

![consistent_hashing.002.jpeg](https://segmentfault.com/img/bVbcu8V?w=1024&h=768)

#### 3. 将缓存实例映射到hash空间

使用缓存实例的ip、port等信息，通过hash函数计算其hash值，将缓存实例映射到hash空间

![consistent_hashing.004.jpeg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbcvC4)

#### 4. 将对象映射到缓存实例

沿着顺时针方向，查找距离对象object最近的缓存实例，并将对象映射到该实例

![consistent_hashing.003.jpeg](https://segmentfault.com/img/bVbcu8Y?w=1024&h=768)

#### 5. 添加缓存实例

按照同样的算法，在添加实例后发现，只有少部分对象的映射关系改变

![consistent_hashing.004.jpeg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbcu9F)

#### 6. 移除缓存实例

按照同样的算法，在移除实例后，同样只有少部分对象的映射关系改变

![consistent_hashing.005.jpeg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbcu9G)

#### 7. 虚拟节点

为了使对象尽可能均匀地映射到所有的缓存实例中（**解决缓存实例分布不均匀的问题**），引入虚拟节点的概念
**虚拟节点其实为真实节点在hash空间中的复制品，一个真实节点可以对应多个虚拟节点**

虚拟节点的hash求值可以在真实节点的求值基础上加入编号等信息 `hash(realCacheKey#1)` 、 `hash(realCacheKey#2)`

![consistent_hashing.006.jpeg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbcvae)



以存储为例，在整个微服务系统中，我们的存储不可能说只是一个单节点。

- 一是为了提高稳定，单节点宕机情况下，整个存储就面临服务不可用；
- 二是数据容错，同样单节点数据物理损毁，而多节点情况下，节点有备份，除非互为备份的节点同时损毁。

那么问题来了，多节点情况下，数据应该写入哪个节点呢？



#### consistent hash

我们来看看 `consistent hash` 是怎么解决这些问题的：

##### rehash

先解决大量 `rehash` 的问题：

![img](https://segmentfault.com/img/remote/1460000040373122)

如上图，当加入一个新的节点时，影响的key只有 `key31`，新加入（剔除）节点后，只会影响该节点附近的数据。其他节点的数据不会收到影响，从而解决了节点变化的问题。

这个正是：单调性。这也是 `normal hash` 算法无法满足分布式场景的原因。

##### 数据倾斜

其实上图可以看出：目前多数的key都集中在 `node 1` 上。如果当 node 数量比较少的情况下，可以回引发多数 key 集中在某个 `node` 上，监控时发现的问题就是：节点之间负载不均。

为了解决这个问题，`consistent hash` 引入了 `virtual node` 的概念。

既然是负载不均，我们就人为地构造一个均衡的场景出来，但是实际 node 只有这么多。所以就使用 `virtual node` 划分区域，而实际服务的节点依然是之前的 node。

### 具体实现

先来看看 `Get()`：

#### Get

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040373123)

先说说实现的原理：

1. 计算 `key` 的hash
2. 找到第一个匹配的 `virtual node` 的 index，并取到对应的 `h.keys[index]` ：**virtual node hash 值**
3. 对应到这个 `ring` 中去寻找一个与之匹配的 `actual node`

其实我们可以看到 `ring` 中获取到的是一个 `[]node` 。这是因为在计算 `virtual node hash` ，可能会发生hash冲突，**不同的 `virtual node hash` 对应到一个实际node**。

这也说明：**`node` 与 `virtual node` 是一对多的关系**。而里面的 `ring` 就是下面这个设计：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040373124)

这个其实也就表明了一致性hash的分配策略：

1. **`virtual node` 作为值域划分**。`key` 去获取 `node` ，从划分依据上是以 `virtual node` 作为边界
2. `virtual node` 通过 `hash` ，在对应关系上保证了不同的 node 分配的key是大致均匀的。也就是 **打散绑定**
3. 加入一个新的 node，会对应分配多个 `virtual node`。新节点可以负载多个原有节点的压力，从全局看，较容易实现扩容时的负载均衡。

```
// Get returns the corresponding node from h base on the given v.
func (h *ConsistentHash) Get(v interface{}) (interface{}, bool) {
	h.lock.RLock()
	defer h.lock.RUnlock()

	if len(h.ring) == 0 {
		return nil, false
	}
	// hash 为 key 的 hash
	hash := h.hashFunc([]byte(repr(v)))
	// 找到第一个符合 virtual node hash 大于 key hash 的下标
	index := sort.Search(len(h.keys), func(i int) bool {
		return h.keys[i] >= hash
	}) % len(h.keys)

	nodes := h.ring[h.keys[index]]
	switch len(nodes) {
	case 0:
		return nil, false
	case 1:
		return nodes[0], true
	default:
		// innerIndex 为 key h
		innerIndex := h.hashFunc([]byte(innerRepr(v)))
		pos := int(innerIndex % uint64(len(nodes)))
		return nodes[pos], true
	}
}
```

#### Add Node

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040373125)

看完 `Get` 其实大致就知道整个一致性hash的设计：

```go
type ConsistentHash struct {
  hashFunc Func                            // hash 函数
  replicas int                            // 虚拟节点放大因子
  keys     []uint64                    // 存储虚拟节点hash
  ring     map[uint64][]interface{}                    // 虚拟节点与实际node的对应关系
  nodes    map[string]lang.PlaceholderType    // 实际节点存储【便于快速查找，所以使用map】
  lock     sync.RWMutex
}
```

好了这样，基本的一个一致性hash就实现完备了。

> 具体代码：[https://github.com/tal-tech/g...](https://link.segmentfault.com/?enc=fWE4K%2B5mTLAyP8pBiz6V7Q%3D%3D.kX9NVPyAJE1PafMZq53T12Ohu5KC8Ib8%2B00isxK8kXvpNmWJ5DPIUcRE2PUWukJLrMSNONMZpBQsRuCOtB1xS61P%2BUk%2BK6hu78E0Cj17eXg%3D)

```
// Add adds the node with the number of h.replicas,
// the later call will overwrite the replicas of the former calls.
func (h *ConsistentHash) Add(node interface{}) {
	h.AddWithReplicas(node, h.replicas)
}

// AddWithReplicas adds the node with the number of replicas,
// replicas will be truncated to h.replicas if it's larger than h.replicas,
// the later call will overwrite the replicas of the former calls.
func (h *ConsistentHash) AddWithReplicas(node interface{}, replicas int) {
	// 删除原有node的节点
	h.Remove(node)

	if replicas > h.replicas {
		replicas = h.replicas
	}
	// repr 返回 node的字符串表示形式。
	nodeRepr := repr(node)
	h.lock.Lock()
	defer h.lock.Unlock()
	h.addNode(nodeRepr)

	for i := 0; i < replicas; i++ {
		hash := h.hashFunc([]byte(nodeRepr + strconv.Itoa(i)))
		// h.keys存储virtual node hash 的有序序列
		h.keys = append(h.keys, hash)
		// virtual node hash 可能相同，h.ring根据 virtual node hash 存储实际节点
		h.ring[hash] = append(h.ring[hash], node)
	}
	
	
	sort.Slice(h.keys, func(i, j int) bool {
		return h.keys[i] < h.keys[j]
	})
}

// AddWithWeight adds the node with weight, the weight can be 1 to 100, indicates the percent,
// the later call will overwrite the replicas of the former calls.
func (h *ConsistentHash) AddWithWeight(node interface{}, weight int) {
	// don't need to make sure weight not larger than TopWeight,
	// because AddWithReplicas makes sure replicas cannot be larger than h.replicas
	replicas := h.replicas * weight / TopWeight
	h.AddWithReplicas(node, replicas)
}
```

#### Remove Node

```
// Remove removes the given node from h.
func (h *ConsistentHash) Remove(node interface{}) {
	nodeRepr := repr(node)

	h.lock.Lock()
	defer h.lock.Unlock()

	if !h.containsNode(nodeRepr) {
		return
	}

	for i := 0; i < h.replicas; i++ {
		hash := h.hashFunc([]byte(nodeRepr + strconv.Itoa(i)))
		index := sort.Search(len(h.keys), func(i int) bool {
			return h.keys[i] >= hash
		})
		if index < len(h.keys) && h.keys[index] == hash {
			h.keys = append(h.keys[:index], h.keys[index+1:]...)
		}
		h.removeRingNode(hash, nodeRepr)
	}

	h.removeNode(nodeRepr)
}

func (h *ConsistentHash) removeRingNode(hash uint64, nodeRepr string) {
	if nodes, ok := h.ring[hash]; ok {
		newNodes := nodes[:0]
		for _, x := range nodes {
			if repr(x) != nodeRepr {
				newNodes = append(newNodes, x)
			}
		}
		if len(newNodes) > 0 {
			h.ring[hash] = newNodes
		} else {
			delete(h.ring, hash)
		}
	}
}

```



### 使用场景

开头其实就说了，一致性hash可以广泛使用在分布式系统中：

1. 分布式缓存。可以在 `redis cluster` 这种存储系统上构建一个 `cache proxy`，自由控制路由。而这个路由规则就可以使用一致性hash算法
2. 服务发现
3. 分布式调度任务

以上这些分布式系统中，都可以在负载均衡模块中使用。