# 预测数值型数据：回归

## 概念及原理

### 概述

前面提到的分类的目标变量时标称型数据，而回归则是对连续型数据做出处理，回归的目的是预测数值型数据的目标值。

### 场景

回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。

回归方程最普通的形式$f(x) = w'x+b$,

其中x向量代表一条样本{x1,x2,x3....xn}，其中x1，x2，x3代表样本的各个特征，w是一条向量代表了每个特征所占的权重，即回归系数，b是一个标量代表特征都为0时的预测值，可以视为模型的basis或者bias。看起来很简单的。

#### 损失函数

如何从一大堆数据中求出回归方程，主要问题是求出w和b。利用平方误差求出w，平方误差公式$\sum_{ i=1 }^{m} (y_i -x^Tw)^2$。

##### 最小二乘

最小二乘优化的思路是线性代数中的矩阵求导，只需要对这个式子进行求导，导数为0的地方就是极值点，矩阵还可以写为$(y-Xw)^T(y-Xw)$, 如果对w求导，得到$X^T(y- Xw)$, 令其等于0，解出w如下：
$$
\hat{w} = (X^TX)^{-1}X^Ty
$$
$\hat{w}$ 表示当前数据的最优。求导过程：[矩阵求导](https://blog.csdn.net/nomadlx53/article/details/50849941)。前提是$X^TX$矩阵可逆。

##### 梯度下降

梯度下降的策略与最小二乘优化不同，它采用的不是用数学方法一步求出解析解，而是一步一步的往让平方误差变到最小值的方向走，直到走到那个点。参考logistics回归。具体求这个grad的方法就是，对平方误差求偏导
$$
grad = 2(w'X - y)X^T
$$
之后沿着w梯度下降
$$
w := w - \alpha{\triangledown _wf(w)}
$$

#### 局部加权线性回归

线性回归的一个问题可能出现欠拟合现象，因为它求得是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。

一个方法是局部加权线性回归(Locally Weighted Linear Regression , LWLR)。在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与线性回归类似，在这个子集上基于最小均方差来进行普通的回归。我们需要最小化的目标函数大致为：
$$
\sum_{i} w(y^{(i)} - \hat{y}^{(i)})^2
$$
该算法解出回归系数w的形式如下：
$$
\hat{w} = (X^TWX)^{-1}X^TWy
$$
其中w是一个矩阵用来给每个数据点赋予权重。

LWLR使用“核”（与支持向量机中的核类似）来对附近的点赋予更高的权重，核的类型可以自由选择，最常用的核实高斯核，高斯核对应的权重如下：
$$
w(i,i) = \exp(\frac{|x^{(i)}-x|}{-2k^2})
$$

#### 缩减系数

如果特征比样本点还多(n>m)，也就是说输入数据的矩阵 x 不是满秩矩阵。非满秩矩阵求逆时会出现问题。

为解决这个问题，我们引入岭回归(ridge regression)这种缩减方法，接着是lasso法，最后是前向逐步回归。

##### 岭回归

简单来说，岭回归就是在矩阵$X^TX$上加上一个$\lambda I$ 从而使得矩阵非奇异，进而能对$X^TX +\lambda I$ 求逆。其中矩阵I是一个m*m的单位矩阵，对角线上元素全为1，其他元素全为0。而λ是用户定义的数值。回归系数的计算公式变成：
$$
\hat{w} = (X^TX +\lambda I)^{-1}X^Ty
$$

岭回归最先用于处理特征多于样本数的情况，现在也用于估计中加入偏差，从而得到更好的估计，这里通过引入λ来限制所有的w之和，通过引入惩罚项，能够减少不重要的参数，这个技术在统计学中也叫做缩减（shrinkage）

##### lasso

在增加如下约束时，普通的最小二乘法回归会得到与岭回归一样的公式:
$$
\sum_{ k=1 }^{n} w_{k}^{2} \le \lambda
$$
上式限定了所有回归系数的平方不能大于λ。使用普通的最小二乘法回归在当两个或者更多的特征相关时，可能会得到一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。

与岭回归类似，lasso缩减方法也对回归系数做出了限定，对应的约束条件如下：
$$
\sum_{k=1}^{n} |w_{k}| \le \lambda
$$
唯一 不同点在于，这个约束条件使用了绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭：在λ足够小的时候，一些系数因此被迫缩减到0.这个特性帮助我们更好的理解数据。

##### 前向逐步回归

就是利用贪心算法，即每步尽可能减少误差，即梯度下降。