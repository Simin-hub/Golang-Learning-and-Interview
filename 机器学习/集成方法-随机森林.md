# 集成方法—随机森林

## 概念及原理

### 集成方法(ensemble  method)概述

- 概念：是对其他算法进行组合的一种形式
- 集成方法：
  1. 投票选举(bagging:自举汇法 bootstrap aggregating)：是基于数据水机重抽样分类器构造的方法。
  2. 在学习(boosting )：是基于所有分类器的加权求和的方法

### 集成方法 场景

目前 bagging 方法最流行的是：随机森林(random forest)

目前 boosting 方法最流行的是：AdaBoost

#### bagging 和 boosting 区别

1. bagging 和 boosting 是两种很类似的技术，所使用的的多个分类器的类型（数据量和特征量）都是一致的。
2. bagging 是由不同分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果
3. bagging 中的分类器权重是相等的；而boosting中的分类器加权求和，所以权重并不相等，每个权重代表的是其分类器在上一轮迭代中的成功度。

### 随机森林

#### 概述：

- 随机森林指的是利用多颗树对样本进行训练并预测的一种分类器。
- 决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。

#### 原理：

随机森林如何构建：

1. 数据的随机性
2. 待选特征的随机性

使得随机森林中的决策树能够彼此不同，提升系统的多样性，从而提升分类性能

##### 数据的随机化：使得随机森林中的决策树更普遍化一点，适合更多的场景。

1. 采取有放回的抽样方式构造子数据集，保证不同子集之间的数量级一样（不同子集/同一子集 之间的元素可以重复）

2. 利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。

3. 然后统计子决策树的投票结果，得到最终的分类就是随机森林的输出结果

   

![随机森林](F:\Markdown\workspace\机器学习\随机森林.jpg)

##### 待选特征的随机化

1. 子树从所有待选特征中随机选取一定的特征。
2. 在选取的特征中选取最优的特征。