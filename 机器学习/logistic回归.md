# logistic回归

## 一、概念

回归概念：

存在一些数据点，我们用一条线对这些点进行拟合，这个拟合的过程就是回归，进而可以得到这些点的拟合直线方程，那么我们根据这个回归方程进行分类。

二值型输出分类函数

我们想要的函数是能接受所有的输入然后预测出类别，例如在两个类的时候，上述函数0或1。这种函数被称为海维塞得阶跃函数，或者称为单位阶跃函数。但是该函数在0跃迁到1的过程很难处理（不好分离1和0的界限），另一函数也有类似的性质，即为sigmoid函数
$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$
下图给出了sigmoid函数在不同坐标尺度下的两条曲线图、当x为0时，sigmoid函数值为0.5，随着

x的增大对应的sigmoid函数逼近与1；随着x的减小，sigmoid值将逼近于0。若果横坐标的刻度足够大，sigmoid函数看起来就像一个阶跃函数。

![img](https://cdn.jsdelivr.net/gh/jiutiananshu/Picture/img/sigmoid-01.jpg)

![sigmoid-02](https://cdn.jsdelivr.net/gh/jiutiananshu/Picture/img/sigmoid-02.jpg)

因此，实现logistics回归分类器，我们可以在每个特征上乘以一个回归系数(如下公式所示)，然后把所有结果值相加，将这个总和代入sigmoid函数中，进而得到一个范围在0~1之前的数值。任何大于0.5的数据被分为1类，小于0.5即被归为0类，所以logistics回归也可以被看成一种概率估计。

基于最优方式的回归系数确定

sigmoid函数的输入即为 z ，由下面公式得到：
$$
z = w_0x_0+w_1x_1+w_2x_2+...+w_nx_n
$$
如果采取向量的写法，上述公式可以写成 $z = w^Tx$, 它表示将这两个数值向量对应元素相乘然后全部加起来即得到 z 值，其中的向量 x 是分类器的输入数据，向量 w 也就是我们要招的最佳参数(系数)，从而使得分类器尽可能精确。为了寻找该最佳参数，需要用到最优理论的一些知识——梯度上升法。

梯度算法：

梯度的介绍

```
向量 = 值 + 方向
梯度 = 向量
梯度 = 梯度值 + 梯度方向
```

梯度上升的思想

要找到某函数的最大值，最好的方式是沿河该函数的梯度方向探寻。梯度算子总是指向函数增长最快的方向。移动量值称为步长，记做 α。 用向量表示的话，梯度上升的迭代公式如下：
$$
w := w+ \alpha{\triangledown _wf(w)}
$$
![img](https://cdn.jsdelivr.net/gh/jiutiananshu/Picture/img/Gradient-algorithm-01.jpg)



### 	logistics回归原理

```
每个回归系数初始化为1
重复 R 次：
	计算整个数据的梯度
	使用步长 x 梯度 更新回归系数的向量
返回回归系数
```

