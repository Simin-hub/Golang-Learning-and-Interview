# 支持向量机(SVM)

### 一、概念及原理

支持向量机(Support Vector Machine):是一种机器学习算法。

- 支持向量(Support Vector)就是分离超平面最近的那些点。
- 机(Machine)就表示一种算法，而不是表示机器

#### 支持向量机 场景

- 要给左右两边的点进行分类

### 寻找最大间距

#### 为什么要寻找最大间距

1. 直觉上是安全的
2. 如果我们的在边界的位置发生了一个小错误（它在垂直反面呗点到），这给我们最小的错误分类机会、
3. CV（Computer Vision 计算机视觉）很容易，因为该模型对任何非支持向量数据点的去除是免疫的。

#### 怎么寻找最大间隔

##### 	点到超平面的距离

- 分割超平面函数间距：$y(x) = w^Tx + b$
- 分类结果：$f(x) = sign(w^Tx+b)$(sign表示>0为1，<0为-1，=0为0)
- 点到超平面的几何间距：$d(x) = (w^Tx+b)/||w||$（||w||表示w矩阵的二范式=>$\sqrt{w*w^T}$,点到超平面的距离也是类似的）

点到直线的距离公式
$$
d = |\frac{Ax_0+By_0+C}{\sqrt{A^2+B^2}}|
$$

##### 	拉格朗日乘子法

- 类别标签用-1、1，是为了后期方面$lable * (w^Tx+b)$的表示和距离计算；如果$lable * (w^Tx+b)$ > 0 表示预测正确，否则预测正确
- 现在目标很明确，就是要找到w和b，因此我们必须要找到最小间隔的数据点，也就是前面所说的支持向量
  - 也就说，让最小的距离取最大(最小的距离:就是最小间隔的数据点；最大：就是最大间距，为了找出最优超平面——最终就是支持向量)
  - 目标函数：$arg:max_{关于w,b}(min[lable*(w^Tx+b)]*\frac{1}{||w||})$  #第一个min是两个分类之间的最小距离，第二个max是点到最小间距离的点到超平面的最大间距
    1. 如果$lable*(w^Tx+b) > 0$ 表示预测正确，也称函数间隔，||w||可以理解为归一化，也称几何间隔。
    2. 令$lable*(w^Tx+b) >= 1$, 因为0~1之间，得到的点存在误判的可能性，所以要保障$min[lable*(w^Tx+b)] = 1$，才能更好降噪数据影响。
    3. 所以本质上是求$arg:max_{关于w,b}\frac{1}{||w||}$；也就是说，我们约束（前提）条件是：$lable*(w^Tx+b) = 1$
- 新的目标函数求解：$arg:max_{关于w,b}\frac{1}{||w||}$
  -  ==>就是求：$arg:min_{关于w,b}||w||$ (求矩阵会比较麻烦，如果x只是$\frac{1}{2} * x^2$的偏导数，那么。。同样是求最小值)
  - ==>就是求：$arg:min_{关于w,b}(\frac{1}{2}*||w||^2)$（二次求导，求极值，平方也方便计算）
  - 本质上就是求线性不等式的二次优化问题（求分隔超平面，等价于求解相应的凸二次规划问题）
- 通过拉格朗日乘子法，求二次优化问题
  - 假设需要求极值的目标函数（object  functio）为f(x,y)，限制条件为$\varphi(x,y)=M \space  \# \space M=1$
  - 设$g(x,y)=M-\varphi(x,y) \space\#临时\varphi(x,y)表示下文中\space lable*(w^Tx+b)$
  - 定义一个新函数：$F(x,y,\lambda)=f(x,y)+\lambda g(x,y)$
  - $\alpha 为 \lambda(\alpha \ge 0)$，代表要引入的拉格朗日乘子（Lagrange multiplier）
  - 那么：$L(w,b,\alpha)=\frac{1}{2}+||w||^2+ \begin{matrix} \sum_{i=1}^n \alpha _{i} *[1-lable*(w^T+b)] \end{matrix}$
  - 因为：$lable*(w^Tx+b) >= 1，\alpha \ge 0,所以 \alpha*[1-lable*(w^Tx+b)] \le 0,\begin{matrix} \sum_{i=1}^n \alpha _{i} *[1-lable*(w^T+b)] \end{matrix} \le 0$
  - 相当于求解：$max_{关于\alpha}L(w,b,\alpha)=\frac{1}{2}*||w||^2$
  - 如果求：$min_{关于w,b}(\frac{1}{2}*||w||^2)$，就是要求：$min_{关于w,b}(max_{关于\alpha}L(w,b,\alpha))$
- 现在转化到对偶问题的求解
  - $min_{关于w,b}(max_{关于\alpha}L(w,b,\alpha)) \ge max_{关于\alpha}(min_{关于w,b}L(w,b,\alpha))$
  - 现在分两步
  - 先求：$min_{关于w,b}L(w,b,\alpha)=\frac{1}{2}*||w||^2+ \begin{matrix} \sum_{i=1}^n \alpha _{i} *[1-lable*(w^T+b)] \end{matrix}$
  - 就是求$L(w,b,\alpha)$关于[w,b]的偏导数，得到w和b的值，并简化为$L和\alpha$的方程
  - 得到公式：$max_{关于\alpha}(\begin{matrix} \sum_{i=1}^n \alpha _{i} *[1-lable*(w^T+b)] \end{matrix})$
  - 约束条件：$ a \ge 0 并且\sum_{i=1}^m a_i \cdot lable =0 $

##### 松弛变量(slack variable)

- 几乎所有数据都不那么干净，通过引入松弛变量来允许数据点可以处于分割面错误的一侧。
- 约束条件：$C \ge a \ge 0 并且\sum_{i=1}^m a_i \cdot lable =0 $
- 这里常量C用于控制“最大化间隔”和保证大部分点的函数间隔小于1.0这两个权重
- 常量C是一个常数，通常调节该参数得到不同的结果。一旦求出了所有的$\alpha$,那么分割超平面就可以通过$\alpha$来表示
- SVM中的主要工作就是求解$\alpha$

#### SMO高效优化算法

- SVM有很多种实现，最流行的一种实现是：序列最小优化(Squential Minimal Optimization, SMO)算法.
- 核函数的方法将SVM扩展到更多数据集上。