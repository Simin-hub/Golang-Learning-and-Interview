# MySQL

[参考](https://github.com/CyC2018/CS-Notes/blob/master/notes/MySQL.md)

## 一、索引

### B+ Tree 原理

[B树](https://github.com/Simin-hub/Golang-Learning-and-Interview/blob/main/Go/%E7%AE%97%E6%B3%95/B%E6%A0%91.md)

#### 1. 数据结构

B Tree 指的是 Balance Tree，也就是**平衡多路查找树**。平衡树是一颗查找树，并且**所有叶子节点位于同一层**。

B+ Tree 是基于 B Tree 和叶子节点顺序访问指针进行实现，它具有 B Tree 的平衡性，并且通过顺序访问指针来提高区间查询的性能。B+树是在B树的基础上又一次的改进，其主要对两个方面进行了提升，一方面是查询的稳定性，另外一方面是在数据排序方面更友好。

在 B+ Tree 中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，且不为 null，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。

**B+树构建规则**

（1）B+树的**非叶子**节点**不保存具体的数据，而只保存关键字的索引**，而所有的数据最终都会保存到叶子节点。因为所有数据必须要到叶子节点才能获取到，所以每次数据查询的次数都一样，这样一来B+树的查询速度也就会比较稳定，而B树的查找过程中，不同的关键字查找的次数很有可能都是不同的（有的数据可能在根节点，有的数据可能在最下层的叶节点），所以在数据库的应用层面，B+树就显得更合适。

（2）B+树叶子节点的关键字从小到大有序排列，左边结尾数据都会保存右边节点开始数据的指针。因为叶子节点都是有序排列的，所以B+树对于数据的排序有着更好的支持。

（3）非叶子节点的子节点数=关键字数（来源百度百科）（根据各种资料 这里有两种算法的实现方式，另一种为非叶节点的关键字数=子节点数-1（来源维基百科)，虽然他们数据排列结构不一样，但其原理还是一样的Mysql 的B+树是用第一种方式实现）;

[![img](https://camo.githubusercontent.com/4d682f9aa8dd74bd32712b7ca85a85b2c213fd4282d62fcc137488dea23ddde9/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f33333537363834392d393237352d343762622d616461372d3864656435663565376337332e706e67)](https://camo.githubusercontent.com/4d682f9aa8dd74bd32712b7ca85a85b2c213fd4282d62fcc137488dea23ddde9/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f33333537363834392d393237352d343762622d616461372d3864656435663565376337332e706e67)



#### 2. 操作

进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。

插入删除操作会破坏平衡树的平衡性，因此在进行插入删除操作之后，需要对树进行分裂、合并、旋转等操作来维护平衡性。

#### 3. 与红黑树的比较

[红黑树](https://github.com/Simin-hub/Golang-Learning-and-Interview/blob/main/Go/%E7%AE%97%E6%B3%95/%E7%BA%A2%E9%BB%91%E6%A0%91.md)

红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用 B+ Tree 作为索引结构，这是因为使用 B+ 树访问磁盘数据有更高的性能。

（一）B+ 树有更低的树高

平衡树的树高 O(h)=O(logdN)，其中 d 为每个节点的出度。红黑树的出度为 2，而 B+ Tree 的出度一般都非常大，所以红黑树的树高 h 很明显比 B+ Tree 大非常多。

（二）磁盘访问原理

操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。

如果数据不在同一个磁盘块上，那么通常需要移动制动手臂进行寻道，而制动手臂因为其物理结构导致了移动效率低下，从而增加磁盘数据读取时间。B+ 树相对于红黑树有更低的树高，进行寻道的次数与树高成正比，在同一个磁盘块上进行访问只需要很短的磁盘旋转时间，所以 B+ 树更适合磁盘数据的读取。

（三）磁盘预读特性

为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的磁盘旋转时间，速度会非常快。**并且可以利用预读特性，相邻的节点也能够被预先载入**。

### MySQL 索引

[参考](https://developer.huawei.com/consumer/cn/forum/topic/0204405591412170236)

索引是在**存储引擎层实现**的，而不是在服务器层实现的，所以不同存储引擎具有不同的索引类型和实现。

#### 什么是索引？ 

**索引是一种特殊的文件(InnoDB 数据表上的索引是表空间的一个组成部分)**，**它们包含着对数据表里所有记录的引用指针**。 **索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据**。索引的实现通常使用 B 树及其变种 B+树。 更通俗的说，**索引就相当于目录**。为了方便查找书中的内容，通过对内容建立 索引形成目录。索引是一个文件，它是要占据物理空间的。

#### 索引的分类

按**数据结构分类**可分为：**B+tree 索引、Hash 索引、Full-text 索引**。

按**物理存储分类**可分为：**聚簇索引、二级索引（辅助索引）**。

按**字段特性分类**可分为：**主键索引、普通索引、前缀索引**。

按**字段个数分类**可分为：**单列索引、联合索引（复合索引、组合索引）**。

#### 1. B+Tree 索引

是大多数 MySQL 存储引擎的默认索引类型。

因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。

因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。

可以指定多个列作为索引列，多个索引列共同组成键。

适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。

**InnoDB** 的 **B+Tree 索引**分为**主索引和辅助索引**。**主索引的叶子节点 data 域记录着完整的数据记录**，这种索引方式被称为**聚簇索引**。因为无法把数据行存放在两个不同的地方，所以**一个表只能有一个聚簇索引**。

[![img](https://camo.githubusercontent.com/7220f50cca2d4a015b0a23df318b71a4764c6ae8009ba7092938d41045a3cf5b/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f34353031366539382d363837392d343730392d383536392d3236326232643664363062392e706e67)](https://camo.githubusercontent.com/7220f50cca2d4a015b0a23df318b71a4764c6ae8009ba7092938d41045a3cf5b/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f34353031366539382d363837392d343730392d383536392d3236326232643664363062392e706e67)



**辅助索引的叶子节点的 data 域记录着主键的值**，因此在使用辅助索引进行查找时，需要先查找到主键值，然后**再到主索引中进行查找**。

[![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f37633334396239312d303530622d346437322d613766382d6563383633323033303765612e706e67)](https://camo.githubusercontent.com/4c90ec27dc7dacdb350c31ab4f072685538f1c89dfc4362750fdfa0ca85e7622/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f37633334396239312d303530622d346437322d613766382d6563383633323033303765612e706e67)



#### 2. 哈希索引

哈希索引能以 O(1) 时间进行查找，但是失去了有序性：

- 无法用于排序与分组；
- 只支持精确查找，无法用于部分查找和范围查找。

**InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引**，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。

##### B+tree与Hash的对比

Hash 索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引。虽然 Hash 索引效率高，但是 Hash 索引本身由于其特殊性也带来了很多限制和弊端，主要有以下这些。

###### Hash 索引仅仅能满足 `=` , `IN` 和 `<=>`(表示NULL安全的等价) 查询，不能使用范围查询。

由于 Hash 索引比较的是进行 Hash 运算之后的 Hash值，所以它**只能用于等值的过滤，不能用于基于范围的过滤**，因为经过相应的 Hash算法处理之后的 Hash 值的大小关系，并不能保证和Hash运算前完全一样。

###### Hash 索引无法适用数据的排序操作。

由于 Hash 索引中存放的是经过 Hash 计算之后的 Hash值，而且Hash值的大小关系并不一定和 Hash运算前的键值完全一样，所以数据库无法利用索引的数据来避免任何排序运算；

###### Hash 索引不能利用部分索引键查询。

对于组合索引，Hash 索引在计算 Hash 值的时候是组合索引键合并后再一起计算 Hash 值，而不是单独计算 Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash 索引也无法被利用。

###### Hash 索引依然需要回表扫描。

Hash 索引是将索引键通过 Hash 运算之后，**将 Hash运算结果的 Hash 值和所对应的行指针信息存放于一个 Hash 表中**，由于不同索引键可能存在相同 Hash 值，所以即使取满足某个 Hash 键值的数据的记录条数，也无法从 Hash索引中直接完成查询，还是要通过访问表中的实际数据进行相应的比较，并得到相应的结果。

###### Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高。

选择性比较低的索引键，如果创建 Hash 索引，那么将会存在大量记录指针信息存于同一个Hash值相关联。这样要定位某一条记录时就会非常麻烦，会浪费多次表数据的访问，而造成整体性能低下

#### 3. 全文索引

**MyISAM 存储引擎支持全文索引**，**用于查找文本中的关键词**，而不是直接比较是否相等。

查找条件使用 MATCH AGAINST，而不是普通的 WHERE。

全文索引使用倒排索引实现，**它记录着关键词到其所在文档的映射**。

InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。

#### 4. 空间数据索引

**MyISAM 存储引擎支持空间数据索引（R-Tree）**，可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。

必须使用 GIS 相关的函数来维护数据。

### 索引优化

#### 1. 独立的列

**在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引**。

例如下面的查询不能使用 actor_id 列的索引：

```
SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;
```

#### 2. 多列索引

在需要**使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好**。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。

```
SELECT film_id, actor_ id FROM sakila.film_actor
WHERE actor_id = 1 AND film_id = 1;
```

#### 3. 索引列的顺序

让**选择性最强的索引列放在前面**。

**索引的选择性是指：不重复的索引值和记录总数的比值**。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。

例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。

```
SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,
COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,
COUNT(*)
FROM payment;
   staff_id_selectivity: 0.0001
customer_id_selectivity: 0.0373
               COUNT(*): 16049
```

#### 4. 前缀索引

对于 BLOB、TEXT 和 VARCHAR 类型的列，**必须使用前缀索引，只索引开始的部分字符**。

前缀长度的选取需要根据索引选择性来确定。

#### 5. 覆盖索引

覆盖索引（covering index ，或称为索引覆盖）即**从非主键索引中就能查到的记录，而不需要查询主键索引中的记录**，避免了回表的产生减少了树的搜索次数，显著提升性能。**索引包含所有需要查询的字段的值**。

在非聚簇索引的叶子节点中保存的数据是主键值和其他索引列的值，如果查询的字段都是索引，则不需要进行回表查询，这就是覆盖索引。

[示例](https://blog.csdn.net/cckevincyh/article/details/119655516)

具有以下优点：

- 索引通常**远小于**数据行的大小，只读取索引能大大减少数据访问量。
- 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。
- 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。

### 索引的优点

- 大大**减少了服务器需要扫描的数据行数**。
- **帮助服务器避免进行排序和分组，以及避免创建临时表**（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。
- **将随机 I/O 变为顺序 I/O**（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。

### 索引的使用条件

- 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效；
- 对于中到大型的表，索引就非常有效；
- 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。

## 二、查询性能优化

### 使用 Explain 进行分析

[参考](https://zhuanlan.zhihu.com/p/281517471)

Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。

比较重要的字段有：

- select_type : 查询类型，有简单查询、联合查询、子查询等
- key : 使用的索引
- rows : 扫描的行数

explain可用来分析SQL的执行计划。格式如下：

```text
{EXPLAIN | DESCRIBE | DESC}
    tbl_name [col_name | wild]

{EXPLAIN | DESCRIBE | DESC}
    [explain_type]
    {explainable_stmt | FOR CONNECTION connection_id}

{EXPLAIN | DESCRIBE | DESC} ANALYZE select_statement    

explain_type: {
    FORMAT = format_name
}

format_name: {
    TRADITIONAL
  | JSON
  | TREE
}

explainable_stmt: {
    SELECT statement
  | TABLE statement
  | DELETE statement
  | INSERT statement
  | REPLACE statement
  | UPDATE statement
}
```

示例：

```text
EXPLAIN format = TRADITIONAL json SELECT tt.TicketNumber, tt.TimeIn,
               tt.ProjectReference, tt.EstimatedShipDate,
               tt.ActualShipDate, tt.ClientID,
               tt.ServiceCodes, tt.RepetitiveID,
               tt.CurrentProcess, tt.CurrentDPPerson,
               tt.RecordVolume, tt.DPPrinted, et.COUNTRY,
               et_1.COUNTRY, do.CUSTNAME
        FROM tt, et, et AS et_1, do
        WHERE tt.SubmitTime IS NULL
          AND tt.ActualPC = et.EMPLOYID
          AND tt.AssignedPC = et_1.EMPLOYID
          AND tt.ClientID = do.CUSTNMBR;
```

结果输出展示：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/v2-a4cb15fb2e3984a4ed007e44e3a080c8_r.jpg)

### 优化数据访问

#### 1. 减少请求的数据量

- **只返回必要的列**：最好不要使用 SELECT * 语句。
- **只返回必要的行**：使用 LIMIT 语句来限制返回的数据。
- **缓存重复查询的数据**：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。

#### 2. 减少服务器端扫描的行数

最有效的方式是使用索引来覆盖查询。

### 重构查询方式

#### 1. 切分大查询

一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。

```
DELETE FROM messages WHERE create < DATE_SUB(NOW(), INTERVAL 3 MONTH);
rows_affected = 0
do {
    rows_affected = do_query(
    "DELETE FROM messages WHERE create  < DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")
} while rows_affected > 0
```

#### 2. 分解大连接查询

**将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联**，这样做的好处有：

- **让缓存更高效**。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。
- **分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到**，从而减少冗余记录的查询。
- **减少锁竞争**；
- **在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩**。
- **查询本身效率也可能会有所提升**。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。

```
SELECT * FROM tag
JOIN tag_post ON tag_post.tag_id=tag.id
JOIN post ON tag_post.post_id=post.id
WHERE tag.tag='mysql';
SELECT * FROM tag WHERE tag='mysql';
SELECT * FROM tag_post WHERE tag_id=1234;
SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904);
```

## 三、存储引擎

### InnoDB

是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。

实现了四个标准的隔离级别，**默认级别是可重复读（REPEATABLE READ）**。在可重复读隔离级别下，**通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读**。

**主索引是聚簇索引**，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。

内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。

**支持真正的在线热备份**。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。

#### InnoDB 存储引擎的特点

自从 MySQL5.1 之后，默认的存储引擎变成了 InnoDB 存储引擎，相对于 MylSAM，InnoDB 存储引擎有了较大的改变，它的主要特点是 

- **支持事务操作**，具有事务 ACID 隔离特性，**默认的隔离级别是可重复读** (repetable-read)、通过 MVCC(并发版本控制)来实现的。能够解决 脏读 和 不可重复读 的问题。 
- InnoDB 支持外键操作。 
- InnoDB 默认的锁粒度行级锁，并发性能比较好，会发生死锁的情况。 
- 和 MyISAM 一样的是，InnoDB 存储引擎也有 frm 文件存储表结构定义，但是不同的是，**InnoDB 的表数据与索引数据是存储在一起的**，都位于 B+数的叶子节点上，而 MylSAM 的表数据和索引数据是分开的。 
- InnoDB **有安全的日志文件**，这个日志文件用于恢复因数据库崩溃或其他情况导致的数据丢失问题，保证数据的一致性。
- InnoDB 和 MylSAM 支持的索引类型相同，但具体实现因为文件结构的不同有 很大差异。
- 增删改查性能方面，如果执行大量的增删改操作，推荐使用 InnoDB 存储引 擎，它在删除操作时是对行删除，不会重建表。

#### 事务

[参考](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/9744607)

**数据库事务**（简称：**事务**）是[数据库管理系统](https://zh.wikipedia.org/wiki/数据库管理系统)执行过程中的一个逻辑单位，由一个有限的[数据库](https://zh.wikipedia.org/wiki/数据库)操作序列构成。

数据库事务( transaction)是访问并可能操作各种[数据项](https://baike.baidu.com/item/数据项/3227309)的一个数据库操作[序列](https://baike.baidu.com/item/序列/1302588)，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。事务由事务开始与事务结束之间执行的全部数据库操作组成。

##### ACID

**ACID**，是指[数据库管理系统](https://zh.wikipedia.org/wiki/数据库管理系统)（[DBMS](https://zh.wikipedia.org/wiki/DBMS)）在写入或更新资料的过程中，为保证[事务](https://zh.wikipedia.org/wiki/数据库事务)（transaction）是正确可靠的，所必须具备的四个特性：[原子性](https://zh.wikipedia.org/w/index.php?title=原子性&action=edit&redlink=1)（atomicity，或称不可分割性）、[一致性](https://zh.wikipedia.org/wiki/一致性_(数据库))（consistency）、[隔离性](https://zh.wikipedia.org/wiki/隔離性)（isolation，又称独立性）、[持久性](https://zh.wikipedia.org/wiki/持久性)（durability）。

在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。ACID的概念在[ISO](https://zh.wikipedia.org/wiki/ISO)/IEC 10026-1:1992文件的第四段内有所说明。

- **原子性**（Atomicity）：**一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成**，不会结束在中间某个环节。事务在执行过程中发生错误，会被[回滚](https://zh.wikipedia.org/wiki/回滚_(数据管理))（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。
- [**一致性**](https://zh.wikipedia.org/wiki/一致性_(数据库))（Consistency）：**在事务开始之前和事务结束以后，数据库的完整性没有被破坏**。这表示写入的资料必须完全符合所有的预设[约束](https://zh.wikipedia.org/wiki/数据完整性)、[触发器](https://zh.wikipedia.org/wiki/触发器_(数据库))、[级联回滚](https://zh.wikipedia.org/wiki/级联回滚)等。
- [**事务隔离**](https://zh.wikipedia.org/wiki/事務隔離)（Isolation）：数据库**允许多个并发事务同时对其数据进行读写和修改的能力**，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。
- [**持久性**](https://zh.wikipedia.org/wiki/持久性)（Durability）：事务处理结束后，**对数据的修改就是永久的**，即便系统故障也不会丢失。

#### redo log、undo log、binlog

[参考](https://segmentfault.com/a/1190000041758784)、[参考](https://juejin.cn/post/6895265596985114638)

`MySQL`日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中比较重要的就是二进制日志`binlog`（归档日志）、事务日志`redo log`（重做日志）和`undo log`（回滚日志）。

日志关系如下图：

<img src="https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758786" alt="img" style="zoom: 50%;" />

##### redo log

`redo log`（重做日志）是`InnoDB`存储引擎独有的，它让`MySQL`有了崩溃恢复的能力。

当`MySQL`实例挂了或者宕机了，重启的时候`InnoDB`存储引擎会使用`rede log`日志恢复数据，保证事务的持久性和完整性。如下图：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758787)

`MySQL`中**数据是以页为单数存储**，当你查询一条记录时，**硬盘会把一整页的数据加载出来，加载出来的数据叫做数据页**，会放到`Buffer Pool`中。后续的查询都是先从`Buffer Pool`中找，没有找到再去硬盘加载其他的数据页直到命中，这样子可以减少磁盘`IO`的次数，提高性能。更新数据的时候也是一样，优先去`Buffer Pool`中找，如果存在需要更新的数据就直接更新。然后会**把“在某个数据页做了什么修改”记录到重做日志缓存（`redo log buffer`）里**，在刷盘的时候会写入`redo log`日志文件里。

如下图：

![img](https://segmentfault.com/img/remote/1460000041758788)

> 小贴士：每条redo记录由“表空间号+数据页号+偏移量+修改数据长度+具体修改的数据”组成。
>
> redo log 记录格式[https://juejin.cn/post/6895265596985114638](https://juejin.cn/post/6895265596985114638)

###### 刷盘时机

理想情况下，**事务一提交就会进行刷盘操作**，但是实际上是刷盘的时机是根据策略来决定的。

`InnoDB`存储引擎为`redo log`的刷盘策略提供了`innodb_flush_log_at_trx_commit`参数，它支持三种策略：

- 0：设置为0的时候，每次提交事务时不刷盘。
- 1：设置为1的时候，每次提交事务时刷盘。
- 2：设置为2的时候，每次提交事务时都只把`redo log buffer`写入`page cache`。

`innodb_flush_log_at_trx_commit`参数默认为1，当事务提交的时候会调用`fsync`对`redo log`进行刷盘，将`redo log buffer`写入`redo log`文件中。

另外，`Innodb`存储引擎有一个后台线程，每隔`1`秒，就会把会`redo log buffer`中的内容写入到文件系统缓存`page cache`，然后调用`fsync`刷盘。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758789)

如上图，所以说一个没有提交事务的`redo log`记录，也会被刷盘。

下面是各种刷盘策略的流程图。

innodb_flush_log_at_trx_commit = 0

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758790)

如上图，如果宕机了或者`MySQL`挂了可能造成`1`秒内的数据丢失。

innodb_flush_log_at_trx_commit = 1

如上图，只要事务提交成功，`redo log`记录就一定在磁盘里，不会有任务数据丢失。

如果执行事务的时候`MySQL`挂了或者宕机了，这部分日志丢失了，但是因为事务没有提交，所以日志丢了也不会有损失。

![img](https://segmentfault.com/img/remote/1460000041758791)

innodb_flush_log_at_trx_commit = 2

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758792)

如上图，当事务提交成功时，`redo log buffer`日志会被写入`page cache`，然后后台线程会刷盘写入`redo log`，由于后台线程是`1`秒执行一次所以宕机或者`MySQL`挂了可能造成`1`秒内的数据丢失。

###### 日志文件组

硬盘上存储的`redo log`日志文件不止一个，而是一个**日志文件组**的形式出现的，每个的`redo log`文件大小都是一样的。它采用的是**环形数组形式**，从头开始写，写到末尾回到头循环写，如下图所示：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758793)

在**日志文件组**中有两个重要的属性，分别是`witre pos、checkpoint`

- **wirte pos**：是当前记录的位置，一边写一边后移。
- **checkpoint**：是当前要擦除的位置，也是后台推移。

每次刷盘`redo log`记录到**日志文件组**中，`wirte log`位置就会后移更新。

每次`MySQL`加载**日志文件组**恢复数据时，会清空加载过的`redo log`，并把`checkpoint`后移更新。

`write pos` 和 `checkpoint` 之间的还空着的部分可以用来写入新的 `redo log` 记录。

如果 `witre pos`追上`checkpoint`，表示**日志文件组**满了，这时候不能再写入新的`redo log`记录，`MySQL`得停下来，清空一些记录，把`checkpoint`推荐一下。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758795)

###### redo log小结

`redo log`的作用和它的刷盘时机、存储形式。

可以思考一个问题：**只要每次把修改后的数据页直接刷盘不就好了，为什么还要用`redo log`刷盘？不都是刷盘吗？有什么区别？**

实际上，数据页大小是`16KB`，刷盘比较耗时，可能就修改了数据页的几`byte`数据，没有必要把整页的数据刷盘。而且数据页刷盘都是随机写，因为一个数据页对应的位置可能是在硬盘文件的随机位置，所以性能很差。

如果是写`redo log`，一行记录就占了几十`byte`，只要包含了表空间号、数据页号、磁盘文件偏移量、修改值，再加上是顺序写，所以刷盘效率很高。

所以用 `redo log` 形式记录修改内容，性能会远远超过刷数据页的方式，这也让数据库的并发能力更强。

##### binlog

`redo log`是物理日志，**记录的是“在某个数据页做了什么修改”**，属于`Innodb`存储引擎。

而`binlog`日志是逻辑日志，**记录内容是语句的原始逻辑**，属于`MySQL Server`层。所有的存储引擎只要发生了数据更新，都会产生`binlog`日志。

###### `binlog`日志的作用

可以说`MySQL`数据库的**数据备份、主备、主主、主从**都离不开`binlog`，需要依赖`binlog`来同步数据，保证数据一致性。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758796)

`binlog`会记录所有涉及更新数据的逻辑规则，并且按顺序写。

###### 记录格式

[参考](https://blog.51cto.com/u_15127534/4339193)

`binlog`日志有三种格式，可以通过`binlog_format`参数设置，有以下三种：

- **statement**：每一条会修改数据的**sql**都会被记录在binlog中，即记录执行的sql语句
- **row**：不设置sql语句上下文相关信息，**仅保存哪条记录被修改**，即记录修改后的数据
- **mixed**：是以上两种level的混合使用，一般的语句修改使用Statement格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用ROW格式保存binlog。

设置`statement`记录的内容是`SQL`语句原文，比如执行一条`update T set update_time = now() where id = 1`，记录内容如下：
![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758797)

同步数据时，会执行记录的`SQL`语句，但是有个问题`update_time = now()`这里会获取到当前系统问题，直接执行会导致与原库数据不一致。

为了解决这种问题，我们需要将`binlog_format`设置成`row`，记录的不再是简单的`SQL`语句了，还包含了操作的具体数据，记录内容如下：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758798)

`row`格式记录的内容看不到详细信息，通过`mysqlbinlog`工具解析出来。

`update_time = now()`变成了具体的时间，条件后面的`@1、@2`都是该行数据第1个~2个字段的原始值（假设这张表只有2个字段）。

设置成`row`带来的好处就是同步数据的一致性，通常情况都设置成`row`，这样可以为数据库的恢复与同步带来更好的可靠性。但是这种格式需要大量的容量来记录，比较占用空间，恢复与同步时会更消耗`IO`资源，影响执行速度。

所以又有了一种折中方案，设置为`mixed`，记录的内容是前两者的混合。

**`MySQL`会判断这条`SQL`语句是否会引起数据不一致，如果是就用`row`格式，否则就用`statement`格式**。

###### 写入机制

`binlog`的写入时机**为事务执行过程中**，先把日志写到`binlog cache`，事务提交的时候再把`binlog cache`写到`binlog`文件中（实际先会写入`page cache`，然后再由`fsync`写入`binlog`文件）。

因为一个事务的`binlog`不能被拆开，无论这个事务多大，也要**确保一次性写入**，所以系统会给每个线程分配一块内存作为`binlog cache`。可以通过`binlog_cache_size`参数控制单线程`binlog_cache`大小，如果存储内容超过了这个参数，就要暂存到磁盘。

`binlog`日志刷盘流程如下：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758799)

- **上图的`write`，是指把日志写入到文件系统的`page cache`，并没有把数据持久化硬盘，所以速度比较快。**
- **上图的` fsync`才是将数据库持久化到硬盘的操作。**

`write`和`fsync`的时机可以由参数`sync_binlog`控制，可以配置成`0、1、N(N>1)`。

- 设置成0时：表示每次提交事务都只会`write`，由系统自行判断什么时候执行`fsync`。
- 设置成1时：表示每次提交事务都会执行`fsync`，就和`redo log`日志刷盘流程一样。
- 设置成N时：表示每次提交事务都会`write`，但是积累`N`个事务后才`fsync`。

设置为0时如下图：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758800)

从上图可知，`sync_bilog = 0`设置成`0`，只把日志写入`page cache`虽然性能得到了提高，但是事务提交了`fsync`的时候宕机了，可能造成`binlog`日志的丢失。

设置为2时如下图：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000041758801)

在出现`IO`瓶颈的场景里，将`sync_binlog`设置成一个比较大的值，可以提升性能。

同样的，如果机器宕机，会丢失最近`N`个事务的`binlog`日志。

##### undo log

想要保证事务的原子性，就需要在发生异常时，对已经执行的操作进行回滚，在`MySQL`中恢复机制是通过`undo log`（回滚日志）实现的，所有事务进行的修改都会先被记录到这个回滚日志，然后再执行其他相关的操作。如果执行过程中遇到异常的话，我们直接利用回滚日志中的信息将数据回滚到修改之前的样子。并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。

另外，`MVCC`的实现依赖：**隐藏字段、`Read View`、`undo log`**。在底层实现中，`InnoDB`通过数据行的`DB_TRX_ID`和`Read View`来判断数据的可见性，如不可见，则通过数据行`DB_ROLL_PTR`找到`undo log`中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事物里，用户只能看到该事务创建`Read View`之前已经提交的修改和该事务本身做的修改。

#### 事务执行流程

##### MySQL 连接器

首先需要在 MySQL 客户端登陆才能使用，所以**需要个连接器来连接用户和 MySQL 数据库**，我们 一般是使用 

```
mysql-u 用户名-p 密码
```

来进行 MySQL 登陆，和服务端建立连接。在完成 TCP 握手后，连接器会根据你输入的用户名和密码验证你的登录身份。如果用户名或者密码错误，MySQL 就 会提示 Access denied for user，来结束执行。如果登录成功后，MySQL 会根据权限表中的记录来判定你的权限。

##### MySQL 分析器

如果没有命中查询，就开始执行真正的 SQL 语句。

- 首先，**MySQL 会根据你写的 SQL 语句进行解析**，分析器会先做**词法分析**，你写的 SQL 就是由多个字符串和空格组成的一条 SQL 语句，MySQL **需要识别出里面的字符串是什么，代表什么**。 
- 然后**进行语法分析**，根据词法分析的结果，**语法分析器会根据语法规则， 判断你输入的这个 SQL 语句是否满足 MySQL 语法**。如果 SQL 语句不正确， 就会提示 You have an error in your SQL syntax。

##### MySQL 优化器

经过分析器的词法分析和语法分析后，你这条 SQL 就合法了，MySQL 就知道你要做什么了。但是在执行前，还需要进行优化器的处理，**优化器会判断你使用了哪种索引，使用了何种连接，优化器的作用就是确定效率最高的执行方案**。

##### MySQL 执行器

MySQL 通过分析器知道了你的 SQL 语句是否合法，你想要做什么操作，通过优化器知道了该怎么做效率最高，然后就进入了执行阶段，开始执行这条 SQL 语 句在执行阶段，**MySQL 首先会判断你有没有执行这条语句的权限**，没有权限的话，就会返回没有权限的错误。如果有权限，就打开表继续执行。打开表的时候，**执行器就会根据表的引擎定义，去使用这个引擎提供的接口**。对于有索引 的表，执行的逻辑也差不多。

##### 详细流程

[详解MySQL执行事务的语法和流程](https://segmentfault.com/a/1190000039032584)

平时执行一个单独的sql语句其实也是一个事务。例如：

Insert into t values (xxxxx);

这就是一个事务，只不过mysql帮我们自动commit了。

所以其实执行这样的单句sql，也是一个事务也会记录到undo和redo中

```
sql:update test set name = 'test' where id=2;
```

1.事务开始

2.申请锁资源，对id=2这行数据上排他锁

3.将需要修改的data pages读取到innodb_buffer_cache

4.记录id=2的数据到undo log

5.记录id=2修改后的数据到redo log buffer

6.将buffer cache中id=2得name改为test

7.commit，触发二阶段提交2pc

8.事务结束

![mysql_一条更新语句的执行流程_weixin_34284188的博客-CSDN博客](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/ade8a223210cdce5e5d50a3e659a94bdca7.jpg)

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20211011004118702.png)

**二阶段提交(2pc two phase commit):**

[参考](https://zhuanlan.zhihu.com/p/343449447)

二阶段提交（就是对应上图的5，6，7）, **首先redo log prepare,然后写入binlog,最后redo log commit**。主要是保证redo log事务写入顺序和binlog 事务顺序一致(通过事务id保证一致)。

完整流程如下：

**prepare**阶段：redo持久化到磁盘（redo group commit），并将**回滚段置为prepared状态**，此时binlog不做操作

**commit**阶段：innodb**释放锁**，**释放回滚段**，**设置undo log提交状态**，binlog持久化到磁盘，然后存储引擎层提交

mysql发生崩溃恢复的过程中，会根据redo log日志，结合 binlog 记录来做事务回滚：

1、如果redo log 和 binlog**都存在，逻辑上一致，那么提交事务**；

2、如果redo log存在而binlog**不存在，逻辑上不一致，那么回滚事务**；

最后大家可发现，**这里的两阶段提交，实际是存在与redo log与binlog**。所以当未开启binlog，那就是提交事务直接写到redo log里面。这也就是redo log事务两阶段提交，看场景区分的原因。

**两阶段提交的主要用意是：为了保证redolog和binlog数据的安全一致性**。只有在这两个日志文件逻辑上高度一致了。你才能放心地使用redolog帮你将数据库中的状态恢复成crash之前的状态，使用binlog实现数据备份、恢复、以及主从复制。而两阶段提交的机制可以保证这两个日志文件的逻辑是高度一致的。没有错误、没有冲突。

#### 隔离级别

为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

**脏读**(Drity Read)：**某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的**。（写未阻塞读）

**不可重复读**(Non-repeatable read)：在**一个事务的两次查询之中数据不一致**，这可能是**两次查询过程中间插入了一个事务更新**的原有的数据。（读未阻塞写）

**可重复读**指的是**在一个事务内**，最开始读到的数据和事务结束前的**任意时刻读到的同一批数据都是一致**的。通常针对数据更新（UPDATE）操作。

**幻读**(Phantom Read)：**在一个事务的两次查询中数据笔数不一致**，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112791)

SQL 标准定义了四个隔离级别：

- READ-UNCOMMITTED(RU，**读取未提交**)： 最低的隔离级别，**允许读取尚未提交的数据变更**，而未提交的数据可能会发生回滚，可能会**导致脏读、幻读或不可重复读**。
- READ-COMMITTED(RC，**读取已提交**)： **允许读取并发事务已经提交的数据**，可以阻止脏读，但由于在事务的执行中可以读取到其他事务提交的结果，所以在不同时间的相同 SQL 查询中，可能会得到不同的结果，但是**幻读或不可重复读**仍有可能发生。
- REPEATABLE-READ(RR，**可重复读)**： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但**幻读**仍有可能发生。
- SERIALIZABLE(**可串行化**)： **最高的隔离级别**，完全服从ACID的隔离级别。**所有的事务依次逐个执行**，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

这里需要注意的是：Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别

事务隔离机制的实现基于锁机制和并发调度。其中**并发调度使用的是MVVC（多版本并发控制）**，**通过保存修改的旧版本信息来支持并发一致性读和回滚等特性**。

#### 乐观锁和悲观锁

数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。**乐观并发控制（乐观锁）**和**悲观并发控制（悲观锁）**是并发控制主要采用的技术手段，**悲观锁和乐观锁是一种思想**，一种处理方式。

悲观锁：**假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作**。在**查询完数据**的时候就把事务锁起来，直到提交事务。**共享锁和排它锁是悲观锁的不同的实现**，它俩都属于悲观锁的范畴。

乐观锁：**假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。**在**修改数据**的时候把事务锁起来，通过version的方式来进行锁定。InnoDB **以乐观锁为理论基础的MVCC**（多版本并发控制）来避免不可重复读和幻读。

##### 两种锁的使用场景

从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像**乐观锁适用于写比较少的情况下**（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。

但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以**一般多写的场景下用悲观锁**就比较合适。

#### MVCC（多版本并发控制）

[参考](https://segmentfault.com/a/1190000037557620)、[参考](https://www.cnblogs.com/jelly12345/p/14889331.html)

##### 什么是多版本并发控制

**多版本并发控制**技术的英文全称是 **Multiversion Concurrency Control**，简称 **MVCC**。

**多版本并发控制（MVCC）** 是**通过保存数据在某个时间点的快照来实现并发控制**的。也就是说，不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。

简单来说，**多版本并发控制** 的思想就是**保存数据的历史版本**，通过对数据行的多个版本管理来实现数据库的并发控制。这样我们就可以通过比较版本号决定数据是否显示出来，读取数据的时候不需要加锁也可以保证事务的隔离效果。

可以认为 **多版本并发控制（MVCC）** 是**行级锁的一个变种**，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。

MySQL的大多数事务型存储引擎实现的都不是简单的行级锁。基于提升并发性能的考虑，它们一般都同时实现了多版本并发控制（MVCC）。不仅是MySQL，包括Oracle、PostgreSQL等其他数据库系统也都实现了MVCC，但各自的实现机制不尽相同，因为MVCC没有一个统一的实现标准，典型的有**乐观（optimistic）并发控制**和**悲观（pessimistic）并发控制**。

##### 多版本并发控制解决了哪些问题

###### 1. 读写之间阻塞的问题

通过 MVCC 可以让**读写互相不阻塞**，即**读不阻塞写，写不阻塞读**，这样就可以提升事务并发处理能力。

> 提高并发的演进思路：
>
> - 普通锁，只能串行执行；
> - 读写锁，可以实现读读并发；
> - 数据多版本并发控制，可以实现读写并发。

###### 2. 降低了死锁的概率

因为 InnoDB 的 MVCC 采用了**乐观锁**的方式，**读取数据时并不需要加锁，对于写操作，也只锁定必要的行**。

###### 3. 解决一致性读的问题

一致性读也被称为**快照读**，当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到这个时间点之后事务提交的更新结果。

##### 快照读与当前读

**快照读（SnapShot Read）** 是一种**一致性不加锁的读**，是**InnoDB并发如此之高的核心原因之一**。

> 这里的**一致性**是指，事务读取到的数据，要么是**事务开始前就已经存在的数据**，要么是**事务自身插入或者修改过的数据**。

快照读：像不加锁的select操作就是快照读，即**不加锁的非阻塞读**；**快照读的前提是隔离级别不是串行级别**，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，**快照读的实现是基于多版本并发控制**，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即**快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本**

不加锁的简单的 SELECT 都属于**快照读**，例如：

```n1ql
`SELECT * FROM t WHERE id=1`
```

与 **快照读** 相对应的则是 **当前读**，**当前读**就是**读取最新数据**，而不是历史版本的数据。

**当前读**：像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读，为什么叫当前读？就是它读取的是记录的最新版本，**读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁**。

加锁的 SELECT 就属于当前读，例如：

```pgsql
SELECT * FROM t WHERE id=1 LOCK IN SHARE MODE;

SELECT * FROM t WHERE id=1 FOR UPDATE;
```

##### Read View(读视图)

Read View就是**事务进行快照读操作的时候生产的读视图(Read View)**，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)
所以我们知道 **Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图**，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。

Read View遵循一个可见性算法，主要是**将要被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护）**，如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本

把Read View简单的理解成有三个全局属性

- trx_list（名字我随便取的）：一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID
- up_limit_id：记录trx_list列表中事务ID最小的ID
- low_limit_id：ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1

- 首先比较DB_TRX_ID < up_limit_id, 如果小于，则当前事务能看到DB_TRX_ID 所在的记录，如果大于等于进入下一个判断

- 接下来判断 DB_TRX_ID 大于等于 low_limit_id , 如果大于等于则代表DB_TRX_ID 所在的记录在Read View生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断

- 判断DB_TRX_ID 是否在活跃事务之中，trx_list.contains(DB_TRX_ID)，如果在，则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，你修改的结果，我当前事务是能看见的

  ![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/816762-20210616160511142-848091275.jpg)

##### **MVCC能解决什么问题，好处是？**

数据库并发场景有三种，分别为：

- 读-读：不存在任何问题，也不需要并发控制
- 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读
- 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失

##### **MVCC带来的好处是？**

多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题

- 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能
- 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题

可以形成两个组合：

- MVCC + 悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突
- MVCC + 乐观锁：MVCC解决读写冲突，乐观锁解决读写冲突

这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题

##### InnoDB 的 MVCC 是如何工作的

###### 1. InnoDB 是如何存储记录的多个版本的

**事务版本号**

每开启一个事务，我们都会从数据库中**获得一个事务 ID（也就是事务版本号）**，这个事务 ID 是自增长的，通过 ID 大小，我们就可以判断事务的时间顺序。

**行记录的隐藏列**

InnoDB 的叶子段存储了数据页，数据页中保存了行记录，而在**行记录中有一些重要的隐藏字段**：

- `DB_ROW_ID`：6-byte，**隐藏的行 ID，用来生成默认聚簇索引**。如果我们创建数据表的时候没有指定聚簇索引，这时 InnoDB 就会用这个隐藏 ID 来创建聚集索引。采用聚簇索引的方式可以提升数据的查找效率。
- `DB_TRX_ID`：6-byte，操作这个数据的事务 ID，也就是**最后一个对该数据进行插入或更新的事务 ID**。
- `DB_ROLL_PTR`：7-byte，回滚指针，也就是**指向这个记录的 Undo Log 信息**。

![InnoDB数据记录隐藏列](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbyzVU)

**Undo Log**

InnoDB 将行记录快照保存在了 Undo Log 里，我们可以在回滚段中找到它们，如下图所示：

![Undo Log回滚历史记录](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/bVbyzV9)

从图中能看到回滚指针将数据行的所有快照记录都通过链表的结构串联了起来，每个快照的记录都保存了当时的 db_trx_id，也是那个时间点操作这个数据的事务 ID。这样如果我们想要找历史快照，就可以通过遍历回滚指针的方式进行查找。

###### 2. 在 **可重复读（REPEATABLE READ）** 隔离级别下， InnoDB 的 MVCC 是如何工作的

**查询（SELECT）**

InnoDB 会根据以下两个条件检查每行记录：

1. InnoDB**只查找版本早于当前事务版本的数据行**（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以**确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的**。
2. **行的删除版本要么未定义，要么大于当前事务版本号**。这可以确保**事务读取到的行，在事务开始之前未被删除**。

只有符合上述两个条件的记录，才能返回作为查询结果。

**插入（INSERT）**

InnoDB为新插入的每一行保存当前系统版本号作为行版本号。

**删除（DELETE）**

InnoDB为删除的每一行保存当前系统版本号作为行删除标识。

删除在内部被视为更新，**行中的一个特殊位会被设置为已删除**。

**更新（UPDATE）**

InnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。

###### RC,RR级别下的InnoDB快照读有什么不同？

正是Read View生成时机的不同，从而造成RC（读取已提交）,RR（可重复读）级别下快照读的结果的不同

- 在RR级别下的某个事务的**对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View**，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见；
- 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见
- 而在RC级别下的，事务中，**每次快照读都会新生成一个快照和Read View**, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因

总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。

### MyISAM

设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。

提供了大量的特性，包括压缩表、空间数据索引等。

不支持事务。

不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。

**可以手工或者自动执行检查和修复操作**，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。

如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。

#### MyISAM 存储引擎的特点 

在 5.1 版本之前，MyISAM 是 MySQL 的默认存储引擎，MylSAM 并发性比较差，使用的场景比较少主要特点是: 

- **不支持事务操作**，ACID 的特性也就不存在了，这一设计是为了性能和效率考虑的， 
- **不支持外键操作**，如果强行增加外键，MySQL 不会报错，只不过外键不起作用。 
- MyISAM 默认的锁粒度是**表级锁**，所以并发性能比较差，加锁比较快，锁冲突比较少，不太容易发生死锁的情况。
- MyISAM 会在磁盘上**存储三个文件**，文件名和表名相同，扩展名分别是 frm(**存储表定义**)、MYD(MYData，**存储数据**)、MYI(MyIndex，**存储索引**)。 这里需要特别注意的是 MyISAM 只缓存索引文件，并不缓存数据文件。
-  MyISAM 支持的索引类型有全局索引(Full-Text)、B-Tree 索引、R-Tree 索引 
  - **Full-Text 索引**:它的出现是**为了解决针对文本的模糊查询效率较低**的问题。
  - **B-Tree 索引**:所有的索引节点都**按照平衡树的数据结构来存储**，所有的索引数据节点都在叶节点
  - **R-Tree 索引**：它的存储方式和 B-Tree 索引有一些区别，主要设计用于**存储空间和多维数据的字段做索引**目前的 MySQL 版本仅支持 geometry 类型的字段作索引，相对于 BTREE,RTREE 的优势在于范围查找。
- 数据库所在主机如果宕机，MyISAM 的数据文件容易损坏，而且难以恢复。
- 增删改查性能方面：SELECT 性能较高，适用于查询较多的情况

### 比较

- 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。
- 并发：**MyISAM 只支持表级锁，而 InnoDB 还支持行级锁**。
- 外键：InnoDB 支持外键。
- 备份：InnoDB 支持在线热备份。
- 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。
- 其它特性：MyISAM 支持压缩表和空间数据索引。

## MySQL 锁

[参考](https://blog.csdn.net/zcl_love_wx/article/details/81977447?spm=1001.2014.3001.5502)

### 一、mysql的锁类型

**(1) 共享/排它锁(Shared and Exclusive Locks)**

共享锁和排他锁是**InnoDB**引擎实现的标准**行级别锁**。

拿共享锁是为了让当前事务去读一行数据。

拿排他锁是为了让当前事务去修改或删除某一行数据。。

设置共享锁：select * from user where id = 1 LOCK IN SHARE MODE;

设置排他锁：select * from user where id = 1 FOR UPDATE;

**(2) 意向锁(Intention Locks)**

意向锁存在的意义在于，**使得行锁和表锁能够共存**。

**意向锁是表级别的锁**，用来说明事务**稍后**会对表中的**数据行加哪种类型的锁**(共享锁或独占锁)。

当一个事务对表加了意向排他锁时，另外一个事务在加锁前就会通过该表的意向排他锁知道前面已经有事务在对该表进行独占操作，从而等待。

**(3) 记录锁(Record Locks)**

记录锁是**索引记录上的锁，**例如：SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE;会阻止其他事务对c1=10的数据行进行插入、更新、删除等操作。

记录锁总是锁定索引记录。如果一个表没有定义索引，那么就会去锁定隐式的“聚集索引”。

**(4) 间隙锁(Gap Locks)**

**间隙锁是一个在索引记录之间的间隙上的锁**。

一个**间隙**可能跨越单个索引值、多个索引值，甚至为空。

对于使用唯一索引 来搜索唯一行的语句，只加记录锁不加间隙锁(这并不包括组合唯一索引）。

**(5) 临键锁(Next-key Locks)**

**Next-Key Locks是行锁与间隙锁的组合**。当InnoDB扫描索引记录的时候，会首先对选中的索引记录加上记录锁（Record Lock），然后再对索引记录两边的间隙加上间隙锁（Gap Lock）。

**(6) 插入意向锁(Insert Intention Locks)**

插入意向锁是**在数据行插入之前**通过插入操作设置的**间隙锁定类型**。

如果多个事务插入到相同的索引间隙中，如果它们不在间隙中的相同位置插入，则无需等待其他事务。例如：在4和7的索引间隙之间两个事务分别插入5和6，则两个事务不会发冲突阻塞。 

**(7) 自增锁(Auto-inc Locks)**

自增锁是事务**插入到有自增列的表**中而获得的一种特殊的**表级锁**。如果一个事务正在向表中插入值，那么任何其他事务都必须等待，保证第一个事务插入的行是连续的自增值。

### 二、锁的实现方式

#### 行锁

**InnoDB行锁是通过给索引加锁来实现的**，如果没有索引，InnoDB会通过隐藏的聚簇索引来对记录进行加锁（全表扫描，也就是表锁）。

但是，为了效率考量，MySQL做了优化，对于不满足条件的记录，会**放锁**，最终持有的，是满足条件的记录上的锁。但是不满足条件的记录上的加锁/放锁动作是不会省略的。所以在没有索引时，不满足条件的数据行会有加锁又放锁的耗时过程。

索引分为主键索引和非主键索引两种。如果一条sql语句操作了主键索引，MySQL就会锁定对应主键索引；如果一条语句操作了非主键索引，MySQL会先锁定非主键索引，再锁定对应的主键索引。



### 三、mysql锁在4种事务隔离级别里的应用

事务的四种隔离级别有：

-  **读未提交**(Read Uncommitted)

  此时select语句不加任何锁。此时并发最高，但会产生脏读。

-  **读提交**(Read Committed, RC)

  普通select语句是**快照读**

  update语句、delete语句、显示加锁的select语句（select … in share mode 或者 select … for update） 等，除了在**外键约束检查**和**重复键检查**时会**封锁区间**，其他情况都只使用**记录锁**；

-  **可重复读**(Repeated Read, RR)

  普通select语句也是**快照读**

  update语句、delete语句、显示加锁的select语句（select … in share mode 或者 select … for update）则要分情况：

  - 在**唯一索引**上使用**唯一的查询条件**，则使用**记录锁**。如: select * from user where id = 1;其中id建立了唯一索引。
  - 在**唯一索引**上使用 **范围查询**条件，则使用**间隙锁与临键锁**。如: select * from user where id >20;

-  **串行化**(Serializable)

  此时所有select语句都会被**隐式加锁**：select … in share mode.

### 四、快照读、当前读

**要理解前面四种隔离级别的加锁方式，对于MVCC、快照读、当前读 都是必须要理解的。**

MVCC并发控制中，读操作可以分成两类：**快照读** (snapshot read)与**当前读** (current read)。

快照读，读取的是记录的可见版本 (有可能是历史版本)，**不用加锁**。

当前读，读取的是记录的最新版本，并且，当前读返回的记录，**都会加上锁**，保证其他事务不会再并发修改这条记录。

#### **什么是多版本并发控制（MVCC：multi-version concurrency control ）**

1. MVCC定义：多版并发控制系统。可认为是行级锁的一个变种，它能够避免更多情况下的加锁操作。

2. **作用**：**避免一些加锁操作，提升并发性能**。

3. **实现**：通过在每行记录的后面保存行的**创建时间和过期时间或删除时间**（它们是隐藏的），这两个时间实际都是系统的版本号。**每开始一个新的事务，版本号都会自动增加**。

4. **具体原理**
   4.1） select：innoBD查询时会检查以下**两个条件**：一个是数据行的版本号早于当前事务的版本号；另一个是行的删除版本号，要么没有，要么大于当前事务的版本号。

   4.2）insert/delete：innoDB将当前的系统版本号作为新插入(删除)的数据行的版本号。

   4.3）update：先新插入一行数据，并将当前系统版本号作为行的版本号，同时将当前系统版本号作为原来行的删除版本号。更新主键时，聚集索引和普通索引都会产生两个版本；而更新非主键时，只要普通索引会产生两个版本。

5. **注意**：MVCC只在read committed和repeatable read两个隔离级别下工作。
   [参考：《高性能mysql》]

#### **快 照 读 是 哪 些**

一个正常的select…语句就是快照读。

快照读，使得在RR（repeatable read）级别下一个普通select...语句也能做到可重复读。即前面MVCC里提到的利用可见版本来保证数据的一致性。

#### **当 前 读 是 哪 些**

insert语句、update语句、delete语句、显示加锁的select语句（select… LOCK IN SHARE MODE、select… FOR UPDATE）是当前读。

**为什么insert、update、delete语句都属于当前读？**

这是因为这些语句在执行时，都会执行一个读取当前数据最新版本的过程。

当前读的SQL语句，InnoDB是逐条与MySQL Server交互的。即先对一条满足条件的记录加锁后，再返回给MySQL Server，当MySQL Server做完DML操作后，再对下一条数据加锁并处理。

### 五、查看行级锁争用情况


执行SQL:mysql> show status like 'InnoDB_row_lock%';

```sql
mysql> show status like 'InnoDB_row_lock%';



+-------------------------------+-------+



| Variable_name                 | Value |



+-------------------------------+-------+



| InnoDB_row_lock_current_waits | 0     |



| InnoDB_row_lock_time          | 0     |



| InnoDB_row_lock_time_avg      | 0     |



| InnoDB_row_lock_time_max      | 0     |



| InnoDB_row_lock_waits         | 0     |



+-------------------------------+-------+
```


如果发现锁争用比较严重，还可以通过设置InnoDB Monitors 来进一步观察发生锁冲突的表、数据行等，并分析锁争用的原因。如：

设置监视器：mysql> create table InnoDB_monitor(a INT) engine=InnoDB;

查看：mysql> show engine InnoDB status;

停止查看：mysql> drop table InnoDB_monitor;

具体参考：[InnoDB Monitor](https://blog.csdn.net/zyz511919766/article/details/50147283)

### 六、死锁

**什么是死锁：一般是由于两个事务同时操作两个表，但加锁的顺序是不一致出现的**。比如A事务先锁a表，B事务先锁b表；当A去锁b表的时候发现b表被B事务锁住了，要等待；当B事务去锁a表的时候发现a表被A锁住了。于是出现了死锁

如何发现死锁： 在InnoDB的事务管理和锁定机制中，有专门检测死锁的机制，会在系统中产生死锁之后的很短时间内就检测到该死锁的存在，一般像一些运维系统是能发现的

解决办法：**回滚较小的那个事务**

在REPEATABLE-READ隔离级别下，如果两个线程同时对相同条件记录用SELECT…FOR UPDATE加排他锁，在没有符合该条件记录情况下，两个线程都会加锁成功。程序发现记录尚不存在，就试图插入一条新记录，如果两个线程都这么做，就会出现死锁。这种情况下，将隔离级别改成READ COMMITTED，就可避免问题。

如何判断事务大小：**事务各自插入、更新或者删除的数据量**

注意：

当产生死锁的场景中涉及到不止InnoDB存储引擎的时候，InnoDB是没办法检测到该死锁的，这时候就只能通过锁定超时限制参数InnoDB_lock_wait_timeout来解决。

###  七、优化行级锁定

（1）要想合理利用InnoDB的行级锁定，做到扬长避短，我们必须做好以下工作： 

a)尽可能**让所有的数据检索都通过索引来完成**，从而避免InnoDB因为无法通过索引键加锁而升级为表级锁定； 

b)合**理设计索引**，让InnoDB在索引键上面加锁的时候尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定而影响其他Query的执行； 

c)**尽可能减少基于范围的数据检索过滤条件**，避免因为间隙锁带来的负面影响而锁定了不该锁定的记录； 

d)尽量**控制事务的大小**，减少锁定的资源量和锁定时间长度； 

e)在业务环境允许的情况下，尽量使用较低级别的事务隔离，以减少MySQL因为实现事务隔离级别所带来的附加成本。

（2）由于InnoDB的行级锁定和事务性，所以肯定会产生死锁，下面是一些比较常用的减少死锁产生概率的小建议：

a)类似业务模块中，尽可能按照相同的访问顺序来访问，防止产生死锁； 

b)在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 

c)对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率。

### **查看SQL语句的锁信息**

**查看sql句的锁信息前，需要做如下几件事**：

查看事务的隔离级别：

- 通过show global variables like “tx_isolation”; 命令查看。
  可通过执行set session transaction isolation level repeatable read;更改成我们想要隔离级别，隔离级别取值如下：
  read uncommitted、read committed、repeatable read、serializable

保证事务为手动提交：

- 通过show global variables like “autocommit”;查看。
  如果为ON，则通过执行set session autocommit=0;改为手动提交。

保证间隙锁开启：

- 通过show global variables like “innodb_locks%”;查看
  OFF时表示开启。默认是OFF

## 四、数据类型

### 整型

TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。

INT(11) 中的**数字只是规定了交互工具显示字符的个数**，对于存储和计算来说是没有意义的。

### 浮点数

FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。

FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。

### 字符串

主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。

**VARCHAR 这种变长类型能够节省空间**，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。

在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。

### 时间和日期

MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。

#### 1. DATETIME

能够保存从 1000 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。

**它与时区无关**。

默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。

#### 2. TIMESTAMP

和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。

它和时区有关，也就是说**一个时间戳在不同的时区所代表的具体时间是不同的**。

MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。

默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。

应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。

## 五、切分

### 水平切分

水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。

当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。

[![img](https://camo.githubusercontent.com/29dbc15634de8e798e7ed68f50fd898b923a5fd9ac6fd2a271af9bdd84e96167/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f36336332393039662d306335662d343936662d396665352d6565393137366233316162612e6a7067)](https://camo.githubusercontent.com/29dbc15634de8e798e7ed68f50fd898b923a5fd9ac6fd2a271af9bdd84e96167/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f36336332393039662d306335662d343936662d396665352d6565393137366233316162612e6a7067)



### 垂直切分

垂直切分是将一张表按列切分成多个表，通常是**按照列的关系密集程度进行切分**，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。

在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。

[![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65313330653562382d623139612d346631652d623836302d3232333034303532356366362e6a7067)](https://camo.githubusercontent.com/eb35b961164a9f970ee6e1da5b6eeefb5610619553494efa9b0a5007524c99d6/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f65313330653562382d623139612d346631652d623836302d3232333034303532356366362e6a7067)



### Sharding 策略

- **哈希取模**：hash(key) % N；
- **范围**：可以是 ID 范围也可以是时间范围；
- **映射表**：使用单独的一个数据库来存储映射关系。

### Sharding 存在的问题

#### 1. 事务问题

使用分布式事务来解决，比如 XA 接口。

#### 2. 连接

可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。

#### 3. ID 唯一性

- 使用全局唯一 ID（GUID）
- 为每个分片指定一个 ID 范围
- 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)

## 六、复制

DML（Data Manipulation Language，**数据操作语言**）：**用于检索或者修改数据**。

DML包括： 

- SELECT：用于检索数据；

- INSERT：用于增加数据到数据库；
- UPDATE：用于从数据库中修改现存的数据 
- DELETE：用于从数据库中删除数据。

DDL（Data Definition Language，**数据定义语言**）： 用于定义数据的结构，比如 创建、修改或者删除数据库对象。

DDL包括：DDL语句可以**用于创建用户和重建数据库对象**。下面是DDL命令：

- CREATE TABLE：创建表

-  ALTER TABLE
- DROP TABLE：删除表
- CREATE INDEX
- DROP INDEX

DCL（Data Control Language，**数据控制语言**）：用于**定义数据库用户的权限**。

DCL包括：

- ALTER PASSWORD 
- GRANT 
- REVOKE 
- CREATE SYNONYM

### 主从复制

主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。

###### 主从复制的作用

- 主数据库出现问题，可以切换到从数据库。
- 可以进行数据库层面的**读写分离**。
- 可以在从数据库上进行日常备份。

###### MySQL主从复制解决的问题

- 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份
- 负载均衡：降低单个服务器的压力
- 高可用和故障切换：帮助应用程序避免单点失败
- 升级测试：可以用更高版本的MySQL作为从库

###### MySQL主从复制工作原理

- 在主库上把数据更高记录到二进制日志
- 从库将主库的日志复制到自己的中继日志
- 从库读取中继日志的事件，将其重放到从库数据中

基本原理流程，3个线程以及之间的关联

主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中；

从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，**放进自己的relay log中**；

从：sql执行线程——**执行relay log中的语句**；

**复制过程**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112807)

Binary log：主数据库的二进制日志

Relay log：从服务器的中继日志

第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。

第二步：salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。

第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。

### 读写分离

**主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。**

读写分离能提高性能的原因在于：

- 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
- 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
- 增加冗余，提高可用性。

**读写分离常用代理方式来实现**，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

[![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652d70726f78792e706e67)](https://camo.githubusercontent.com/7f9279aeb3dd23a8a0a64895594bd76ac9fce2dfb6bc24974a07cc83888c6fc9/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652d70726f78792e706e67)

#### 读写分离有哪些解决方案？

读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。因为**主从复制要求slave不能写只能读**（如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时你需要按照前面提到的手动同步一下slave）。

###### 方案一：使用mysql-proxy代理

优点：直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用

缺点：降低性能， 不支持事务

###### 方案二：使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。

如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，**所有的select 都访问salve库**，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。

###### 方案三：使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务.

缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。

### 主从同步的延迟的原因

一个服务器开放Ｎ个链接给客户端来连接的，这样有会有大并发的更新操作, 但是**从服务器的里面读取binlog 的线程仅有一个**， 当某个SQL在从服务器上执行的时间稍长或者由于某个SQL要进行锁表就会导致，**主服务器的SQL大量积压**，未被同步到从服务器里。这就导致了主从不一致， 也就是主从延迟。

#### 主从同步延迟的解决办法

 实际上主从同步延迟根本没有什么一招制敌的办法， 因为所有的SQL必须都要在从服务器里面执行一遍，但是主服务器如果不断的有更新操作源源不断的写入， 那么一旦有延迟产生， 那么延迟加重的可能性就会原来越大。 当然我们可以做一些缓解的措施。

  a. 我们知道因为主服务器要负责更新操作， 他对安全性的要求比从服务器高， 所有有些设置可以修改，比如sync_binlog=1，innodb_flush_log_at_trx_commit = 1 之类的设置，而slave则不需要这么高的数据安全，完全可以将sync_binlog设置为0或者关闭binlog，innodb_flushlog， innodb_flush_log_at_trx_commit **也 可以设置为0来提高sql的执行效率 这个能很大程度上提高效率**。另外就是使用比主库更好的硬件设备作为slave。

  b. 就是把，一台从服务器当度作为备份使用， 而不提供查询， 那边他的负载下来了， 执行relay log 里面的SQL效率自然就高了。

  c. **增加从服务器**喽，这个目的还是分散读的压力， 从而降低服务器负载。

## 备份

### 备份计划，mysqldump以及xtranbackup的实现原理

#### (1)备份计划

视库的大小来定，**一般来说 100G 内的库，可以考虑使用 mysqldump** 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以**每天进行都进行全量备份**(mysqldump 备份出来的文件比较小，压缩之后更小)。

**100G 以上的库，可以考虑用 xtranbackup** 来做，备份速度明显要比 mysqldump 要快。**一般是选择一周一个全备**，其余**每天进行增量备份**，备份时间为业务低峰期。

#### (2)备份恢复时间

**物理备份恢复快，逻辑备份恢复慢**

这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考

20G的2分钟（mysqldump）

80G的30分钟(mysqldump)

111G的30分钟（mysqldump)

288G的3小时（xtra)

3T的4小时（xtra)

逻辑导入时间一般是备份时间的5倍以上

#### (3)备份恢复失败如何处理

首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。

#### (4)mysqldump和xtrabackup实现原理

- mysqldump

**mysqldump 属于逻辑备份**。加入–single-transaction 选项可以进行一致性备份。**后台进程会先设置 session 的事务隔离级别为 RR**(SET SESSION TRANSACTION ISOLATION LEVELREPEATABLE READ，可重复读)，之后**显式开启一个事务**(START TRANSACTION /*!40100 WITH CONSISTENTSNAPSHOT* /)，这样就**保证了该事务里读到的数据都是事务事务时候的快照**。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，**再记录下数据库此时 binlog 的位置**(showmaster status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务

- Xtrabackup:

**xtrabackup 属于物理备份**，**直接拷贝表空间文件**，**同时不断扫描产生的 redo 日志并保存下来**。最后完成 innodb 的备份后，**会做一个 flush engine logs 的操作**(老版本在有 bug，在5.6 上不做此操作会丢数据)，**确保所有的 redo log 都已经落盘**(涉及到事务的两阶段提交

概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事情)。然后还需要 flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。

#### 数据表损坏的修复方式有哪些？

使用 myisamchk 来修复，具体步骤：

- 1）修复前将mysql服务停止。
- 2）打开命令行方式，然后进入到mysql的/bin目录。
- 3）执行myisamchk –recover 数据库所在路径/*.MYI

使用repair table 或者 OPTIMIZE table命令来修复，REPAIR TABLE table_name 修复表 OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了OPTIMIZE TABLE命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库）

## 面试题

[参考](https://segmentfault.com/a/1190000040112777)

[参考2](https://blog.51cto.com/wangshiyu/4874168)

### 什么是数据库事务

[参考](https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1/9744607)

**数据库事务**（简称：**事务**）是[数据库管理系统](https://zh.wikipedia.org/wiki/数据库管理系统)执行过程中的一个逻辑单位，由一个有限的[数据库](https://zh.wikipedia.org/wiki/数据库)操作序列构成。

数据库事务( transaction)是访问并可能操作各种[数据项](https://baike.baidu.com/item/数据项/3227309)的一个数据库操作[序列](https://baike.baidu.com/item/序列/1302588)，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。事务由事务开始与事务结束之间执行的全部数据库操作组成。

### ACID

**ACID**，是指[数据库管理系统](https://zh.wikipedia.org/wiki/数据库管理系统)（[DBMS](https://zh.wikipedia.org/wiki/DBMS)）在写入或更新资料的过程中，为保证[事务](https://zh.wikipedia.org/wiki/数据库事务)（transaction）是正确可靠的，所必须具备的四个特性：[原子性](https://zh.wikipedia.org/w/index.php?title=原子性&action=edit&redlink=1)（atomicity，或称不可分割性）、[一致性](https://zh.wikipedia.org/wiki/一致性_(数据库))（consistency）、[隔离性](https://zh.wikipedia.org/wiki/隔離性)（isolation，又称独立性）、[持久性](https://zh.wikipedia.org/wiki/持久性)（durability）。

在数据库系统中，一个事务是指：由一系列数据库操作组成的一个完整的逻辑过程。例如银行转帐，从原账户扣除金额，以及向目标账户添加金额，这两个数据库操作的总和，构成一个完整的逻辑过程，不可拆分。这个过程被称为一个事务，具有ACID特性。ACID的概念在[ISO](https://zh.wikipedia.org/wiki/ISO)/IEC 10026-1:1992文件的第四段内有所说明。

- 原子性（Atomicity）：**一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成**，不会结束在中间某个环节。事务在执行过程中发生错误，会被[回滚](https://zh.wikipedia.org/wiki/回滚_(数据管理))（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。
- [一致性](https://zh.wikipedia.org/wiki/一致性_(数据库))（Consistency）：**在事务开始之前和事务结束以后，数据库的完整性没有被破坏**。这表示写入的资料必须完全符合所有的预设[约束](https://zh.wikipedia.org/wiki/数据完整性)、[触发器](https://zh.wikipedia.org/wiki/触发器_(数据库))、[级联回滚](https://zh.wikipedia.org/wiki/级联回滚)等。
- [事务隔离](https://zh.wikipedia.org/wiki/事務隔離)（Isolation）：数据库**允许多个并发事务同时对其数据进行读写和修改的能力**，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。
- [持久性](https://zh.wikipedia.org/wiki/持久性)（Durability）：事务处理结束后，**对数据的修改就是永久的**，即便系统故障也不会丢失。

### 事务完整流程

[详解MySQL执行事务的语法和流程](https://segmentfault.com/a/1190000039032584)

平时执行一个单独的sql语句其实也是一个事务。例如：

Insert into t values (xxxxx);

这就是一个事务，只不过mysql帮我们自动commit了。

所以其实执行这样的单句sql，也是一个事务也会记录到undo和redo中

```
sql:update test set name = 'test' where id=2;
```

1.事务开始

2.申请锁资源，对id=2这行数据上排他锁

3.将需要修改的data pages读取到innodb_buffer_cache

4.记录id=2的数据到undo log

5.记录id=2修改后的数据到redo log buffer

6.将buffer cache中id=2得name改为test

7.commit，触发二阶段提交2pc

8.事务结束

![mysql_一条更新语句的执行流程_weixin_34284188的博客-CSDN博客](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/ade8a223210cdce5e5d50a3e659a94bdca7.jpg)

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20211011004118702.png)

**二阶段提交(2pc two phase commit):**

[参考](https://zhuanlan.zhihu.com/p/343449447)

二阶段提交,**首先redo log prepare,然后写入binlog,最后redo log commit**。主要是保证redo log事务写入顺序和binlog 事务顺序一致(通过事务id保证一致)。

完整流程如下：

prepare阶段：redo持久化到磁盘（redo group commit），并将回滚段置为prepared状态，此时binlog不做操作

commit阶段：innodb释放锁，释放回滚段，设置undo log提交状态，binlog持久化到磁盘，然后存储引擎层提交

mysql发生崩溃恢复的过程中，会根据redo log日志，结合 binlog 记录来做事务回滚：

1、如果redo log 和 binlog都存在，逻辑上一致，那么提交事务；

2、如果redo log存在而binlog不存在，逻辑上不一致，那么回滚事务；

最后大家可发现，这里的两阶段提交，实际是存在与redo log与binlog。所以当未开启binlog，那就是提交事务直接写到redo log里面。这也就是redo log事务两阶段提交，看场景区分的原因。

**两阶段提交的主要用意是：为了保证redolog和binlog数据的安全一致性**。只有在这两个日志文件逻辑上高度一致了。你才能放心地使用redolog帮你将数据库中的状态恢复成crash之前的状态，使用binlog实现数据备份、恢复、以及主从复制。而两阶段提交的机制可以保证这两个日志文件的逻辑是高度一致的。没有错误、没有冲突。

### 三大范式

**1．第一范式(确保每列保持原子性)**

**每个列**都不可以再拆分

**2．第二范式(确保表中的每列都和主键相关)**

在第一范式的基础上，非主键列完全依赖于主键，而**不能是依赖于主键的一部分**。

> 依赖的意思：
>
> 例如 b 依赖于 a， 由 a 可得到 b，a -> b，即在表中查询 a 列 就能找到 b

**3．第三范式(确保每列都和主键列直接相关,而不是间接相关)**

在第二范式的基础上，**非主键列只依赖于主键**，**不依赖于其他非主键**。

在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。

### MySQL 有关权限的表都有哪几个？

MySQL 服务器通过权限表来控制用户对数据库的访问，权限表存放在 MySQL 数 据库里，由 mysql_install_db 脚本初始化。这些权限表分别 **user，db， table_priv，columns_priv 和 host**。下面分别介绍一下这些表的结构和内容： 

- user 权限表：记录**允许连接到服务器的用户帐号信息**，里面的权限是**全局级**的。 
- db 权限表：记录**各个帐号在各个数据库**上的操作权限。 
- table_priv 权限表：记录**数据表级**的操作权限。 
- columns_priv 权限表：记录**数据列级**的操作权限。
-  host 权限表：**配合 db 权限表对给定主机上数据库级操作权限作更细致的控制**。这个权限表不受 GRANT 和 REVOKE 语句的影响。

### SQL语句主要分为哪几类

**数据定义语言**（Data Ddefinition Language, DDL）CREATE，DROP，ALTER

主要为以上操作 **即对逻辑结构等有操作**的，其中包括**表结构，视图和索引**。

**数据查询语言**（Data Query Language, DQL）SELECT， 查

这个较为好理解即查询操作，**以select关键字。各种简单查询，连接查询**等都属于DQL。

**数据操纵语言**（Data Manipulation Language, DML）INSERT，UPDATE，DELETE

主要为以上操作**即对数据进行操作**的，对应上面所说的查询操作 DQL与DML共同构建了多数初级程序员常用的增删改查操作。而查询是较为特殊的一种 被划分到DQL中。

**数据控制功能**（Data Control Language， DCL）GRANT，REVOKE，COMMIT，ROLLBACK

主要为以上操作 即**对数据库安全性完整性等有操作**的，可以简单的理解为权限控制等。

### MySQL有哪几种log

重做日志(redo log)、回滚日志(undo log)、二进制日志(binlog)、错误日志(errorlog)、慢查询日志(slow query log)、一般查询日志(general log)，中继日志(relay log)

错误日志：记录**出错信息**，也记录一些警告信息或者正确的信息。

查询日志：记录所有**对数据库请求**的信息，不论这些请求是否得到了正确的执行。

慢查询日志：设置一个阈值，**将运行时间超过该值的所有SQL语句都记录到慢查询**的日志文件中。

二进制日志：记录**对数据库执行更改**的所有操作。

中继日志：中继日志也是二进制日志，用来给slave 库恢复

事务日志：重做日志redo和回滚日志undo

- UNDO 日志：**复制事务执行前的数据**，用于在事务发生异常时回滚数据。
- REDO 日志：**记录在事务执行中，每条对数据进行更新的操作**，当事务提交时，该内容将被刷新到磁盘。

### mysql有哪些数据类型

　MySQL支持多种类型，大致可以分为四类：**数值型、浮点型、日期/时间和字符串(字符)类型**。

　　**1、数值类型**

　　MySQL支持所有标准SQL数值数据类型。

　　**这些数值类型包括**严格数值数据类型(INTEGER、SMALLINT、DECIMAL和NUMERIC)，以及近似数值数据类型(FLOAT、REAL和DOUBLE PRECISION)。

　　关键字INT是INTEGER的同义词，关键字DEC是DECIMAL的同义词。

　　作为SQL标准的扩展，MySQL也支持整数类型TINYINT、MEDIUMINT和BIGINT。下面的表显示了需要的每个整数类型的存储和范围：

![1.png-600](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1597043441153050.png-600)

　　**2、 浮点型**

![2.png-600](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1597043451403103.png-600)

　　比如，我们发的工资，一般都带有小数。

　　**3、日期和时间类型**

　　表示时间值的日期和时间类型为DATETIME、DATE、TIMESTAMP、TIME和YEAR。

　　每个时间类型有一个有效值范围和一个"零"值，当指定不合法的MySQL不能表示的值时使用"零"值。

　　TIMESTAMP类型有专有的自动更新特性，将在后面描述。

![3.png-600](https://ss.html.cn/upload/image/480/100/214/1597043460541653.png-600)

　　**在生产里，日期时间型，往往用的比较少，而是用数字类型来取代日期类型！**

　　**4 字符串类型**

　　字符串类型指CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM和SET。该节描述了这些类型如何工作以及如何在查询中使用这些类型。

![4.png-600](https://ss.html.cn/upload/image/483/938/702/1597043468142374.png-600)

　　CHAR和VARCHAR类型类似，但它们保存和检索的方式不同。它们的最大长度和是否尾部空格被保留等方面也不同。在存储或检索过程中不进行大小写转换。

　　BINARY和VARBINARY类类似于CHAR和VARCHAR，不同的是它们包含二进制字符串而不要非二进制字符串。也就是说，它们包含字节字符串而不是字符字符串。这说明它们没有字符集，并且排序和比较基于列值字节的数值值。

　　有4种TEXT类型：TINYTEXT、TEXT、MEDIUMTEXT和LONGTEXT。这些对应4种BLOB类型，有相同的最大长度和存储需求。

　　**ENUM**是枚举类型

　　**SET**是集合类型不同于ENUM类型，它是一个排列组合。假如有abc，它可以选择a或b或c，也有选择是ab,ac,bc，也可以选择abc。

**总结**

　　这些数据类型可以用于数据表或存储过程或以后的函数中，也就是说只要用到数据类型的时候，可以从我们刚讲到的**数值型、浮点型、日期/时间和字符串(字符)类型中任意选择**

### MySQL 的 Binlog 有有几种录入格式？

分别有什么区别？ 有三种格式statement，row 和 mixed。 

- statement 模式下，**每一条会修改数据的 SQL 都会记录在 Binlog 中**。不需要记录每一行的变化，减少了 Binlog 日志量，节约了 IO，提高性能。由于 **sql 的执行是有上下文的**，因此在**保存的时候需要保存相关的信息**，同时还有一些使用了函数之类的语句无法被记录复制。 
- row 级别下，**不记录 SQL 语句上下文相关信息，仅保存哪条记录被修改。**记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如 alter table)，因此这种模式的文件保存的信息太 多，日志量太大。 
- mixed，一种折中的方案，普通操作使用 statement 记录，当无法使用 statement 的时候使用 row。会影响一致性的操作时，使用statement 。

### MySQL 存储引擎 MyISAM 与 InnoDB 区别

- **锁粒度方面**：由于锁粒度不同，InnoDB 比 MyISAM 支持更高的并发;**InnoDB  的锁粒度为行锁**、**MyISAM 的锁粒度为表锁**、行锁需要对每一行进行加锁， 所以锁的开销更大，但是能解决脏读和不可重复读的问题，相对来说也更 容易发生死锁 
- 可恢复性上：**由于 InnoDB 是有事务日志的**，所以在产生由于数据库崩溃等条件后，可以根据日志文件进行恢复。而 MyISAM 则**没有事务日志**，没有热备份。
- 查询性能上：**MylSAM 要优于 InnoDB**。因为 InnoDB 在查询过程中，是**需要维护数据缓存**（InnoDB缓存池，InnoDB buffer pool），而且查询过程是**先定位到行所在的数据块，然后在从数据块中定位到要查找的行**;而 **MyISAM 可以直接定位到数据所在的内存地址**，可以直接找到数据。 
- **表结构文件**上：MyISAM 的表结构文件包括：frm(表结构定义),.MYI(索 引),.MYD(数据);而 InnoDB 的表数据文件为：ibd 和 frm(表结构定义)。

### MyISAM 索引与 InnoDB 索引的区别？

- InnoDB 索引是聚簇索引，MyISAM 索引是非聚簇索引。 
-  InnoDB 的主键索引的叶子节点存储着行数据，因此主键索引非常高效。
-  **MyISAM 索引的叶子节点存储的是行数据地址**，需要再寻址一次才能得到数据。 
- InnoDB **非主键索引的叶子节点存储的是主键和其他带索引的列数据**，因此查询时做到覆盖索引会非常高效。

### 什么是索引？ 

**索引是一种特殊的文件(InnoDB 数据表上的索引是表空间的一个组成部分)**，**它们包含着对数据表里所有记录的引用指针**。 **索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据**。索引的实现通常使用 B 树及其变种 B+树。 更通俗的说，**索引就相当于目录**。为了方便查找书中的内容，通过对内容建立 索引形成目录。索引是一个文件，它是要占据物理空间的。

### 索引有哪些优缺点？

 索引的优点  

- 可以大大**加快数据的检索速度**，这也是创建索引的最主要的原因。 
- 通过使用索引，可以在查询的过程中，**使用优化隐藏器**，提高系统的性能。 

索引的缺点  

- 时间方面：**创建索引和维护索引要耗费时间**，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 
- 空间方面：**索引需要占物理空间**。

### 索引失效的情况有哪些？索引何时会失效？

#### 列与列对比

某个表中，有两列（id和c_id）都建了单独索引，下面这种查询条件不会走索引

```javascript
select * from test where id=c_id;
```

#### 存在NULL值条件

[参考](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247524678&idx=3&sn=a153fd4fae16c357d55414e067d7bf25&chksm=eb50e470dc276d6676c923b597cbd28cf29e7a31393f1f03afedaf22f6aaf9e0e5517b9bdb32&scene=21#wechat_redirect)

[我们在设计数据库表时，应该尽力避免NULL值出现，如果非要不可避免的要出现NULL值，也要给一个DEFAULT值，数值型可以给0、-1之类的， 字符串有时候给空串有问题，就给一个空格或其他。如果索引列是可空的，是不会给其建索引的，索引值是少于表的count(*)值的，所以这种情况下，执行计划自然就去扫描全表了](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247524678&idx=3&sn=a153fd4fae16c357d55414e067d7bf25&chksm=eb50e470dc276d6676c923b597cbd28cf29e7a31393f1f03afedaf22f6aaf9e0e5517b9bdb32&scene=21#wechat_redirect)

```
select * from test where id is not null;
```

#### **NOT条件**

我们知道建立索引时，给每一个索引列建立一个条目，如果查询条件为等值或范围查询时，索引可以根据查询条件去找对应的条目。反过来当查询条件为非时，索引定位就困难了，执行计划此时可能更倾向于全表扫描，这类的查询条件有：<>、NOT、in、not exists

```javascript
select * from test where id<>500;
select * from test where id in (1,2,3,4,5);
select * from test where not in (6,7,8,9,0);
select * from test where not exists (select 1 from test_02 where test_02.id=test.id);
```

#### LIKE通配符

当使用模糊搜索时，尽量采用后置的通配符，例如：name||’%’，因为走索引时，其会从前去匹配索引列，这时候是可以找到的，如果采用前匹配，那么查索引就会很麻烦，比如查询所有姓张的人，就可以去搜索’张%’。

相反如果你查询所有叫‘明’的人，那么只能是%明。这时候索引如何定位呢？前匹配的情况下，执行计划会更倾向于选择全表扫描。后匹配可以走INDEX RANGE SCAN。

所以业务设计的时候，尽量考虑到模糊搜索的问题，要更多的使用后置通配符。

```javascript
select * from test where name like 张||'%';
```

#### 条件上包括函数

查询条件上尽量不要对索引列使用函数，比如下面这个SQL

```javascript
select * from test where upper(name)='SUNYANG';
```

这样是不会走索引的，因为索引在建立时会和计算后可能不同，无法定位到索引。但如果查询条件不是对索引列进行计算，那么依然可以走索引。比如

```javascript
select * from test where name=upper('sunyang');
--INDEX RANGE SCAN
```

这样的函数还有：to_char、to_date、to_number、trunc等

#### 复合索引前导列区分大

当复合索引前导列区分小的时候，我们有INDEX SKIP SCAN，当前导列区分度大，且查后导列的时候，前导列的分裂会非常耗资源，执行计划想，还不如全表扫描来的快，然后就索引失效了。

```javascript
select * from test where owner='sunyang';
```

#### 数据类型的转换

当查询条件存在隐式转换时，索引会失效。[一条垃圾SQL，把 64 核 CPU 快跑崩了](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247493425&idx=1&sn=93e802fc6a74dc27538e6e0b200bad7b&chksm=eb506207dc27eb1103613907dbb512ba65201014be639ab3fdc5e3f9bc53207ef019aac608b8&scene=21#wechat_redirect)，这篇可以看下。

比如在[数据库](https://cloud.tencent.com/solution/database?from=10680)里id存的number类型，但是在查询时，却用了下面的形式：

```javascript
select * from sunyang where id='123';
```

#### Connect By Level

使用connect by level时，不会走索引。

#### 谓词运算

我们在上面说，不能对索引列进行函数运算，这也包括加减乘除的谓词运算，这也会使索引失效。

建立一个sunyang表，索引为id，看这个SQL：

```javascript
select * from sunyang where id/2=:type_id;
```

这里很明显对索引列id进行了’/2’除二运算，这时候就会索引失效，这种情况应该改写为：

```javascript
select * from sunyang where id=:type_id*2;
```

就可以使用索引了。另外，关注公众号Java技术栈，在后台回复：面试，可以获取我整理的 Java/ 数据库系列面试题和答案，非常齐全。

#### Vistual Index

先说明一下，虚拟索引的建立是否有用，需要看具体的执行计划，如果起作用就可以建一个，如果不起作用就算了。普通索引这么建：

```javascript
create index idx_test_id on test(id);
```

虚拟索引Vistual Index这么建：

```javascript
create index idx_test_id on test(id) nosegment;
```

做了一个实验，首先创建一个表：

```javascript
CREATE TABLE test_1116( 
id number, 
a number 
); 

CREATE INDEX idx_test_1116_id on test_1116(id); 
CREATE INDEX idx_test_1116_a on test_1116(a)nosegment; 
```

其中id为普通索引，a为虚拟索引。

在表中插入十万条数据

```javascript
begin 
for i in 1 .. 100000 loop 
        insert into test_1116 values (i,i); 
end loop; 
commit; 
end; 
```

接着分别去执行下面的SQL看时间，由于在内网机做实验，图贴不出来，数据保证真实性。

```javascript
select count(id) from test_1116;
--第一次耗时：0.061秒
--第二次耗时：0.016秒

select count(a) from test_1116; 
--第一次耗时：0.031秒
--第二次耗时：0.016秒
```

因为在执行过一次后，oracle对结果集缓存了，所以第二次执行耗时不走索引，走内存就都一样了。

可以看到在这种情况下，虚拟索引比普通索引快了一倍。

具体虚拟索引的使用细节，这里不再展开讨论。更多 Java 技术教程可以看这个：https://github.com/javastacks/javastack

#### Invisible Index

Invisible Index是oracle 11g提供的新功能，对优化器（还接到前面博客里讲到的CBO吗）不可见，[MySQL](https://cloud.tencent.com/product/cdb?from=10680) 也有，[MySQL 8.0 中的索引可以隐藏了](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw==&mid=2247521326&idx=1&sn=6b88035c6bfb4a62086a57022b49b8d6&chksm=eb501118dc27980e55ad2df5f0cd5c96731b59afc760452d7bbc09bcef16afb90671b7aa3f11&scene=21#wechat_redirect)。我感觉这个功能更主要的是测试用，假如一个表上有那么多索引，一个一个去看执行计划调试就很慢了，这时候不如建一个对表和查询都没有影响的Invisible Index来进行调试，就显得很好了。

通过下面的语句来操作索引

```javascript
alter index idx_test_id invisible;
alter index idx_test_id visible;
```

如果想让CBO看到Invisible Index，需要加入这句：

```javascript
alter session set optimizer_use_invisible_indexes = true;
```

### 索引的使用场景

- **where** 
  ![img](https://segmentfault.com/img/remote/1460000040112783)

上图中，根据id查询记录，因为id字段仅建立了主键索引，因此此SQL执行可选的索引只有主键索引，如果有多个，最终会选一个较优的作为检索的依据。

```sql
-- 增加一个没有建立索引的字段
alter table innodb1 add sex char(1);
-- 按sex检索时可选的索引为null
EXPLAIN SELECT * from innodb1 where sex='男';
```

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112784)

可以尝试在一个字段未建立索引时，根据该字段查询的效率，然后对该字段建立索引（alter table 表名 add index(字段名)），同样的SQL执行的效率，你会发现查询效率会有明显的提升（数据量越大越明显）。

- **order by**

当我们使用order by将查询结果**按照某个字段排序**时，如果该字段没有建立索引，那么**执行计划会将查询出的所有数据使用外部排序**（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这个操作是很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。

但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么**由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可**。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的）

- **join**

**对join语句匹配关系（on）涉及的字段建立索引能够提高效率**

- **索引覆盖**

如果要查询的字段**都建立过索引**，那么引擎会**直接在索引表中查询而不会访问原始数据**（否则**只要有一个字段没有建立索引就会做全表扫描**），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。

这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。

### 索引有哪几种类型？ 

**主键索引**：数据列不允许重复，不允许为 NULL，一个表**只能有一个**主键。 

**唯一索引**：数据列不允许重复，允许为 NULL 值，一个表允许**多个**列创建唯一索引。 

- 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索 引。 
- 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引。 

**普通索引**：基本的索引类型，没有唯一性的限制，允许为 NULL 值。

- 可以通过 ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 

- 可以通过 ALTER TABLE table_name ADD INDEX index_name(column1,  column2, column3);创建组合索引。 

**全文索引**：是目前搜索引擎使用的一种关键技术。 

- 可以通过 ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引。

### 索引的数据结构（B树，hash）

[参考](https://developer.aliyun.com/ask/281188)

索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。

1）B树索引

mysql通 过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引）

![1.jpg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/4e146c2f85664adaba86496264a6c9bb.jpg)

查询方式：

主键索引区:PI(关联保存的时数据的地址)按主键查询,

普通索引区:si(关联的id的地址,然后再到达上面的地址)。所以按主键查询,速度最快

B+tree性质：

1.）n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。

2.）所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。

3.）所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。

4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。

5.）B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。

2）哈希索引

简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，**与这条数据的行指针一并存入Hash表的对应位置**；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。

![2.jpg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/a61549a9eaab450aa1751808f925d9ce.jpg)

### Hash索引和B+树所有有什么区别或者说优劣呢?

[参考](https://book.itheima.net/study/1258677827423715330/1277261541762146305/1290887310354763777)

#### 底层原理

- hash索引底层是hash表
- b+树底层是多路平衡查找树

#### 等值查询速度

一般情况下，Hash索引在等值查询时效率高

#### 范围查询的支持

hash索引不支持，b+树索引支持。

hash索引的顺序和原顺序不一致，故不支持。

B+树左节点小于父节点，右节点大于父节点，故支持。

#### 其他

- 哈希表不支持索引排序，不支持组合索引的最左前缀原则，不支持模糊查询
- 哈希索引不能避免回表操作，hash索引在使用聚集索引或者覆盖索引的时候可以不回表
- 哈希索引不稳定，如果出现hash碰撞，效率较低。

综上所述，在多数情况下，使用稳定且速度较快的b+树索引

### 数据库为什么使用B+树而不是B树

[参考](https://blog.csdn.net/zhoucheng05_13/article/details/79825246)

#### B树和B+树的区别

B树和B+树的区别主要有两点：

1. 在B树中，你可以将键和值存放在内部节点和叶子节点，但在B+树中，内部节点都是键，没有值。叶子节点同时存放键和值
2. B+树的叶子节点有一条链相连，而B+树的叶子节点各自独立。

#### 使用B+树的好处

1. **由于B+树的内部节点只存放键，不存放值**，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。
2. B+树的叶节点由一条链相连，因此，**当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点**，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间

#### 使用B树的好处

**B树可以在内部节点同时存储键和值**，因此，**把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率**。这种特性使得B树在特定数据重复多次查询的场景中更加高效。

#### 数据库为什么使用B+树而不是B树

因为就是上面提到的B+树的好处。数据库的数据读取都是需要进行代价巨大的磁盘IO操作，因此，**更快地缩小范围和更少的读取次数是数据库需要关注的重点**。而B+树在这些点上比B树做的更好。这就是为什么数据库要选用B+树作为底层实现。

### MySQL 中有哪几种锁？ 

从锁的粒度分：

- 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 
- 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
- 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。

从锁的类别分：

- 共享锁： 又叫做**读锁**。 当用户要进行数据的**读取**时，对数据加上共享锁。共享锁可以同时加上多个。
- 排他锁： 又叫做**写锁**。 当用户要进行数据的**写入**时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。

数据库遵循的是两段锁协议，将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁）

- **加锁阶段**：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得S锁（共享锁，其它事务可以继续加共享锁，但不能加排它锁），在进行写操作之前要申请并获得X锁（排它锁，其它事务不能再获得任何锁）。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。
- **解锁阶段**：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。

### InnoDB存储引擎中的锁

- Record lock：**单个行记录**上的锁 
- Gap lock：间隙锁，锁定一个范围，**不包括记录本身**
- Next-key lock：record+gap **锁定一个范围**，**包含记录本身**

### MySQL中InnoDB引擎的行锁是怎么实现的？

答：InnoDB是**基于索引来完成行锁**

```n1ql
例: select * from tab_with_index where id = 1 for update;
```

**for update 可以根据条件来完成行锁锁定**，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起

select[加锁](https://www.cnblogs.com/rjzheng/p/9950951.html)

###### InnoDB存储引擎的锁的算法有三种

- Record lock：**单个行记录**上的锁 
- Gap lock：间隙锁，锁定一个范围，**不包括记录本身**
- Next-key lock：record+gap **锁定一个范围**，**包含记录本身**

###### 相关知识点：

- innodb**对于行的查询**使用next-key lock
- Next-locking keying**为了解决Phantom Problem幻读问题**
- 当**查询的索引含有唯一属性**时，将next-key lock降级为record key
- Gap锁设计的目的是**为了阻止多个事务将记录插入到同一范围内**，而这会导致幻读问题的产生
- 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock）
  - A. 将事务隔离级别设置为RC 
  - B. 将参数innodb_locks_unsafe_for_binlog设置为1

### 什么是死锁？怎么解决？

死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。

常见的解决死锁的方法

- 1、如果不同程序会并发存取多个表，**尽量约定以相同的顺序访问表**，可以大大降低死锁机会。
- 2、在**同一个事务**中，尽可能做到**一次锁定所需要的所有资源**，减少死锁产生概率；
- 3、对于非常容易产生死锁的业务部分，可以尝试使用**升级锁定颗粒度**，通过表级锁定来减少死锁产生的概率；

如果业务处理不好可以用**分布式事务锁或者使用乐观锁**

### 数据库的乐观锁和悲观锁是什么？怎么实现的？

数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。**乐观并发控制（乐观锁）**和**悲观并发控制（悲观锁）**是并发控制主要采用的技术手段，悲观锁和乐观锁是一种思想，一种处理方式。

悲观锁：**假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作**。在**查询完数据**的时候就把事务锁起来，直到提交事务。**实现方式：使用数据库中的锁机制**，即数据库中的排他锁（写锁）和共享锁（读锁）。读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。

乐观锁：**假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。**在**修改数据**的时候把事务锁起来，通过version的方式来进行锁定。**实现方式：乐观锁一般会使用版本号机制或CAS算法实现**，即使用了MVCC（多版本并发控制）。在创建数据行的时候，会额外的维护一个version的数据列，用来表示当前的版本，当读取数据的时候，将版本号也读取出来，当对数据进行更新的时候，同时也会使得**当前的version加一**，此时提交数据库的信息，当**提交的表单数据的version数字小于或者等于当前表单行的版本号的时候，就说明这是一个过期的数据，也就不会被提交**。

#### 两种锁的使用场景

从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像**乐观锁适用于写比较少的情况下**（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。

但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以**一般多写的场景下用悲观锁**就比较合适。

### 什么是脏读？幻读？不可重复读？

**脏读**(Drity Read)：**某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的**。

**不可重复读**(Non-repeatable read)：在**一个事务的两次查询之中数据不一致**，这可能是两次查询过程中间插入了一个事务更新的原有的数据。

**可重复读**指的是**在一个事务内**，最开始读到的数据和事务结束前的**任意时刻读到的同一批数据都是一致**的。通常针对数据更新（UPDATE）操作。

**幻读**(Phantom Read)：**在一个事务的两次查询中数据笔数不一致**，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。

### MySQL 中 InnoDB 支持的四种事务隔离级别名称，以及逐级之间的区别？ 

[参考](https://www.51cto.com/article/699269.html)

为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112791)

SQL 标准定义了四个隔离级别：

- READ-UNCOMMITTED(**读取未提交**)： 最低的隔离级别，**允许读取尚未提交的数据变更**，可能会**导致脏读、幻读或不可重复读**。
- READ-COMMITTED(**读取已提交**)： **允许读取并发事务已经提交的数据**，可以阻止脏读，但是**幻读或不可重复读**仍有可能发生。
- REPEATABLE-READ(**可重复读)**： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但**幻读**仍有可能发生。
- SERIALIZABLE(**可串行化**)： **最高的隔离级别**，完全服从ACID的隔离级别。**所有的事务依次逐个执行**，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。

这里需要注意的是：Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别

事务隔离机制的实现基于锁机制和并发调度。其中**并发调度使用的是MVVC（多版本并发控制）**，**通过保存修改的旧版本信息来支持并发一致性读和回滚等特性**。

#### MVCC的作用

1、每一行的数据都会存在一个版本，每一次的数据更新的时候都会更新该版本。

2、修改时Copy出当前版本随意修改，各个事务之间无干扰。

3、把修改前的数据存放于undo log，通过回滚指针的主数据关联。

4、修改成功(commit)啥都不做，失败的话就恢复undo log中的数据(rollback)

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）**并不会有任何性能损失。

InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别

- SELECT时，读取创建版本号<=当前事务版本号，删除版本号为空或>当前事务版本号。
- INSERT时，保存当前事务版本号为行的创建版本号
- DELETE时，保存当前事务版本号为行的删除版本号
- UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行

#### 快照读与当前读

快照读：就是select

```
select * from table ….;
```

当前读：**特殊的读操作，插入/更新/删除操作**，属于当前读，处理的都是当前的数据，需要加锁。

```
select * from table where ? lock in share mode;
select * from table where ? for update;
insert;
update ;
delete;
```

#### ReadView

InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。具体实现可见

### 隔离级别与锁的关系

[参考](https://segmentfault.com/a/1190000022361260)

- 在Read Uncommitted级别下，读取数据**不需要加任何锁**，这样就不会跟被修改的数据上的排他锁冲突。
  - 不做任何加锁和MVCC操作。

- 在Read Committed级别下

  - 对于读操作，不加锁，为快照读，**每次读取都使用最新的事务版本号生成最新的ReadView**。

  - 对于写操作，**每次加行锁**（**提交事务时才解锁**，并且更新数据库的事务版本）。

    这样的话就解决了脏读问题，只要事务没提交，数据库的事务版本号就不会更新，那么ReadView中的数据永远都是这个新事务之前的数据。

    但是没有解决不可重复读的问题，因为一个事务内每次查询的ReadView版本不一致。

- 在Repeatable Read级别下

  - 对于读操作，不加锁，只有第一次读取的时候才会生成一个ReadView。

  - 对于写操作，加临键锁Next-key锁（行锁+GAP间隙锁），就是除了给当行记录加锁，还会给当行记录周围区间加间隙锁。

    ReadView不同的生成策略解决了不可重复读的问题，由于一个事务内**用的都是第一次查询的ReadView**，所以查出来的数据都是一致的。

    而Next-key锁机制又在一定程度上解决了幻读的问题，由于GAP锁会把一些相邻的区间也锁上，那么插入时就会被阻塞，从而在一定程度上解决了幻读的问题，但是又没有完全解决，因为之后相距比较远的数据还是可以插入。

- SERIALIZABLE 是限制性最强的隔离级别
  - 悲观锁机制实现
  - 对于读操作，加读锁。
  - 对于写操作，加写锁。
  - 读读不互斥，读写互斥，写写互斥。
  - 由于读写互斥，完全解决了三个问题，但是并发度比较低。

### char 和 varchar 的区别？ 

char 和 varchar 类型在**存储和检索方面**有所不同 

- **char的长度是不可变的，而varchar的长度是可变的**
- char **列长度固定为创建表时声明的长度**，长度值范围是 1 到 255
- 当 char 值被存储时，**它们被用空格填充到特定长度**，检索 char 值时需删除尾随空格。
- **char的存取速度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。**

char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。**varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。**

**两者的存储数据都非unicode的字符数据。**

**nchar和nvarchar是存储的unicode字符串数据**

### varchar(10)和varchar(100)有什么区别

先举个例子：如果要存储`'hello12345'`这个字符串，使用`varchar(10)`和`varchar(100)`存储，占用磁盘空间是一样的么？

答案是：**占用磁盘的存储空间是一样的**。

**既然存储时磁盘占用空间一样，还有什么其他的区别吗？**

虽然使用`varchar(100)`和`varchar(10)`存储`'hello12345'`字符串占用的磁盘空间一样，但是**消耗的内存不一样，更长的列消耗的内存会更多**。因为MySQL通常会**分配固定大小的内存块来保存内部值**。尤其是使用临时表进行排序会操作时，会消耗更多的内存。在使用磁盘进行排序时，也是一样。

所以此时`varchar(100)` ***会消耗更多的内存。***

#### varchar(10)和varchar(100)的优劣势是什么？

因为**涉及到文件排序或者基于磁盘的临时表时，更长的列会消耗更多的内存**，所以在使用使用时，我们不能太过浪费内存空间，还是需要评估实际使用的长度来设置字符的长度。***推荐冗余10%的长度***（因业务而异）。

所使用varchar(10)会更加***节约内存空间***，但是实际业务中字符长度一旦超过10就需要更改表结构，在表数据量特别大时，***不易拓展***。

而这时使用更长的列：varchar(100)无需更改表结构，***业务拓展性更好***。

### 主键和候选键有什么区别？

[参考](https://zhuanlan.zhihu.com/p/33394962)

 表格的每一行都由主键唯一标识,一个表只有一个主键。 主键也是候选键。按照惯例，候选键可以被指定为主键，并且可以用于任何外键引用。

**超键(**super key)：在关系中能**唯一标识元组**的属性集称为关系模式的超键

**候选键**(candidate key)：**不含有多余属性的超键称为候选键**

**主键(**primary key)：用户选作元组标识的一个候选键程序主键

**外键**(foreign key)：如果关系模式R1中的某属性集不是R1的主键，而是另一个关系R2的主键则该属性集是关系模式R1的外键。

### 如何在 Unix 和 MySQL 时间戳之间进行转换？ 

UNIX_TIMESTAMP 是从 MySQL 时间戳转换为 Unix 时间戳的命令

 FROM_UNIXTIME 是从 Unix 时间戳转换为 MySQL 时间戳的命令。

### MyISAM 表类型将在哪里存储，并且还提供其存储格式？

 每个 MyISAM 表格以三种格式存储在磁盘上： 

- “.frm”文件 **存储表定义** 
- 数据文件具有“.MYD”（MYData）扩展名 
- 索引文件具有“.MYI”（MYIndex）扩展名

### MySQL 里记录货币用什么字段类型好

**NUMERIC 和 DECIMAL 类型**被 MySQL 实现为同样的类型，这在 SQL92 标准允许。 他们被用于保存值，该值的准确精度是极其重要的值，例如与金钱有关的数 据。当声明一个类是这些类型之一时，精度和规模的能被(并且通常是)指定。 

例如： salary DECIMAL(9,2)

在这个例子中，9(precision)代表将被用于存储值的总的小数位数，而 2(scale)代表将被用于存储小数点后的位数。 因此，在这种情况下，能被存储在 salary 列中的值的范围是从-9999999.99 到 9999999.99。

### 创建索引时需要注意什么？ 

- **非空字段**：应该指定列为 NOT NULL，除非你想存储 NULL。在 MySQL 中，**含有空值的列很难进行查询优化**，因为它们使得索引、索引的统计信息以及比较运算更加复杂。应该用 0、一个特殊的值或者一个空串代替空值； 
- **取值离散大的字段**：（变量各个取值之间的差异程度）的列放到联合索引 的前面，可以通过 count()函数查看字段的差异值，返回值越大说明字段的**唯一值越多字段的离散程度高**； 
- **索引字段越小越好**：数据库的数据存储以页为单位一页存储的数据越多一 次 I/O 操作获取的数据越大效率越高。
- **最左前缀匹配原则**，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
- 较频繁作为查询条件的字段才去创建索引
- **更新频繁字段不适合创建索引**
- 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。
- 定义有外键的数据列一定要建立索引。
- 对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。

### 创建索引的三种方式，删除索引

#### 创建表时创建索引

```sql
CREATE TABLE user_index2 (
	id INT auto_increment PRIMARY KEY,
	first_name VARCHAR (16),
	last_name VARCHAR (16),
	id_card VARCHAR (18),
	information text,
	KEY name (first_name, last_name),
	FULLTEXT KEY (information),
	UNIQUE KEY (id_card)
);
```

#### 更新表时创建索引

```sql
ALTER TABLE table_name ADD INDEX index_name (column_list);
```

这种方式可以用来创建普通索引，唯一索引，主键索引；=

其中table_name是要增加索引的表名，column_list指出对哪些列进行索引，多列时各列之间用逗号分隔。

索引名index_name可自己命名，缺省时，MySQL将根据第一个索引列赋一个名称。另外，ALTER TABLE允许在单个语句中更改多个表，因此可以在同时创建多个索引。

#### 使用CREATE INDEX命令创建

```arduino
CREATE INDEX index_name ON table_name (column_list);
```

CREATE INDEX可对表增加普通索引或UNIQUE索引。（但是，不能创建PRIMARY KEY索引）

#### 删除索引

根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名

```sql
alter table user_index drop KEY name;
alter table user_index drop KEY id_card;
alter table user_index drop KEY information;
```

删除主键索引：`alter table 表名 drop primary key`（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）：

需要取消自增长再行删除：

```sql
alter table user_index
-- 重新定义字段
MODIFY id int,
drop PRIMARY KEY
```

但通常不会删除主键，因为设计主键一定与业务逻辑无关。

### 索引设计原则

[参考](https://juejin.cn/post/6844904009329803277)、[创建索引的原则](https://developer.aliyun.com/article/6719)

根据数据库的功能，可以在数据库设计器中创建三种索引

- 唯一索引：唯一索引是不允许其中任何两行具有相同索引值的索引
- 主键索引：表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时， 它允许对数据的快速访问
- 聚集索引：表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表 只能包含一个聚集索引

> 选择索引的最终目的是为了使查询的速度变快。下面给出的原则是最基本的准则，但不能拘泥于这些准则，应该根据应用的实际情况进行分析和判断，选择最合适的索引方式。

#### 1.索引最左匹配原则

- 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。
- 如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），遇到范围查询(>、<、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。因此，列的排列顺序决定了可命中索引的列数。

例子：

如有索引(a, b, c, d)，查询条件a = 1 and b = 2 and c > 3 and d = 4，则会在每个节点依次命中a、b、c，无法命中d。(很简单：索引命中只能是相等的情况，不能是范围匹配)

> 明白最左匹配原则，对我们设计索引和编写高效SQL语句非常有帮助

#### 2.为经常需要排序、分组操作的字段建立索引

经常需要ORDER BY、GROUP BY、DISTINCT等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。

> 分组字段或者排序字段应该创建索引

#### 3.为常作为查询条件的字段建立索引

如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度。因此，为这样的字段建立索引，可以提高整个表的查询速度。

> Where 子句中经常使用的字段应该创建索引

#### 4.限制索引的数目

索引的数目不是越多越好。每个索引都需要占用磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。

#### 5.尽量选择区分度高的列作为索引

尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录

#### 6.索引列不能参与计算

索引列不能参与计算，保持列“干净”，比如from*unixtime(create*time) = ’2019-12-02’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create*time = unix*timestamp(’2014-05-29’);

> 即索引列不能带函数，否则会导致索引失效

#### 7.扩展索引

尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可

#### 8.条件带like 注意事项

like 模糊查询中，右模糊查询(abc%)会使用索引，而(%abc)和(%abc%)会放弃索引而使用全表扫描

#### 9.尽量使用数据量少的索引

如果索引的值很长，那么查询的速度会受到影响。例如，对一个CHAR(100)类型的字段进行全文检索需要的时间要比对CHAR(10)类型的字段需要的时间要多。

#### 10.尽量使用前缀来索引

如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT和BLOG类型的字段，进行全文检索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度。

#### 11.删除不再使用或者很少使用的索引

表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。

#### 12. =和in可以乱序。

比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式

#### 13.联合查询

联合查询，子查询等多表操作时关连字段要加索引




### 使用索引查询一定能提高查询的性能吗？为什么 

通常，**通过索引查询数据比全表扫描要快**。但是我们也必须注意到它的代价。 **索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改**。 这意味着每条记录的 INSERT，DELETE，UPDATE 将为此多付出 4，5 次的磁盘 I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。

**使用索引查询不一定能提高查询性能**，索引范围查询(INDEX RANGE SCAN)适用于两种情况： 

- 基于一个范围的检索，一般查询返回结果集小于表中记录数的 30% 
- 基于非唯一性索引的检索

### 百万级别或以上的数据如何删除 

关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所 以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的 IO,会降低增/改/删的执行效率。所以，在我们删除数据库 百万级别数据的时候，查询 MySQL 官方手册得知**删除数据的速度和创建的索引数量是成正比的**。 

- 所以我们想要删除百万数据的时候可以**先删除索引**（此时大概耗时三分多 钟） 
- 然后**删除其中无用数据**（此过程需要不到两分钟） 
- **删除完成后重新创建索引**(此时数据较少了)创建索引也非常快，约十分钟 左右。 
- 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

### 什么是最左前缀原则？什么是最左匹配原则

顾名思义，就是**最左优先，在创建多列索引时，要根据业务需求，where 子句中使用最频繁的一列放在最左边**。

 最左前缀匹配原则，非常重要的原则，**MySQL 会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配**，比如 a = 1 and b = 2 and c > 3 and  d = 4 如果建立(a,b,c,d)顺序的索引，d 是用不到索引的，如果建立 (a,b,d,c)的索引则都可以用到，a,b,d 的顺序可以任意调整。 =和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任 意顺序，MySQL 的查询优化器会帮你优化成索引可以识别的形式。

### 什么是聚簇索引？何时使用聚簇索引与非聚簇索引 

- 聚簇索引：**将数据存储与索引放到了一块，找到索引也就找到了数据** 
- 非聚簇索引：将数据存储于索引分开结构，索引结构的**叶子节点指向了数据的对应行**，myisam 通过 key_buffer 把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找 到磁盘相应数据，这也就是为什么索引不在 key buffer 命中时，速度慢的原因。

#### **何时使用聚簇索引与非聚簇索引**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/rcil81aoyd.jpeg)

**聚簇索引具有唯一性**

由于聚簇索引是将数据跟索引结构放到一块，因此一个表仅有一个聚簇索引

#### 一个误区：把主键自动设为聚簇索引

**聚簇索引默认是主键**，如果表中没有定义主键，InnoDB 会选择一个**唯一的非空索引**代替。如果没有这样的索引，InnoDB 会**隐式定义一个主键**来作为聚簇索引。InnoDB 只聚集在同一个页面中的记录。包含相邻键值的页面可能相距甚远。**如果你已经设置了主键为聚簇索引，必须先删除主键，然后添加我们想要的聚簇索引，最后恢复设置主键即可**。

此时其他索引只能被定义为非聚簇索引。这个是最大的误区。有的主键还是无意义的自动增量字段，那样的话Clustered index对效率的帮助，完全被浪费了。

刚才说到了，聚簇索引性能最好而且具有唯一性，所以非常珍贵，必须慎重设置。**一般要根据这个表最常用的SQL查询方式来进行选择，某个字段作为聚簇索引，或组合聚簇索引**，这个要看实际情况。

记住我们的**最终目的**就是**在相同结果集情况下，尽可能减少逻辑IO**。

**结合图再仔细点看**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/2w157wzq2u.jpeg)

![img](https://ask.qcloudimg.com/http-save/yehe-2823867/2q05hsflfa.jpeg)

1. InnoDB使用的是聚簇索引，将**主键组织到一棵B+树**中，而**行数据就储存在叶子节点**上，若使用"where id = 14"这样的条件查找主键，则**按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据**。
2. 若**对Name列进行条件搜索，则需要两个步骤**：**第一步在辅助索引B+树中检索Name，到达其叶子节点获取对应的主键**。第二步**使用主键在主索引B+树种再执行一次B+树检索操作，最终到达叶子节点即可获取整行数据**。（**重点在于通过其他键需要建立辅助索引**）

MyISM使用的是非聚簇索引，**非聚簇索引的两棵B+树看上去没什么不同**，节点的结构完全一致只是存储的内容不同而已，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。表数据存储在独立的地方，这两颗B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于**索引树是独立的，通过辅助键检索无需访问主键的索引树**。

#### 聚簇索引的优势

看上去聚簇索引的效率明显要低于非聚簇索引，因为**每次使用辅助索引检索都要经过两次B+树查找**，这不是多此一举吗？聚簇索引的优势在哪？

1. 由于**行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问**，不必访问磁盘。这样**主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回**了，**如果按照主键Id来组织数据，获得数据更快**。
2. 辅助索引使用主键作为"指针"而不是使用地址值作为指针的好处是，**减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间**，换来的好处是InnoDB**在移动行时无须更新辅助索引中的这个"指针"**。也就是说行的位置（实现中通过16K的Page来定位）会随着[数据库](https://cloud.tencent.com/solution/database?from=10680)里数据的修改而发生变化（前面的B+树节点分裂以及Page的分裂），使用聚簇索引就可以保证不管这个主键B+树的节点如何变化，辅助索引树都不受影响。
3. 聚簇索引适合用在**排序的场合**，非聚簇索引不适合
4. 取出一定范围数据的时候，使用用聚簇索引
5. 二级索引需要两次索引查找，而不是一次才能取到数据，因为存储引擎第一次需要通过二级索引找到索引的叶子节点，从而找到数据的主键，然后在聚簇索引中用主键再次查找索引，再找到数据
6. 可以把**相关数据保存在一起**。例如实现电子邮箱时，可以根据用户 ID 来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件。如果没有使用聚簇索引，则每封邮件都可能导致一次磁盘 I/O。

#### 聚簇索引的劣势

1. **维护索引很昂贵，特别是插入新行或者主键被更新导至要分页(page split)的时候**。建议在大量插入新行后，选在负载较低的时间段，通过OPTIMIZE TABLE优化表，因为必须被移动的行数据可能造成碎片。使用独享表空间可以弱化碎片
2. 表因为使用UUId（随机ID）作为主键，使数据存储稀疏，这就会出现聚簇索引有可能有比全表扫面更慢，

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/iywj5q0imm.jpeg)

所以建议使用int的 auto_increment 作为主键

![img](https://ask.qcloudimg.com/http-save/yehe-2823867/td2fso5cth.jpeg)

主键的值是顺序的，所以 InnoDB 把每一条记录都存储在上一条记录的后面。当达到页的最大填充因子时（InnoDB 默认的最大填充因子是页大小的 15/16，留出部分空间用于以后修改），下一条记录就会写入新的页中。一旦数据按照这种顺序的方式加载，主键页就会近似于被顺序的记录填满（二级索引页可能是不一样的）

1. 如果主键比较大的话，那辅助索引将会变的更大，因为**辅助索引的叶子存储的是主键值；过长的主键值，会导致非叶子节点占用占用更多的物理空间**

#### 为什么主键通常建议使用自增id

**聚簇索引的数据的物理存放顺序与索引顺序是一致的**，即：**只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的**。如果主键不是自增id，那么可以想 象，它会干些什么，不断地调整数据的物理地址、分页，当然也有其他一些措施来减少这些操作，但却无法彻底避免。但，如果是**自增的，那就简单了，它只需要一 页一页地写，索引结构相对紧凑，磁盘碎片少，效率也高**。

因为**MyISAM的主索引并非聚簇索引，那么他的数据的物理地址必然是凌乱的，拿到这些物理地址，按照合适的算法进行I/O读取，于是开始不停的寻道不停的旋转**。**聚簇索引则只需一次I/O**。（强烈的对比）

不过，如果**涉及到**[**大数据**](https://cloud.tencent.com/solution/bigdata?from=10680)**量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的**。

#### mysql中聚簇索引的设定

聚簇索引**默认是主键**，如果表中没有定义主键，InnoDB 会选择一个**唯一的非空索引**代替。如果没有这样的索引，InnoDB 会**隐式定义一个主键**来作为聚簇索引。**InnoDB 只聚集在同一个页面中的记录。包含相邻健值的页面可能相距甚远。**

### 联合索引是什么？为什么需要注意联合索引中的顺序？

[参考](https://book.itheima.net/study/1258677827423715330/1277261541762146305/1290887364041854978)

MySQL可以使用多个字段同时建立一个索引,叫做联合索引.在联合索引中,**如果想要命中索引,需要按照建立索引时的字段顺序挨个使用,否则无法命中索引.**

**具体原因**:

MySQL使用索引时需要索引有序,假设现在建立了"name,age,school"的联合索引,那么索引的排序为: 先按照name排序,如果name相同,则按照age排序,如果age的值也相等,则按照school进行排序.

当进行查询时,此时索引仅仅按照name严格有序,因此必须首先使用name字段进行等值查询,之后对于匹配到的列而言,其按照age字段严格有序,此时可以使用age字段用做索引查找,,,以此类推.因此在建立联合索引的时候应该注意索引列的顺序,一般情况下,将查询需求频繁或者字段选择性高的列放在前面.此外可以根据特例的查询或者表结构进行单独的调整.

### 索引失效的 10 种场景

[参考](https://mp.weixin.qq.com/s/9Kb01g7LqPzWtoAKsLiT2w)

<img src="https://mmbiz.qpic.cn/mmbiz_png/ibJZVicC7nz5ianHlF0AzOr530aPCbgeARxMduoIvwQDZJJOOSIKFHwAdXQTOzBnEWOlvY9lR9matXZ8joTruQl4Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

如果使用了`or`关键字，那么它前面和后面的字段都要加索引，不然所有的索引都会失效，这是一个大坑。

如果order by语句中没有加where或limit关键字，该sql语句将不会走索引。

如果对多个索引进行order by，索引也失效了

order by如果满足最左匹配原则，还是会走索引。不满足最左匹配原则的情况，就不会有索引

### MySQL 连接器

首先需要在 MySQL 客户端登陆才能使用，所以**需要个连接器来连接用户和 MySQL 数据库**，我们 一般是使用 

```
mysql-u 用户名-p 密码
```

来进行 MySQL 登陆，和服务端建立连接。在完成 TCP 握手后，连接器会根据你输入的用户名和密码验证你的登录身份。如果用户名或者密码错误，MySQL 就 会提示 Access denied for user，来结束执行。如果登录成功后，MySQL 会根据权限表中的记录来判定你的权限。

### MySQL 查询缓存

连接完成后，你就可以执行 SQL 语句了，这行逻辑就会来到第二步：**查询缓存**。 MySQL 在得到一个执行请求后，会**首先去查询缓存 中查找，是否执行过这条 SQL 语句，之前执行过的语句以及结果会以 key-value 对的形式，被直接放在内存中**。key 是查询语句，value 是查询的结果。 

如果通过 key 能够查找到这条 SQL 语句，就直接妾返回 SQL 的执行结果。

如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果就会被放入查询缓存中。 可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，效率会很高。

### MySQL 分析器

如果没有命中查询，就开始执行真正的 SQL 语句。

- 首先，**MySQL 会根据你写的 SQL 语句进行解析**，分析器会先做**词法分析**，你写的 SQL 就是由多个字符串和空格组成的一条 SQL 语句，MySQL **需要识别出里面的字符串是什么，代表什么**。 
- 然后**进行语法分析**，根据词法分析的结果，**语法分析器会根据语法规则， 判断你输入的这个 SQL 语句是否满足 MySQL 语法**。如果 SQL 语句不正确， 就会提示 You have an error in your SQL syntax。

### MySQL 优化器

经过分析器的词法分析和语法分析后，你这条 SQL 就合法了，MySQL 就知道你要做什么了。但是在执行前，还需要进行优化器的处理，**优化器会判断你使用了哪种索引，使用了何种连接，优化器的作用就是确定效率最高的执行方案**。

### MySQL 执行器

MySQL 通过分析器知道了你的 SQL 语句是否合法，你想要做什么操作，通过优化器知道了该怎么做效率最高，然后就进入了执行阶段，开始执行这条 SQL 语 句在执行阶段，**MySQL 首先会判断你有没有执行这条语句的权限**，没有权限的话，就会返回没有权限的错误。如果有权限，就打开表继续执行。打开表的时候，**执行器就会根据表的引擎定义，去使用这个引擎提供的接口**。对于有索引 的表，执行的逻辑也差不多。

### 什么是临时表，何时删除临时表？

 **什么是临时表?**

MySQL 在执行 SQL 语句的过程中 通常会**临时创建一些存储中间结果集的表**，**临时表只对当前连接可见**，**在连接关闭时，临时表会被删除并释放所有表空间**。

 临时表分为两种：**一种是内存临时表，一种是磁盘临时表**，什么区别呢?内存临时表使用的是 MEMORY 存储引擎，而临时表采用的是 MylSAM 存储引擎。 

MySQL 会在下面这几种情况产生临时表。 

- **使用 UNION 查询**：UNION 有两种，一种是 UNION，一种是 UNION ALL，它们都用于**联合查询**;区别是使用 UNION 会去掉两个表中的重复数据，相当于对结果集做了一下去重(distinct)。使用 UNIONALL，则不会排重，返回所有的行。使用 UNION 查询会产生临时表。 
- **使用 TEMPTABLE 算法或者是 UNION 查询中的视图**。TEMPTABLE 算法是一种创建临时表的算法，它是将结果放置到临时表中，意味这要 MySQL 要先创建 好一个临时表，然后将结果放到临时表中去，然后再使用这个临时表进行 相应的查询。
- **ORDER BY 和 GROUPBY 的子句不一样时也会产生临时表。** 
- DISTINCT 查询并且加上 ORDER BY 时; 
- SQL 中用到 SQL_SMALL_RESULT 选项时;如果查询结果比较小的时候，可以加 上 SQL SMALL RESULT 来优化，产生临时表 
- FROM 中的子查询; 
- EXPLAIN 查看执行计划结果的 Extra 列中，如果使用 Using Temporary 就 表示会用到临时表。

### 谈谈 SQL 优化的经验

- 查询语句无论是使用哪种判断条件等于、小于、大于，**WHERE 左侧的条件查询字段不要使用函数或者表达式** 
- **使用 EXPLAIN 命令优化你的 SELECT 查询**，对于复杂、效率低的 SQL 语 句，我们通常是使用 explainsql 来分析这条 SQL 语句，这样方便我们分 析，进行优化。
- **当你的 SELECT 查询语句只需要使用一条记录时，要使用 LIMIT 1**。**不要直接使用 SELECT***，而应该使用具体需要查询的表字段，因为使用 EXPLAIN 进行分析时，SELECT * 使用的是全表扫描，也就是 type =all 。
- **为每一张表设置一个 ID 属性**。 
- 避免在 WHERE 字句中对字段进行 NULL  
- **判断避免在 WHERE 中使用!或>操作符** 
- **使用 BETWEEN AND 替代 IN** 
- 为搜索字段创建索引 
- 选择正确的存储引擎，InnoDB、MyISAM、MEMORY 等
- **使用 LIKE%abc%不会走索引**，而使用 LIKE abc%会走索引。
- **对于枚举类型的字段(即有固定罗列值的字段)，建议使用 ENUM 而不是 VARCHAR**，如性别、星期、类型、类别等。
- **拆分大的 DELETE 或 INSERT 语句** 
- 选择合适的字段类型，**选择标准是尽可能小、尽可能定长、尽可能使用整数**。
- 字段设计尽可能使用 NOT NULL 
- **进行水平切割或者垂直分割**

### 为什么要使用视图？什么是视图？

**为了提高复杂SQL语句的复用性和表操作的安全性**，MySQL数据库管理系统提供了视图特性。**所谓视图，本质上是一种虚拟表，在物理上是不存在的，其内容与真实的表相似，包含一系列带有名称的列和行数据。但是，视图并不在数据库中以储存的数据值形式存在。行和列数据来自定义视图的查询所引用基本表，并且在具体引用视图时动态生成。**

视图使开发者只关心感兴趣的某些特定数据和所负责的特定任务，只能看到视图中所定义的数据，而不是视图所引用表中的数据，从而提高了数据库中数据的安全性。

### 视图有哪些特点？

视图的特点如下：

- **视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系**。
- 视图是由基本表(实表)产生的表(虚表)。
- 视图的建立和删除不影响基本表。
- 对**视图内容的更新(添加，删除和修改)直接影响基本表**。
- 当视图**来自多个基本表时，不允许添加和删除数据**。

视图的操作包括创建视图，查看视图，删除视图和修改视图。

### 视图的使用场景有哪些？

视图根本用途：**简化sql查询，提高开发效率**。如果说还有另外一个用途那就是兼容老的表结构。

下面是视图的常见使用场景：

- 重用SQL语句；
- 简化复杂的SQL操作。在编写查询后，可以方便的重用它而不必知道它的基本查询细节；
- 使用表的组成部分而不是整个表；
- **保护数据**。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限；
- 更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。

### 视图的优缺点

#### 优点

- 查询简单化。视图能简化用户的操作
- 数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护
- 逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性

#### 视图的缺点

**性能**。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。

**修改限制**。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，**对于比较复杂的视图，可能是不可修改的**

这些视图有如下特征：1.有**UNIQUE等集合操作符的视图**。2.有**GROUP BY子句的视图**。3.有诸如**AVG\SUM\MAX等聚合函数**的视图。 4.使用**DISTINCT关键字的视图**。5.连接表的视图（其中有些例外）

### 什么是游标？

**游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果，每个游标区都有一个名字**。用户可以通过游标逐一获取记录并赋给主变量，交由主语言进一步处理。

### 什么是存储过程？有哪些优缺点？

存储过程是一个预编译的SQL语句，**优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次**。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。

优点

- 存储过程是预编译过的，执行效率高。
- 存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。
- **安全性高**，执行存储过程需要有一定权限的用户。
- 存储过程可以重复使用，减少数据库开发人员的工作量。

缺点

- **调试麻烦**，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。
- **移植问题**，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。
- **重新编译问题**，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。
- 如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候**随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了**，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。

### 什么是触发器？触发器的使用场景有哪些？

触发器是用户定义**在关系表上的一类由事件驱动的特殊的存储过程**。触发器是指一段代码，**当触发某个事件时，自动执行这些代码**。

###### 使用场景

- 可以通过数据库中的相关表**实现级联更改**。
- **实时监控某张表中的某个字段的更改而需要做出相应的处理**。
- 例如可以生成某些业务的编号。

### MySQL中都有哪些触发器？

在MySQL数据库中有如下六种触发器：

- Before Insert
- After Insert
- Before Update
- After Update
- Before Delete
- After Delete



### 超键、候选键、主键、外键分别是什么？

- 超键：在关系中能**唯一标识元组的属性集称为关系模式的超键**。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。
- 候选键：是最小超键，即没有冗余元素的超键。
- 主键：数据库表中对储存数据对象予以**唯一和完整标识的数据列或属性的组合**。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。
- 外键：在一个表中存在的另一个表的主键称此表的外键。

### SQL 约束有哪几种？

- NOT NULL： 用于控制字段的内容一定**不能为空**（NULL）。
- UNIQUE： 控件字段内容**不能重复**，一个表**允许有多个** Unique 约束。
- PRIMARY KEY： 也是用于控件字段内容**不能重复**，但它在一个表只允许出现一个。
- FOREIGN KEY： 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。
- CHECK： 用于控制字段的值范围。

### 六种关联查询

- 交叉连接（CROSS JOIN）

- 内连接（INNER JOIN）

- 外连接（LEFT JOIN/RIGHT JOIN）

- 联合查询（UNION与UNION ALL）

- 全连接（FULL JOIN）

  ![MySqlJoinTypesThumbnail](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MySqlJoinTypesThumbnail-774x1024.png)


**交叉连接（CROSS JOIN）**

```sql
SELECT * FROM A,B(,C)或者SELECT * FROM A CROSS JOIN B (CROSS JOIN C)#没有任何关联条件，结果是笛卡尔积，结果集会很大，没有意义，很少使用内连接（INNER JOIN）SELECT * FROM A,B WHERE A.id=B.id或者SELECT * FROM A INNER JOIN B ON A.id=B.id多表中同时符合某种条件的数据记录的集合，INNER JOIN可以缩写为JOIN
```

[笛卡尔积](https://baike.baidu.com/item/%E7%AC%9B%E5%8D%A1%E5%B0%94%E4%B9%98%E7%A7%AF/6323173)



**内连接**分为三类

- 等值连接：ON A.id=B.id

- 不等值连接：ON A.id > B.id

- 自连接：SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid



**外连接**（LEFT JOIN/RIGHT JOIN）

- 左外连接：LEFT OUTER JOIN, **以左表为主**，先查询出左表，按照ON后的关联条件匹配右表，**没有匹配到的用NULL填充**，可以简写成LEFT JOIN
- 右外连接：RIGHT OUTER JOIN, **以右表为主**，先查询出右表，按照ON后的关联条件匹配左表，没有匹配到的用NULL填充，可以简写成RIGHT JOIN



**联合查询**（UNION与UNION ALL）

```sql
SELECT * FROM A UNION SELECT * FROM B UNION ...
```

- 就是**把多个结果集集中在一起**，**UNION前的结果为基准**，需要注意的是联合查询的列数要相等，**相同的记录行会合并**
- 如果**使用UNION ALL，不会合并重复的记录行**
- 效率 UNION 高于 UNION ALL



**全连接**（FULL JOIN）

- MySQL不支持全连接

- 可以使用LEFT JOIN 和UNION和RIGHT JOIN联合使用

  ```sql
  SELECT * FROM A LEFT JOIN B ON A.id=B.id UNIONSELECT * FROM A RIGHT JOIN B ON A.id=B.id
  ```

  
  
  表连接面试题
  
  有2张表，1张R、1张S，R表有ABC三列，S表有CD两列，表中各有三条记录。

![img](https://segmentfault.com/img/remote/1460000040112792)

```sql
#交叉连接(笛卡尔积):

select r.*,s.* from r,s
```

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112793)

内连接结果：

```sql
select r.*,s.* from r inner join s on r.c=s.c
```

![img](https://segmentfault.com/img/remote/1460000040112794)

左连接结果：

```sql
select r.*,s.* from r left join s on r.c=s.c
```

![img](https://segmentfault.com/img/remote/1460000040112795)

右连接结果：

```sql
select r.*,s.* from r right join s on r.c=s.c
```

![img](https://segmentfault.com/img/remote/1460000040112796)

全表连接的结果（MySql不支持，Oracle支持）：

```sql
select r.*,s.* from r full join s on r.c=s.c
```

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112797)

### 什么叫外链接？ 

外连接分为三种，分别是

- **左外连接(**LEFT OUTER J0IN 或 LEFT JOIN ，以左表为主，先查询出左表，按照ON后的关联条件匹配右表，没有匹配到的用NULL填充
- **右外连接**(RIGHT OUTER JOIN 或 RIGHT JOIN，以右表为主，先查询出右表，按照ON后的关联条件匹配左表，没有匹配到的用NULL填充
- 全外连接(FULL OUTER JOIN  或 FULLJOIN)。 


### 什么叫内链接？

结合两个表中相同的字段，返回关联字段相符的记录就是内链接。

![image-20220421165719026](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/image-20220421165719026.png)

### 使用 union 和 union all 时需要注意些什么？ 

通过 union 连接的 SQL 分别**单独取出的列数必须相同**。 使用 union 时，**多个相等的行将会被合并**，由于合升比较耗时，一般不直接使用 union 进行合并，而是通常采用 union all 进行合并。

### 什么是子查询

条件：一条SQL语句的查询结果做为另一条查询语句的条件或查询结果

嵌套：多条SQL语句嵌套使用，内部的SQL查询语句称为子查询。

### 子查询的三种情况

子查询是单行单列的情况：结果集是一个值，父查询使用：=、 <、 > 等运算符

```sql
-- 查询工资最高的员工是谁？ 
select  * from employee where salary=(select max(salary) from employee);   
```

子查询是多行单列的情况：结果集类似于一个数组，父查询使用：in 运算符

```sql
-- 查询工资最高的员工是谁？ 
select  * from employee where salary=(select max(salary) from employee);    
```

子查询是多行多列的情况：结果集类似于一张虚拟表，不能用于where条件，用于select子句中做为子表

```sql
-- 1) 查询出2011年以后入职的员工信息
-- 2) 查询所有的部门信息，与上面的虚拟表中的信息比对，找出所有部门ID相等的员工。
select * from dept d,  (select * from employee where join_date > '2011-1-1') e where e.dept_id =  d.id;    

-- 使用表连接：
select d.*, e.* from  dept d inner join employee e on d.id = e.dept_id where e.join_date >  '2011-1-1'  
```

### mysql中 in 和 exists 区别

mysql中的**in语句是把外表和内表作hash 连接**，而**exists语句是对外表作loop循环，每次loop循环再对内表进行查询**。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。

- 如果查询的两个表大小相当，那么用in和exists差别不大。
- 如果两个表中一个较小，一个是大表，则**子查询表大的用exists**，**子查询表小的用in**。
- not in 和not exists：如果**查询语句使用了not in，那么内外表都进行全表扫描**，没有用到索引；而**not extsts的子查询依然能用到表上的索引**。所以无论那个表大，用not exists都比not in要快。

[数据库三种基本连接操作(HASH JOIN MERGE JOIN NESTED LOOP) ](https://www.cnblogs.com/onlywujun/articles/5697693.html)

### FLOAT和DOUBLE的区别是什么？

- FLOAT 类型数据可以存储至多8位十进制数，并在内存中占4字节。
- DOUBLE 类型数据可以存储至多18位十进制数，并在内存中占8字节。

### drop、delete与truncate的区别

三者都表示删除，但是三者有一些差别：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112798)

因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。

### SQL的生命周期？

- 应用服务器与数据库服务器建立一个连接
- 数据库进程拿到请求sql
- 解析并生成执行计划，执行
- 读取数据到内存并进行逻辑处理
- 通过步骤一的连接，发送结果到客户端
- 关掉连接，释放资源

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112801)

### 大表数据查询，怎么优化

- **优化shema、sql语句+索引**；
- 第二加缓存，memcached, redis；
- **主从复制，读写分离**；
- **垂直拆分**，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；
- **水平切分**，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；

### 超大分页怎么处理？

超大的分页一般从两个方向上来解决.

- **数据库层面**,这也是我们主要集中关注的(虽然收效没那么大),类似于`select * from table where age > 20 limit 1000000,10`这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为`select * from table where id in (select id from table where age > 20 limit 1000000,10).`这样虽然也load了一百万的数据,但是由于**索引覆盖**, 要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以`select * from table where id > 1000000 limit 10,`效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是**减少load的数据**.
- 从需求的角度减少这种请求…**主要是不做类似的需求(**直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击.

解决超大分页,其实**主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中**,直接返回即可.

在阿里巴巴《Java开发手册》中,对超大分页的解决办法是类似于上面提到的第一种.

```pgsql
【推荐】利用延迟关联或者子查询优化超多分页场景。 

说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 

正例：先快速定位需要获取的id段，然后再关联： 

SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id
```

###### mysql 分页

LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1)

```sql
mysql> SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15 
```

为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1：

```sql
mysql> SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last. 
```

如果只给定一个参数，它表示返回最大的记录行数目：

```sql
mysql> SELECT * FROM table LIMIT 5; //检索前 5 个记录行 
```

换句话说，LIMIT n 等价于 LIMIT 0,n。

### 慢查询日志

**用于记录执行时间超过某个临界值的SQL日志**，用于快速定位慢查询，为我们的优化做参考。

开启慢查询日志

配置项：slow_query_log

可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。

设置临界时间

配置项：long_query_time

查看：show VARIABLES like 'long_query_time'，单位秒

设置：set long_query_time=0.5

实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉

查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中

### 关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？

在业务系统中，除了使用主键进行的查询，其他的我都会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。

慢查询的优化首先要搞明白慢的原因是什么？ 是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？

所以优化也是针对这三个方向来的，

- 首先**分析语句**，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。
- **分析语句的执行计划，然后获得其使用索引的情况**，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。
- 如果对语句的优化已经无法进行，可以**考虑表中的数据量是否太大**，如果是的话可以进行横向或者纵向的分表。

### 为什么要尽量设定一个主键？

**主键是数据库确保数据行在整张表唯一性的保障**，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。

### 主键使用自增ID还是UUID？

推荐使用自增ID，不要使用UUID。

因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。

总之，在数据量大一些的情况下，用自增主键性能会好一些。

关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。

### 字段为什么要求定义为not null？

**null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。**

### 如果要存储用户的密码散列，应该使用什么字段进行存储？

密码散列，盐，用户身份证号等固定长度的字符串**应该使用char**而不是varchar来存储，这样可以节省空间且提高检索效率。

### 优化查询过程中的数据访问

- 访问数据太多导致查询性能下降
- 确定应用程序是否在检索大量超过需要的数据，可能是太多行或列
- 确认MySQL服务器是否在分析大量不必要的数据行
- 避免犯如下SQL语句错误
- 查询不需要的数据。解决办法：使用limit解决
- 多表关联返回全部列。解决办法：指定列名
- 总是返回全部列。解决办法：避免使用SELECT *
- 重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存
- 是否在扫描额外的记录。解决办法：
- 使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化：
- 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。
- 改变数据库和表的结构，修改数据表范式
- 重写SQL语句，让优化器可以以更优的方式执行查询。

#### 优化长难的查询语句

- 一个复杂查询还是多个简单查询
- MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多
- 使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。
- 切分查询
- 将一个大的查询分为多个小的相同的查询
- 一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。
- 分解关联查询，让缓存的效率更高。
- 执行单个查询可以减少锁的竞争。
- 在应用层做关联更容易对数据库进行拆分。
- 查询效率会有大幅提升。
- 较少冗余记录的查询。

#### 优化特定类型的查询语句

- count(*)会忽略所有的列，直接统计所有列数，不要使用count(列名)
- MyISAM中，没有任何where条件的count(*)非常快。
- 当有where条件时，MyISAM的count统计不一定比其它引擎快。
- 可以使用explain查询近似值，用近似值替代count(*)
- 增加汇总表
- 使用缓存

#### 优化关联查询

- 确定ON或者USING子句中是否有索引。
- 确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。

#### 优化子查询

- 用关联查询替代
- 优化GROUP BY和DISTINCT
- 这两种查询据可以使用索引来优化，是最有效的优化方法
- 关联查询中，使用标识列分组的效率更高
- 如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。
- WITH ROLLUP超级聚合，可以挪到应用程序处理

#### 优化LIMIT分页

- LIMIT偏移量大的时候，查询效率较低
- 可以记录上次查询的最大ID，下次查询时直接根据该ID来查询

#### 优化UNION查询

- UNION ALL的效率高于UNION

#### 优化WHERE子句

解题方法

对于此类考题，先说明如何定位低效SQL语句，然后根据SQL语句可能低效的原因做排查，先从索引着手，如果索引没有问题，考虑以上几个方面，数据访问的问题，长难查询句的问题还是一些特定类型优化的问题，逐一回答。

### SQL语句优化的一些方法？

1. **对查询进行优化，应尽量避免全表扫描**，首先应考虑在 where 及 order by 涉及的列上建立索引。

2. 应**尽量避免在 where 子句中对字段进行 null 值判断**，否则将导致引擎放弃使用索引而**进行全表扫描**，如：

```sql
select id from t where num is null
-- 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：
select id from t where num=
```

3. 应**尽量避免在 where 子句中使用!=或<>操作符**，否则引擎将放弃使用索引而进行**全表扫描**。

4. 应**尽量避免在 where 子句中使用or 来连接条件**，否则将导致引擎放弃使用索引而进行**全表扫描**，如：

```sql
select id from t where num=10 or num=20
-- 可以这样查询：
select id from t where num=10 union all select id from t where num=2
```

5. **in 和 not in 也要慎用，否则会导致全表扫描**，如：

```sql
select id from t where num in(1,2,3) 
-- 对于连续的数值，能用 between 就不要用 in 了：
select id from t where num between 1 and 3
```

6. 下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索。

7. 如果**在 where 子句中使用参数，也会导致全表扫描**。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：

```sql
select id from t where num=@num
-- 可以改为强制查询使用索引：
select id from t with(index(索引名)) where num=@num
```

8. 应**尽量避免在 where 子句中对字段进行表达式操作**，这将导致引擎放弃使用索引而进行全表扫描。如：

```sql
select id from t where num/2=100
-- 应改为:
select id from t where num=100*2
```

9. 应**尽量避免在where子句中对字段进行函数操作**，这将导致引擎放弃使用索引而进行全表扫描。如：

```pgsql
select id from t where substring(name,1,3)=’abc’
-- name以abc开头的id应改为:
select id from t where name like ‘abc%’
```

10. **不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算**，否则系统将可能无法正确使用索引。

### 为什么要进行数据库优化

- 系统的吞吐量瓶颈往往出现在数据库的访问速度上
- 随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢
- 数据是存放在磁盘上的，读写速度无法和内存相比
- 优化原则：减少系统瓶颈，减少资源占用，增加系统的反应速度。

### 数据库结构优化

一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。

需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。

**将字段很多的表分解成多个表**

对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。

因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。

**增加中间表**

对于需要经常联合查询的表，可以建立中间表以提高查询效率。

通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。

**增加冗余字段**

设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。

表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。

**注意**：冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。

### MySQL数据库cpu飙升到500%的话他怎么处理？

当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。

如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。

一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。

也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等

### 大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？

当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：

- **限定数据的范围**： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。；
- **读/写分离**： 经典的数据库拆分方案，主库负责写，从库负责读；
- **缓存**： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存；
  还有就是通过分库分表的方式进行优化，主要有垂直分表和水平分表

**垂直分区**

根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。

简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。

![img](https://segmentfault.com/img/remote/1460000040112802)

垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。

垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；

**垂直分表**

把主键和一些列放在一个表，然后把主键和另外的列放在另一个表中

![img](https://segmentfault.com/img/remote/1460000040112803)

**适用场景**

- 1、如果一个表中某些列常用，另外一些列不常用
- 2、可以使数据行变小，一个数据页能存储更多数据，查询时减少I/O次数
- 缺点
  - 有些分表的策略基于应用层的逻辑算法，一旦逻辑算法改变，整个分表逻辑都会改变，扩展性较差
  - 对于应用层来说，逻辑算法增加开发成本
  - 管理冗余列，查询所有数据需要join操作

**水平分区**

保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。

水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112804)

水品拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。

水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。

《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。

**水平分表**

表很大，分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询次数

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000040112805)

- 适用场景
  - 1、表中的数据本身就有独立性，例如表中分表记录各个地区的数据或者不同时期的数据，特别是有些数据常用，有些不常用。
  - 2、需要把数据存放在多个介质上。
- 水平切分的缺点
  - 1、给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作
  - 2、在许多数据库应用中，这种复杂度会超过它带来的优点，查询时会增加读一个索引层的磁盘次数

下面补充一下数据库分片的两种常见方案：

**客户端代理**： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。

**中间件代理**： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。

### 分库分表后面临的问题

- **事务支持** 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。
- **跨库join**
  只要是进行切分，**跨节点Join的问题是不可避免**的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品
- **跨节点的count,order by,group by以及聚合函数问题** 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。
- **数据迁移，容量规划，扩容等问题** 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。
- **ID问题**
- **一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制**。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略

UUID 使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。 Twitter的分布式自增ID算法Snowflake 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。

跨分片的排序分页

般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。如下图所示：

![img](https://segmentfault.com/img/remote/1460000040112806)

### [MySQL存储引擎MyISAM与InnoDB区别](https://segmentfault.com/a/1190000008227211)

- 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。
- 并发：**MyISAM 只支持表级锁，而 InnoDB 还支持行级锁**。
- 外键：InnoDB 支持外键。
- 备份：InnoDB 支持在线热备份。
- 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。
- 其它特性：MyISAM 支持压缩表和空间数据索引。

### MyISAM索引与InnoDB索引的区别？

[参考](https://segmentfault.com/a/1190000022206424)

- InnoDB 索引是**聚簇索引**，MyISAM 索引是**非聚簇索引**
- InnoDB 的主键索引的叶子节点**存储着行数据**，主键索引非常高效
- MyISAM 索引的叶子节点存储的是**行数据地址**，需要再寻址一次才能得到数据
- InnoDB 非主键索引的叶子节点存储的是**主键和其他带索引的列数据**，因此查询时做到覆盖索引会非常高效

### InnoDB引擎的4大特性

[参考](https://zhuanlan.zhihu.com/p/109528131)

#### 一、插入缓冲insert buffer

1. 目的：**提升插入性能**；

2. 使用插入缓冲的条件：

3. 1. 非聚集索引；
   2. 非唯一索引；
   3. 就是说**必须是辅助索引且非唯一索引**；
   4. 为何？**insert buffer的设计是为了避免读取索引页**，如果是唯一索引，在插入时需要判断插入的记录是否唯一，这需要读取辅助索引页，导致失去insert buffer的设计意义；

4. **原理**：Insert Buffer是怎么做的呢？mysql**对于非聚集索引的插入，先去判断要插入的索引页是否已经在内存中**了，如果不在，暂时不着急先**把索引页**加载到内存中，而是把它**放到了一个Insert Buffer对象**中，临时先放在这，然后等待情况，等待很多和现在情况一样的非聚集索引，再和要插入的非聚集索引页合并，比如说现在Insert Buffer中有1，99，2，100，合并之前可能要4次插入，合并之后1，2可能是一个页的，99，100可能是一个页的，这样就减少到了2次插入。这样就提升了效率和插入性能，减少了随机IO带来性能损耗。

5. ![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20210330151746375.png)

6. 插入缓冲：change buffer是对insert buffer的加强，insert buffer只对insert有效，change buffer对insert、delete、update（delete+insert）、purge都有效；

7. change buffer作为buffer pool的一部分，innodb_change_buffering参数的值有：

8. 1. all：默认值，缓存insert、delete、purge操作；
   2. inserts：缓存insert操作；
   3. deletes：缓存delete操作；
   4. changes：缓存insert、delete操作；
   5. purges：缓存后台执行的物理删除操作；
   6. none：不缓存；

9. innodb_change_buffer_max_size参数：控制使用的大小，默认25%，最大可设置50%，如果mysql实例中有大量的修改操作，可考虑增大该参数；

10. insert buffer的数据结构是一颗B+树；

11. 1. 全局只有一颗insert buffer B+树，负责对所有表的辅助索引进行insert buffer；
    2. 这颗B+树放在共享表空间中，试图通过独立表空间ibd文件恢复表中数据时，往往会导致check table失败，因为表中的辅助索引中的数据可能还在insert buffer中，也就是共享表空间中，所以ibd文件恢复后，还需要repair table操作来重建表上所有的辅助索引；

12. 对满足插入缓存条件的插入，每一次的插入不是写到索引页中；

13. 1. 会先判断插入的非聚集索引页是否在缓冲池中，如果在直接插入；
    2. 如果不在，则先放到insert buffer中，再按照一定的频率进行合并操作，再写会磁盘；
    3. 通常可以将多个插入合并到一个操作中，目的是为了减少随机IO带来的性能损耗；

14. 一定频率进行合并操作，这个频率有什么条件？

15. 1. 辅助索引页被读取到缓冲池中，正常的select先检查insert buffer是否有该非聚集索引页存在，有则合并插入；
    2. 辅助索引页没有可用空间。空间小于1/32页大小，会强制合并操作；
    3. master thread每秒和每10秒的合并操作；

#### 二、二次写double write

**原理**：

在InnoDB将BP中的Dirty Page刷（flush）到磁盘上时，首先会将（memcpy函数）Page刷到InnoDB tablespace的一个区域中，我们称该区域为Double write Buffer（大小为2MB，每次写入1MB，128个页，每个页16k,其中120个页为后台线程的批量刷Dirty Page，还有8个也是为了前台起的sigle Page Flash线程，用户可以主动请求，并且能迅速的提供空余的空间）。在向Double write Buffer写入成功后，第二步、再将数据分别刷到一个共享空间和真正应该存在的位置。

MySQL可以根据redolog进行恢复,而mysq在恢复的过程中是检查page"的checksum, checksum就是pgae的最后事务号,发生partial page write问题时. DageR经损坏,找不到该page中的事务号就无法恢复。

具体的流程如下图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210330151850245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMyMDY2MA==,size_16,color_FFFFFF,t_70)

在不同的写入阶段，操作系统crash后，double write带来的保护机制：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210330151901793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMyMDY2MA==,size_16,color_FFFFFF,t_70)

阶段一：copy过程中，操作系统crash，重启之后，脏页未刷到磁盘，但更早的数据并没有发生损坏，重新写入即可

阶段二：write到共享表空间过程中，操作系统crash，重启之后，脏页未刷到磁盘，但更早的数据并没有发生损坏，重新写入即可

阶段三：write到独立表空间过程中，操作系统crash，重启之后，发现：

1. 数据文件内的页损坏：头尾checksum值不匹配（即出现了partial page write的问题）。从共享表空间中的doublewrite segment内恢复该页的一个副本到数据文件，再应用redo log；
2. 若页自身的checksum匹配，但与doublewrite segment中对应页的checksum不匹配，则统一可以通过apply redo log来恢复。

阶段X：recover过程中，操作系统crash，重启之后，innodb面对的情况同阶段三一样（数据文件损坏，但共享表空间内有副本），再次应用redo log即可。

1. doublewrite缓存位于系统表空间的存储区域，用来缓存innodb的数据页从innodb buffer pool中flush之后并写入到数据文件之前；

2. 当操作系统或数据库进程在数据页写入磁盘的过程中崩溃，可以在doublewrite缓存中找到数据页的备份，用来执行crash恢复；

3. 数据页写入到doublewrite缓存的动作所需要的io消耗要小于写入到数据文件的消耗，因为此写入操作会以一次大的连续块的方式写入；

   

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/v2-8dc0d0503cc0075578d896ae71ed1609_r.jpg)

根据上图知道：

1. 1. 内存中doublewrite buffer大小2M；物理磁盘上共享表空间中连续的128个页，也就是2个区（extent）大小同样为2M；

   2. 对缓冲池脏页进行刷新时，不是直接写磁盘。

   3. 1. 第一步：通过memcpy()函数将脏页先复制到内存中的doublewrite buffer；
      2. 第二步：通过doublewrite分两次，每次1M顺序的写入共享表空间的物理磁盘上。这个过程中，doublewrite页是连续的，因此这个过程是顺序的，所以开销并不大；
      3. 第三步：完成doublewrite页的写入后，再将doublewrite buffer中的页写入各个表空间文件中，此时写入是离散的，可能会较慢；
      4. 如果操作系统在第三步的过程中发生了崩溃，在恢复过程中，可以从共享表空间中的doublewrite中找到该页的一个副本，将其复制到表空间文件中，再应用重做日志；

#### 三、自适应hash索引ahi

1. innodb存储引擎会监控对表上二级索引的查找，如果发现某二级索引被频繁访问，此索引成为热数据，建立hash索引以提升查询速度，此建立是自动建立哈希索引，故称为自适应哈希索引（adaptive hash index）；
2. 自适应哈希索引会占用innodb buffer pool；
3. 只适合搜索等值（=）的查询，对于范围查找等操作，是不能使用的；

#### 四、预读read ahead

预读（read-ahead)操作是一种IO操作，用于异步将磁盘的页读取到buffer pool中，预料这些页会马上被读取到。预读请求的所有页集中在一个范围内。

1. 两种预读算法来提高性能：

2. 1. 线性预读：以extent为单位，将下一个extent提前读取到buffer pool中；
   2. 随机预读：以extent中的page为单位，将当前extent中的剩余的page提前读取到buffer pool中；

3. 线性预读一个重要参数：innodb_read_ahead_threshold，控制什么时间（访问extent中多少页的阈值）触发预读；

4. 1. 默认：56，范围：0～64，值越高，访问模式检查越严格；
   2. 没有该变量之前，当访问到extent最后一个page时，innodb会决定是否将下一个extent放入到buffer pool中；

5. 随机预读说明：

6. 1. 当同一个extent的一些page在buffer pool中发现时，innodb会将extent中剩余page一并读取到buffer pool中；
   2. 随机预读给innodb code带来一些不必要的复杂性，性能上也不稳定，在5.5版本已经废弃，如果启用，需要修改变量：innodb_random_read_ahead为ON；

### 存储引擎选择

[参考](https://segmentfault.com/a/1190000038935511)

InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。**如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择**。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。

MyISAM ： **如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的**。

MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供几块的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。**MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果**。

MERGE：用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用他们。MERGE表的优点在于可以突破对单个MyISAM表的大小限制，并且通过将不同的表分布在多个磁盘上，可以有效的改善MERGE表的访问效率。**这对于存储诸如数据仓储等VLDB环境十分合适**。

### 数据表损坏的修复方式有哪些？

[参考](https://blog.csdn.net/qq_35440678/article/details/60321689)

#### 数据损坏原因

MySQL表损坏一般是数据损坏，引起损坏的原因可能是由于磁盘损坏、系统崩溃或者MySQL服务器被崩溃等外部原因。例如有人使用`kill -9`终止进程，导致MySQL进程未能正常关闭，那么就很有可能导致数据损坏。
对于不同的引擎，数据损坏修复的方式不一样，作为一般情况可以尝试使用`CHECK TABLE`和`REPAIR TABLE`命令修复。

#### MyISAM损坏的修复方案

MyISAM损坏有两种修复方式：

##### 1.通过SQL修复MyISAM表:

查看表是否损坏：

```
mysql> CHECK TABLE t1;
+---------------+-------+----------+----------+
| Table         | Op    | Msg_type | Msg_text |
+---------------+-------+----------+----------+
| db1.t1 | check | status   | OK       |
+---------------+-------+----------+----------+
1 row in set (0.00 sec)1234567
```

修复表：

```
mysql> repair table t1;
+---------------+--------+----------+---------------------------------------------------------+
| Table         | Op     | Msg_type | Msg_text                                                |
+---------------+--------+----------+---------------------------------------------------------+
| db1.t1 | repair | note     | The storage engine for the table doesn't support repair |
+---------------+--------+----------+---------------------------------------------------------+
1 row in set (0.00 sec)1234567
```

如果单纯执行`REPAIR TABLE`没有起到什么效果，那么可以选择另外两个选项：
\- `REPAIR TABLE EXTENDED`,速度比`REPAIR TABLE`慢得多，但是可以修复99%的错误；
\- `REPAIR TABLE USE_FRM`,它会删除索引并利用table_name.frm文件中的描述重建索引，并通过table_name.MYD文件填充健对应的值。

##### 2. 使用myisamchk修复MyISAM

myisamchk可以直接访问表文件，而无须启动MySQL服务器。
进入datadir文件目录，执行基本命令：

```
myisamchk --backup --recover t11
```

其中，`--backup`选项是在尝试修复表之前先进行数据文件备份，还有其他使用选项就不一一介绍了。

#### InnoDB数据损坏修复

InnoDB是带有事务的存储引擎，并且其内部机制会自动修复大部分数据损坏错误，它会在服务器启动时进行修复。
不过，有时候数据损坏得很严重并且InnoDB无法在没有用户交互的情况下完成修复，在这种情况下，有`--innodb_force_recovery`启动选项。

该选项可以设置0~6(0 不强制修复 1是最低级别 6最高级别)。

如果发生损坏，可以从1开始尝试修复，直到可以启动服务器并且可以访问有问题的表为止.

启动后使用`select into outfile`将表转储到文件中，然后使用`drop`和`create`命令重新创建表，最后用`--innodb_force_recovery=0`重新启动服务器，然后加载文件数据。

当需要在`--innodb_force_recovery`选项是正数的情况下修复数据库时，错误日志通常会有明确的提示信息。

### MySQL问题排查都有哪些手段

#### MySQL排查问题常用思路 

- 使用 show processlist 命令**查看当前所有连接信息**。
- 使用 explain 命令查询 SQL 语句执行计划。
- 开启慢查询日志，查看慢查询的 SQL。

#### 定制化 MySQL Show Processlist 输出结果 

在 MySQL 中使用 Show Processlist 等指令时常常会出现一些无用信息，比如：Sleep 状态的。此时，如果我们想要过滤掉不需要的内容，应该怎么做呢？

**方法一**

在 MySQL 的命令提示符下，可以使用 \P 的命令设定要过滤的内容，步骤如下：

```
$ mysql -u root -p
$ mysql> \P grep -v Sleep
# 此时输出结果就没有 Sleep 相关的信息了
$ mysql> Show Full Processlist;
```

**方法二**

在使用 SQL 命令时想要过滤指定的内容，可以使用下面的命令。

```
SELECT * FROM information_schema.processlist WHERE STATE != '';
SELECT * FROM information_schema.processlist WHERE COMMAND != 'Sleep';
SELECT * FROM information_schema.processlist WHERE db = 'dbname' AND COMMAND != 'Sleep';
```

#### MySQL EXPLAIN 详解

MySQL EXPLAIN命令是查询性能优化不可缺少的一部分，本文主要讲解explain命令的使用及相关参数说明。

EXPLAIN Output Columns

![mysql.jpg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/291609249864148090076.jpg)

**id**

id是用来顺序标识整个查询中SELELCT 语句的，在嵌套查询中id越大的语句越先执行。该值可能为NULL，如果这一行用来说明的是其他行的联合结果。

**select_type**

表示查询的类型

![mysql.jpg](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/291609249922493057539.jpg)

**table**

对应行正在访问哪一个表，表名或者别名

关联优化器会为查询选择关联顺序，左侧深度优先

当from中有子查询的时候，表名是derivedN的形式，N指向子查询，也就是explain结果中的下一列

当有union result的时候，表名是union 1,2等的形式，1,2表示参与union的query id

注意：MySQL对待这些表和普通表一样，但是这些“临时表”是没有任何索引的。

**type**

type显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是：

system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL ，一般来说，得保证查询至少达到range级别，最好能达到ref。

**possible_keys**

显示查询使用了哪些索引，表示该索引可以进行高效地查找，但是列出来的索引对于后续优化过程可能是没有用的

**key**

key列显示MySQL实际决定使用的键（索引）。如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。

**key_len**

key_len列显示MySQL决定使用的键长度。如果键是NULL，则长度为NULL。使用的索引的长度。在不损失精确性的情况下，长度越短越好 。

**ref**

ref列显示使用哪个列或常数与key一起从表中选择行。

**rows**

rows列显示MySQL认为它执行查询时必须检查的行数。注意这是一个预估值。

**Extra**

Extra是EXPLAIN输出中另外一个很重要的列，该列显示MySQL在查询过程中的一些详细信息，MySQL查询优化器执行查询的过程中对查询计划的重要补充信息。

#### 如何开启MySQL慢查询日志

##### 参数说明：

slow_query_log 慢查询开启状态

slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）

long_query_time 查询超过多少秒才记录

log_output #输出方式，可以是file和table，当为file时会输出到slow_query_log_file文件，当为table时则会输出到mysql数据库中的slow_log表

##### 操作步骤 

正常情况下，只需要在配置文件中增加slow_query_log = 1配置，即打开慢查询日志，未指定slow_query_log_file的情况下，会自动生成一个以主机名+‘slow'.log 的文件。　

在 MySQL中，提供了慢查询查询日志，基于性能方面的考虑，该配置默认为OFF(关闭) 状态。那么如何开启慢日志查询呢？其步骤如下：

在 MySQL 中，慢查询日志默认为OFF状态，通过如下命令进行查看：

```
mysql> show variables like "slow_query_log";
+---------------------+----------------------------------------------+
| Variable_name       | Value                                        |
+---------------------+----------------------------------------------+
| slow_query_log      | OFF                                          |
+---------------------+----------------------------------------------+
2 rows in set (0.01 sec)
```

通过如下命令进行设置为 ON 状态：

```
set global slow_query_log = "ON";
```

其中slow_query_log_file属性，表示慢查询日志存储位置，其日志默认名称为 host 名称。

如下所示：

```
mysql> show variables like "slow_query_log_file";
+---------------------+----------------------------------------------+
| Variable_name       | Value                                        |
+---------------------+----------------------------------------------+                                     |
| slow_query_log_file | /usr/local/mysql/data/hostname.log |
+---------------------+----------------------------------------------+
2 rows in set (0.01 sec)
```

也可使用 以下命令进行修改：

```
set global slow_query_log_file = ${path}/${filename}.log;
```

其中: path 表示路径， filename 表示文件名，如果不指定，其默认filename 为hostname。

慢查询 查询时间，当SQL执行时间超过该值时，则会记录在slow_query_log_file 文件中，其默认为 10 ，最小值为 0，(单位：秒)。

```
mysql> show variables like "long_query_time";
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| long_query_time | 10.000000 |
+-----------------+-----------+
1 row in set (0.00 sec)
```

可通过以下命令进行修改：

```
mysql> set global long_query_time = 5;
```

当设置值小于0时，默认为 0。

通过上述设置后，退出当前会话或者开启一个新的会话，执行如下命令：

```
select sleep(11);
```

备注: 这里的 11 并不是固定值，仅仅为了展示，其值只需要符合以下条件即可：

该值大于等于long_query_time 值即可。

该 SQL 则会进入慢查询日志中。通过cat 命令查看后如下所示：

```
# Time: 200310 13:30:57
# User@Host: root[root] @ localhost []  Id: 21528
# Query_time: 6.000164  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0
SET timestamp=1583818257;
select sleep(6);
```
