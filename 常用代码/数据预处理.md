# 数据预处理常用代码

1. 首先下载第三方库

   ```
   !pip install tweet-preprocessor # tweet的处理库
   !pip install transformers
   !pip install tensorboardX
   !pip install tensorboard
   !pip install tensorflow
   ```

2. 下载停用词

   ```
   import nltk
   nltk.download('punkt')
   nltk.download('wordnet')
   nltk.download('stopwords')
   ```

3. 引入包

   ```
   import pandas as pd
   import io
   import numpy as np
   import matplotlib.pyplot as plt
   % matplotlib inline
   
   import torch
   from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
   import torch.nn as nn
   import torch.nn.functional as F
   from transformers import AutoTokenizer, AutoModelForPreTraining
   from transformers import AdamW
   
   # 可视化
   from datetime import datetime
   from time import time
   from tensorboardX import SummaryWriter
   from tensorflow import summary
   
   from sklearn.model_selection import train_test_split
   from sklearn import metrics
   import preprocessor as p
   
   
   import nltk
   from nltk.stem import WordNetLemmatizer
   from nltk.stem.porter import PorterStemmer
   from nltk.corpus import stopwords
   stop_words = set(stopwords.words('english'))
   
   import re
   from tqdm import tqdm, trange
   ```

4. 读取数据

   ```
   dir_path = '/content/drive/MyDrive/kaggle/RIVF2021_fakenews-main/'
   df = pd.read_csv(dir_path+'train.csv')
   df.head(5)
   ```

5. 提取停用词处理

   ```
   # 词性还原：分词、词性标注，再词性还原。基于字典的映射
   wordnet_lemmatizer = WordNetLemmatizer()
   # 词干提取。基于语言的规则。
   porter_stemmer  = PorterStemmer()
   ```

6. tweet内容处理

   ```
   # p为预处理，提取URL和表情
   p.set_options(p.OPT.URL, p.OPT.EMOJI)
   
   def preprocess(row, lemmatizer, stemmer):
       text = row['text']
       text = p.clean(text)
       tokenization = nltk.word_tokenize(text)
       tokenization = [w for w in tokenization if not w in stop_words]
       text = ' '.join([stemmer.stem(w) for w in tokenization])
       text = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])
       #删除数字
       text = re.sub(r'\([0-9]+\)', '', text).strip()
       return text
   
   df['text'] = df.apply(lambda x:preprocess(x, wordnet_lemmatizer, porter_stemmer), axis=1)
   df.head()
   ```

7. 获取内容长度

   ```
   df['text_len'] = df['text'].str.len()
   print(df.text_len.quantile([0.25,0.5,0.75]))
   plt.hist(x=df['text_len'],label=df['target'])
   ```

8. 构建dataloader

   ```
   def build_tensor(dataset, Sampler, batch_size):
       sampler = Sampler(dataset)
       dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)
       return dataloader
       
   def list2tensor(data):
     
     return torch.from_numpy(data)
     
   train_datalodar = build_tensor(TensorDataset(list2tensor(X_train), list2tensor(Y_train)), RandomSampler, config.batch_size)
   ```

   