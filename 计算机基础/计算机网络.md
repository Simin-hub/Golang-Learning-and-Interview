# 计算机网络

[参考](https://segmentfault.com/a/1190000038526729)

[参考2](https://javaguide.cn/)

## 概述

### ISP

**互联网服务提供商 （Internal Service Provider，ISP）** 可以从互联网管理机构获得许多 IP 地址，同时拥有通信线路以及路由器等联网设备，个人或机构向 ISP 缴纳一定的费用就可以接入互联网。

<img src="https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f37326265303163642d343161652d343566372d393962392d6138643238346534346464342e706e67" alt="img" style="zoom: 50%;" />

目前的互联网是一种多层次 ISP 结构，ISP 根据覆盖面积的大小分为第一层 ISP、区域 ISP 和接入 ISP。互联网交换点 IXP 允许两个 ISP 直接相连而不用经过第三个 ISP。

<img src="https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f33626534323630312d396433332d346432392d383335382d6139643136343533616639332e706e67" alt="img" style="zoom:50%;" />

### 主机之间的通信方式

- 客户-服务器（C/S）：客户是服务的请求方，服务器是服务的提供方。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f39313438393463322d306263342d343662352d626566392d3033313661363965663532312e6a7067)

- 对等（P2P）：不区分客户和服务器。

![img](https://camo.githubusercontent.com/1ce82a88a4c4c6b6125484c6ab4f5587e2e390c49392aa9309ba7e0bc1514466/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f34323433306539342d333133372d343863302d626462362d3363656261663931303265332e6a7067)



端系统、分组交换机和其他因特网部件都要运行一系列协议（protocol）,这些协议控 制因特网中信息的接收和发送。

**协议（protocol）**定义了在两个或多个通信实体之间交换的报文的格式和顺序，以及报文发送和/或接收一条报文或其他事件所采取的动作。 

### 电路交换与分组交换

#### 1. 电路交换

**电路交换用于电话通信系统，两个用户要通信之前需要建立一条专用的物理链路，并且在整个通信过程中始终占用该链路**。由于通信的过程中不可能一直在使用传输线路，因此电路交换对线路的利用率很低，往往不到 10%。

#### 2. 分组交换

**每个分组都有首部和尾部，包含了源地址和目的地址等控制信息，在同一个传输线路上同时传输多个分组互相不会影响，因此在同一条传输线路上允许同时传输多个分组，也就是说分组交换不需要占用传输线路**。

在一个邮局通信系统中，邮局收到一份邮件之后，先存储下来，然后把相同目的地的邮件一起转发到下一个目的地，这个过程就是存储转发过程，分组交换也使用了存储转发过程。

### 时延

总时延 = 排队时延 + 处理时延 + 传输时延 + 传播时延

![img](https://camo.githubusercontent.com/0a962523171ffa6b3b517823ab985c378b1acf07cd439aec5e9b535b6351efce/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f34623261653738632d653235342d343464662d396533372d3537386532663262656635322e6a7067)

#### 1. 排队时延

**分组在路由器的输入队列和输出队列中排队等待的时间，取决于网络当前的通信量。**

#### 2. 处理时延

**主机或路由器收到分组时进行处理所需要的时间**，例如分析首部、从分组中提取数据、进行差错检验或查找适当的路由等。

#### 3. 传输时延

**主机或路由器传输数据帧所需要的时间。**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f64636462623936632d393037372d343132312d616562382d3734336535346163303261342e706e67)



其中 l 表示数据帧的长度，v 表示传输速率。

#### 4. 传播时延

电磁波在**信道中传播所需要花费的时间**，电磁波传播的速度接近光速。

![img](https://camo.githubusercontent.com/c9de58438de83509f26f14b6dbab912917d677287c10ec56e26572bbd5527de9/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f61313631366461632d306531322d343062322d383237642d3965336637663062393430642e706e67)

其中 l 表示信道长度，v 表示电磁波在信道上的传播速度。

## 网络协议

### 什么是网络协议，为什么要对网络协议分层	*

网络协议是计算机在通信过程中要遵循的一些约定好的规则。

网络分层的原因：

- 易于实现和维护，因为各层之间是独立的，层与层之间不会收到影响。
- 有利于标准化的制定

### 计算机网络的各层协议及作用	***

> 计算机网络体系可以大致分为一下三种，**OSI七层模型**、**五层模型和TCP/IP四层模型**，一般面试能流畅回答出五层模型就可以了，表示层和会话层被问到的不多。

<img src="https://raw.githubusercontent.com/Simin-hub/Picture/master/img/%E4%B8%8B%E8%BD%BD.jpg" style="zoom:200%;" />

- 应用层

  应用层的任务是**通过应用进程之间的交互来完成特定的网络作用**，常见的应用层协议有**域名系统DNS，HTTP协议**等。应用层的信息分组称为**报文（message）**。

- 表示层

  表示层的**主要作用是数据的表示、安全、压缩**。可确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。

- 会话层

  会话层的**主要作用是建立通信链接，保持会话过程通信链接的畅通，同步两个节点之间的对话**，决定通信是否被中断以及通信中断时决定从何处重新发送。

- 传输层

  传输层的**主要作用是负责向两台主机进程之间的通信提供数据传输服务。**传输层的协议主要有**传输控制协议TCP和用户数据协议UDP**。运输层的分组称为**报文段 (segment)**。

- 网络层

  网络层的**主要作用是选择合适的网间路由和交换结点，确保数据及时送达**。常见的协议有**IP协议**。网络层的分组称为**数据报（datagram）**

- 数据链路层

  数据链路层的作用是**在物理层提供比特流服务的基础上，建立相邻结点之间的数据链路，通过差错控制提供数据帧（Frame）**在信道上无差错的传输，并进行各电路上的动作系列。 **常见的协议有SDLC、HDLC、PPP**等。把链路层分组称为**帧（frmie）。**

- 物理层

  物理层的主要作用是实现相邻计算机结点之间比特流的透明传输，并尽量屏蔽掉具体传输介质和物理设备的差异。
  
  ![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/5293318_1593560537637_F02EC5233B79B410F00EF03032424943)

![preview](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/v2-2af488004591cbc12cd82c44518523de_r.jpg)

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-ba1e2d8a01c7492d49d39cb93378550f.png)

## 应用层

### HTTP:超文本传输协议

**超文本传输协议（HTTP，HyperText Transfer Protocol)** 主要是为 Web 浏览器与 Web 服务器之间的通信而设计的。当我们使用浏览器浏览网页的时候，我们网页就是通过 HTTP 请求进行加载的，整个过程如下图所示。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/450px-HTTP-Header.png)

**HTTP 是基于 TCP协议**，发送 HTTP 请求之前首先要建立 TCP 连接也就是要经历 3 次握手。目前使用的 HTTP 协议大部分都是 1.1。在 1.1 的协议里面，默认是开启了 Keep-Alive 的，这样的话建立的连接就可以在多次请求中被复用了。

另外， **HTTP 协议是”无状态”的协议**，它无法记录客户端用户的状态，一般我们都是通过 Session 来记录客户端用户的状态。

### SMTP:简单邮件传输(发送)协议

**简单邮件传输(发送)协议（SMTP，Simple Mail Transfer Protocol）** **基于 TCP 协议**，用来发送电子邮件。

注意⚠️：**接受邮件的协议不是 SMTP 而是 POP3 协议。**

SMTP 协议这块涉及的内容比较多，下面这两个问题比较重要：

1. 电子邮件的发送过程
2. 如何判断邮箱是真正存在的？

**电子邮件的发送过程？**

比如我的邮箱是“dabai@cszhinan.com”，我要向“xiaoma@qq.com”发送邮件，整个过程可以简单分为下面几步：

1. 通过 **SMTP** 协议，我将我写好的邮件交给163邮箱服务器（邮局）。
2. 163邮箱服务器发现我发送的邮箱是qq邮箱，然后它**使用 SMTP协议将我的邮件转发到 qq邮箱服务器**。
3. qq邮箱服务器接收邮件之后就**通知邮箱为“xiaoma@qq.com”的用户来收邮件**，然后用户就通过 **POP3/IMAP** 协议将邮件取出。

**如何判断邮箱是真正存在的？**

很多场景(比如邮件营销)下面我们需要判断我们要发送的邮箱地址是否真的存在，这个时候我们可以利用 SMTP 协议来检测：

1. 查找邮箱域名对应的 SMTP 服务器地址
2. 尝试与服务器建立连接
3. 连接成功后尝试向需要验证的邮箱发送邮件
4. 根据返回结果判定邮箱地址的真实性

推荐几个在线邮箱是否有效检测工具：

1. https://verify-email.org/
2. http://tool.chacuo.net/mailverify
3. https://www.emailcamel.com/

### POP3/IMAP:邮件接收的协议

这两个协议没必要多做阐述，只需要了解 **POP3 和 IMAP 两者都是负责邮件接收的协议**即可。另外，需要注意不要将这两者和 SMTP 协议搞混淆了。**SMTP 协议只负责邮件的发送，真正负责接收的协议是POP3/IMAP。**

IMAP 协议相比于POP3更新一点，为用户提供的可选功能也更多一点,几乎所有现代电子邮件客户端和服务器都支持IMAP。大部分网络邮件服务提供商都支持POP3和IMAP。

### FTP:文件传输协议

**FTP 协议** 主要提供文件传输服务，**基于 TCP 实现可靠**的传输。使用 FTP 传输文件的好处是可以屏蔽操作系统和文件存储方式。

FTP 是基于客户—服务器（C/S）模型而设计的，在客户端与 FTP 服务器之间建立两个连接。如果我们要基于 FTP 协议开发一个文件传输的软件的话，首先需要搞清楚 FTP 的原理。关于 FTP 的原理，很多书籍上已经描述的非常详细了：

> FTP 的独特的优势同时也是与其它客户服务器程序最大的不同点就在于它在两台通信的主机之间**使用了两条 TCP 连接**（其它客户服务器应用程序一般只有一条 TCP 连接）：
>
> 1. 控制连接：用于传送控制信息（命令和响应）
> 2. 数据连接：用于数据传送；
>
> 这种将命令和数据分开传送的思想大大提高了 FTP 的效率。

![FTP工作过程](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/ftp.png)

### Telnet:远程登陆协议

**Telnet 协议** 通过一个终端登陆到其他服务器，**建立在可靠的传输协议 TCP 之上**。Telnet 协议的最大缺点之一是所有数据（包括用户名和密码）**均以明文形式发送**，这有潜在的安全风险。这就是为什么如今很少使用Telnet并被一种称为SSH的非常安全的协议所取代的主要原因。

### SSH:安全的网络传输协议

**SSH（ Secure Shell）** 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。**SSH 建立在可靠的传输协议 TCP 之上**。

**Telnet 和 SSH 之间的主要区别在于 SSH 协议会对传输的数据进行加密保证数据安全性。**

### URI和URL的区别	*

- URI(Uniform Resource Identifier)：中文全称为**统一资源标志符，主要作用是唯一标识一个资源**。
- URL(Uniform Resource Location)：中文全称为**统一资源定位符，主要作用是提供资源的路径**。

有个经典的比喻是URI像是身份证，可以唯一标识一个人，而URL更像一个住址，可以通过URL找到这个人。

#### 在浏览器中输⼊url地址到显示主页的过程	***

> 面试超高频的一道题，一般能说清楚流程就可以。

1. 对输入到浏览器的url进行DNS解析，将域名转换为IP地址。
2. 和目的服务器建立TCP连接
3. 向目的服务器发送HTTP请求
4. 服务器处理请求并返回HTTP报文
5. 浏览器解析并渲染页面

### DNS

#### DNS的工作流程	***

DNS的定义：DNS的全称是domain name system，即域名系统。DNS是因特网上作为域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的去访问互联网而不用去记住能够被机器直接读取的IP地址。比如大家访问百度，更多地肯定是访问www.baidu.com，而不是访问112.80.248.74，因为这几乎无规则的IP地址实在太难记了。DNS要做的就是将www.baidu.com解析成112.80.248.74。

**DNS 的过程？**

关于DNS的获取流程：

DNS是应用层协议，事实上他是为其他应用层协议工作的，包括不限于HTTP和SMTP以及FTP，用于将用户提供的主机名解析为ip地址。

具体过程如下：

①用户主机上运行着DNS的客户端，就是我们的PC机或者手机客户端运行着DNS客户端了

②浏览器将接收到的url中抽取出域名字段，就是访问的主机名，比如http://www.baidu.com/, 并将这个主机名传送给DNS应用的客户端

③DNS客户机端向DNS服务器端发送一份查询报文，报文中包含着要访问的主机名字段（中间包括一些列缓存查询以及分布式DNS集群的工作）

④该DNS客户机最终会收到一份回答报文，其中包含有该主机名对应的IP地址

⑤一旦该浏览器收到来自DNS的IP地址，就可以向该IP地址定位的HTTP服务器发起TCP连接

![请添加图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/0215d27a7fce45fb93e20581b73cc99b.png)

![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/d3da10017481426ca8611b5bc2612e69.png)

### HTTP 和 HTTPS 

[**HTTP 请求包（浏览器信息）**](https://zh.wikipedia.org/wiki/HTTP%E5%A4%B4%E5%AD%97%E6%AE%B5)

我们先来看看 Request 包的结构，Request 包分为 3 部分，第一部分叫 Request line（请求行）, 第二部分叫 Request header（请求头）, 第三部分是 body（主体）。header 和 body 之间有个空行，请求包的例子所示:

```
GET /domains/example/ HTTP/1.1      // 请求行: 请求方法 请求 URL HTTP 协议/协议版本
Host：www.iana.org               // 服务端的主机名
User-Agent：Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4          // 浏览器信息
Accept：text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8  // 客户端能接收的 mine 能够接受的回应内容类型
Accept-Encoding：gzip,deflate,sdch       // 是否支持流压缩 	能够接受的编码方式列表
Accept-Charset：UTF-8,*;q=0.5        // 客户端字符编码集 	能够接受的字符集
// 空行,用于分割请求头和消息体
// 消息体,请求资源参数,例如 POST 传递的参数
```

HTTP 协议定义了很多与服务器交互的请求方法，最基本的有 4 种，分别是 GET, POST, PUT, DELETE。**一个 URL 地址用于描述一个网络上的资源，而 HTTP 中的 GET, POST, PUT, DELETE 就对应着对这个资源的查，增，改，删 4 个操作。**我们最常见的就是 GET 和 POST 了。GET 一般用于获取 / 查询资源信息，而 POST 一般用于更新资源信息。

通过 fiddler 抓包可以看到如下请求信息:

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/3.1.http.png)

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/3.1.httpPOST.png)

我们看看 **GET 和 POST 的区别**:

1. 我们可以看到 GET 请求消息体为空，POST 请求带有消息体。
2. **GET 提交的数据会放在 URL 之后，以 ? 分割 URL 和传输数据，参数之间以 & 相连**，如 EditPosts.aspx?name=test1&id=123456。**POST 方法是把提交的数据放在 HTTP 包的 body 中**。
3. GET 提交的数据大小有限制（因为浏览器对 URL 的长度有限制），而 POST 方法提交的数据没有限制。
4. GET 方式提交数据，会带来安全问题，比如一个登录页面，通过 GET 方式提交数据时，用户名和密码将出现在 URL 上，如果页面可以被缓存或者其他人可以访问这台机器，就可以从历史记录获得该用户的账号和密码。

**HTTP 响应包（服务器信息）**

我们再来看看 HTTP 的 response 包，他的结构如下：

```
HTTP/1.1 200 OK                     // 状态行
Server: nginx/1.0.8                 // 服务器使用的 WEB 软件名及版本
Date: Tue, 30 Oct 2012 04:14:25 GMT     // 发送时间
Content-Type: text/html             // 服务器发送信息的类型
Transfer-Encoding: chunked          // 表示发送 HTTP 包是分段发的
Connection: keep-alive              // 保持连接状态
Content-Length: 90                  // 主体内容长度
// 空行 用来分割消息头和主体
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"... // 消息体
```


Response 包中的**第一行叫做状态行**，由 HTTP 协议版本号， 状态码， 状态消息三部分组成。


状态码用来告诉 HTTP 客户端，HTTP 服务器是否产生了预期的 Response。HTTP/1.1 协议中定义了 5 类状态码， 状态码由三位数字组成，第一个数字定义了响应的类别

- 1XX 提示信息 - 表示请求已被成功接收，继续处理
- 2XX 成功 - 表示请求已被成功接收，理解，接受
- 3XX 重定向 - 要完成请求必须进行更进一步的处理
- 4XX 客户端错误 - 请求有语法错误或请求无法实现
- 5XX 服务器端错误 - 服务器未能实现合法的请求

我们看下面这个图展示了详细的返回信息，左边可以看到有很多的资源返回码，200 是常用的，表示正常信息，302 表示跳转。response header 里面展示了详细的信息。

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/3.1.response.png)

**HTTP 协议是无状态的和 Connection: keep-alive 的区别**

无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。从另一方面讲，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系。

HTTP 是一个无状态的面向连接的协议，无状态不代表 HTTP 不能保持 TCP 连接，更不能代表 HTTP 使用的是 UDP 协议（面对无连接）。

从 HTTP/1.1 起，默认都开启了 Keep-Alive 保持连接特性，简单地说，当一个网页打开完成后，客户端和服务器之间用于传输 HTTP 数据的 TCP 连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的 TCP 连接。

Keep-Alive 不会永久保持连接，它有一个保持时间，可以在不同服务器软件（如 Apache）中设置这个时间。

请求实例

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/3.1.web.png)

上面这张图我们可以了解到整个的通讯过程，同时细心的读者是否注意到了一点，一个 URL 请求但是左边栏里面为什么会有那么多的资源请求 (这些都是静态文件，go 对于静态文件有专门的处理方式)。

这个就是浏览器的一个功能，第一次请求 url，服务器端返回的是 html 页面，然后浏览器开始渲染 HTML：当解析到 HTML DOM 里面的图片连接，css 脚本和 js 脚本的链接，浏览器就会自动发起一个请求静态资源的 HTTP 请求，获取相对应的静态资源，然后浏览器就会渲染出来，最终将所有资源整合、渲染，完整展现在我们面前的屏幕上。

网页优化方面有一项措施是减少 HTTP 请求次数，就是把尽量多的 css 和 js 资源合并在一起，目的是尽量减少网页请求静态资源的次数，提高网页加载速度，同时减缓服务器的压力。

#### HTTP 和 HTTPS 

**一般http中存在如下问题：**

- 请求信息明文传输，容易被窃听截取。
- 数据的完整性未校验，容易被篡改
- 没有验证对方身份，存在冒充危险

HTTPS 协议（HyperText Transfer Protocol over Secure Socket Layer）：一般理解为HTTP+SSL/TLS，通过 SSL证书来验证服务器的身份，并为浏览器和服务器之间的通信进行加密。

**SSL（Secure Socket Layer，安全套接字层）**

[TLS](https://zh.wikipedia.org/wiki/%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A)（Transport Layer Security，传输层安全）及其前身**SSL**是一种[安全协议](https://zh.wikipedia.org/wiki/安全协议)，目的是为[互联网](https://zh.wikipedia.org/wiki/網際網路)通信提供安全及数据[完整性](https://zh.wikipedia.org/wiki/完整性)保障。

SSL握手过程：

![SSL安全套接层_服务端](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/wKioL1f680WxnnUsAAAfPPyWg7w510.png)

1.客户端发送client hello消息（消息中包括，ssl版本、加密算法列表、随机数）

2.服务端回应server hello消息（消息中包括，确认ssl版本、确认加密算法、随机数、server证书）

3.客户端收到消息后，首先检查证书的有效期，证书链，CRL等，确认身份，客户端再次生成随机数，然后利用证书公钥将3个随机数进行加密并发送给server

4.服务端收到消息后，利用私钥将加密信息解密得到三个随机数，并生成会话密钥



#### HTTP 与 HTTPS 的区别	***

|              |        HTTP        |                  HTTPS                  |
| :----------: | :----------------: | :-------------------------------------: |
|     端口     |         80         |                   443                   |
|    安全性    | 无加密，安全性较差 |         有加密机制，安全性较高          |
|   资源消耗   |        较少        |       由于加密处理，资源消耗更多        |
| 是否需要证书 |       不需要       |                  需要                   |
|     协议     | 运行在TCP协议之上  | 运行在SSL协议之上，SSL运行在TCP协议之上 |

#### 什么是对称加密与非对称加密	**

- 对称加密

  **对称加密指加密和解密使用同一密钥**，优点是运算速度快，缺点是如何安全将密钥传输给另一方。常见的对称加密算法有DES、AES等等。

- 非对称加密

  **非对称加密指的是加密和解密使用不同的密钥，一把公开的公钥，一把私有的私钥**。公钥加密的信息只有私钥才能解密，私钥加密的信息只有公钥才能解密。优点解决了对称加密中存在的问题。**缺点是运算速度较慢**。常见的非对称加密算法有RSA、DSA、ECC等等。

  **非对称加密的工作流程**：A生成一对非对称密钥，将公钥向所有人公开，B拿到A的公钥后使用A的公钥对信息加密后发送给A，经过加密的信息只有A手中的私钥能解密。这样B可以通过这种方式将自己的公钥加密后发送给A，两方建立起通信，可以通过对方的公钥加密要发送的信息，接收方用自己的私钥解密信息。

#### HTTPS的加密过程	***

上面已经介绍了对称加密和非对称加密的优缺点，**HTTPS是将两者结合起来，使用的对称加密和非对称加密的混合加密算法**。具体做法就是**使用非对称加密来传输对称密钥来保证安全性，使用对称加密来保证通信的效率**。

简化的**工作流程**：服务端生成一对非对称密钥，将公钥发给客户端。客户端生成对称密钥，用服务端发来的公钥进行加密，加密后发给服务端。服务端收到后用私钥进行解密，得到客户端发送的对称密钥。通信双方就可以通过对称密钥进行高效地通信了。

但是仔细想想这**其中存在一个很大地问题，就是客户端最开始如何判断收到的这个公钥就是来自服务端而不是其他人冒充的？**

这就需要证书上场了，服务端会**向一个权威机构申请一个证书来证明自己的身份**，到时候将证书（证书中包含了公钥）发给客户端就可以了，客户端收到证书后既证明了服务端的身份又拿到了公钥就可以进行下一步操作了。

HTTPS的加密过程：

1. 客户端向服务端发起第一次握手请求，告诉服务端客户端所支持的SSL的指定版本、加密算法及密钥长度等信息。
2. 服务端将自己的公钥发给数字证书认证机构，数字证书认证机构利用自己的私钥对服务器的公钥进行数字签名，并给服务器颁发公钥证书。
3. 服务端将证书发给客户端。
4. 客户端利用数字认证机构的公钥，向数字证书认证机构验证公钥证书上的数字签名，确认服务器公开密钥的真实性。
5. 客户端使用服务端的公开密钥加密自己生成的对称密钥，发给服务端。
6. 服务端收到后利用私钥解密信息，获得客户端发来的对称密钥。
7. 通信双方可用对称密钥来加密解密信息。

上述流程存在的一个问题是客户端哪里来的数字认证机构的公钥，其实，在很多浏览器开发时，会**内置常用数字证书认证机构的公钥**。

流程图如下：

![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000038526733)

#### 常用HTTP状态码	***

> 这也是一个面试经常问的题目,背下来就行了.

| 状态码 |       类别       |
| :----: | :--------------: |
|  1XX   |   信息性状态码   |
|  2XX   |    成功状态码    |
|  3XX   |   重定向状态码   |
|  4XX   | 客户端错误状态码 |
|  5XX   | 服务端错误状态码 |

常见的HTTP状态码

**1XX**

- 100 Continue：表示正常，客户端可以继续发送请求
- 101 Switching Protocols：切换协议，服务器根据客户端的请求切换协议。

**2XX**

- 200 OK：请求成功
- 201 Created：已创建，表示成功请求并创建了新的资源
- 202 Accepted：已接受，已接受请求，但未处理完成。
- 204 No Content：无内容，服务器成功处理，但未返回内容。
- 205 Reset Content：重置内容，服务器处理成功，客户端应重置文档视图。
- 206 Partial Content：表示客户端进行了范围请求，响应报文应包含Content-Range指定范围的实体内容

**3XX**

- 301 Moved Permanently：永久性重定向
- 302 Found：临时重定向
- 303 See Other：和301功能类似，但**要求客户端采用get方法**获取资源
- 304 Not Modified：所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。
- 305 Use Proxy：所请求的资源必须通过代理访问
- 307 Temporary Redirect： 临时重定向，与302类似，要求使用get请求重定向。

**4XX**

- 400 Bad Request：客户端请求的语法错误，服务器无法理解。
- 401 Unauthorized：表示发送的请求需要有认证信息。
- 403 Forbidden：服务器理解用户的请求，但是拒绝执行该请求
- 404 Not Found：服务器无法根据客户端的请求找到资源。
- 405 Method Not Allowed：客户端请求中的方法被禁止
- 406 Not Acceptable：服务器无法根据客户端请求的内容特性完成请求
- 408 Request Time-out：服务器等待客户端发送的请求时间过长，超时

**5XX**

- 500 Internal Server Error：服务器内部错误，无法完成请求
- 501 Not Implemented：服务器不支持请求的功能，无法完成请求
- 502 示作为网关或代理角色的服务器，从上游服务器（如tomcat、php-fpm）中接收到的响应是无效的
- 503 服务器尚未处于可以接受请求的状态

#### 常见的HTTP方法　***

[方法详解](https://blog.csdn.net/sycamorelg/article/details/111520804)

|  方法   | 作用                                                         |
| :-----: | :----------------------------------------------------------- |
|   GET   | 获取资源                                                     |
|  POST   | 用于将实体提交到指定的资源，通常**导致在服务器上的状态变化或副作用** |
|   PUT   | 用请求有效载荷替换目标资源的所有当前表示                     |
| DELETE  | 删除指定的资源                                               |
|  HEAD   | 和GET方法类似，但只返回报文首部，不返回报文实体主体部分      |
|  PATCH  | 对资源进行部分修改                                           |
| OPTIONS | 查询指定的URL支持的方法                                      |
| CONNECT | 要求用隧道协议连接代理                                       |
|  TRACE  | 服务器会将通信路径返回给客户端                               |

为了方便记忆，可以将PUT、DELETE、POST、GET理解为客户端对服务端的增删改查。

- PUT：上传文件，向服务器添加数据，可以看作增，put是幂等的
- DELETE：删除文件
- POST：传输数据，向服务器提交数据，对服务器数据进行更新。
- GET：获取资源，查询服务器资源

[post和put的区别](https://blog.csdn.net/mad1989/article/details/7918267)

#### GET和POST区别　***

- 作用

  GET用于获取资源，POST用于传输实体主体

- 参数位置

  **GET的参数放在URL中，POST的参数存储在实体主体中**，并且GET方法提交的请求的URL中的数据做多是2048字节，POST请求没有大小限制。

- 安全性

  GET方法因为参数放在URL中，安全性相对于POST较差一些

- 幂等性

  GET方法是具有幂等性的，而POST方法不具有幂等性。这里**幂等性指客户端连续发出多次请求，收到的结果都是一样的**.

#### HTTP 1.0、HTTP 1.1及HTTP 2.0的主要区别是什么　***

HTTP 1.0和HTTP 1.1的区别

- **长连接**

  HTTP 1.1支持长连接和请求的流水线操作。**长连接是指不在需要每次请求都重新建立一次连接**，HTTP 1.0默认使用短连接，每次请求都要重新建立一次TCP连接，资源消耗较大。**请求的流水线操作是指客户端在收到HTTP的响应报文之前可以先发送新的请求报文**，不支持请求的流水线操作需要等到收到HTTP的响应报文后才能继续发送新的请求报文。

- **缓存处理**

  在HTTP 1.0中主要使用header中的If-Modified-Since,Expires作为缓存判断的标准，HTTP 1.1引入了Entity tag，If-Unmodified-Since, If-Match等更多**可供选择的缓存头来控制缓存策略**。

- **错误状态码**

  在HTTP 1.1新增了24个错误状态响应码

- **HOST域**

  在HTTP 1.0 中认为每台服务器都会绑定唯一的IP地址，所以，请求中的URL并没有传递主机名。但后来一台服务器上可能存在多个虚拟机，它们共享一个IP地址，所以**HTTP 1.1中请求消息和响应消息都应该支持Host域**。

- **带宽优化及网络连接的使用**

  在HTTP 1.0中会存在浪费带宽的现象，**主要是因为不支持断点续传功能**，客户端只是需要某个对象的一部分，服务端却将整个对象都传了过来。**在HTTP 1.1中请求头引入了range头域，它支持只请求资源的某个部分**，返回的状态码为206。

HTTP 2.0的新特性

- **新的二进制格式**：HTTP 1.x的解析是基于文本，HTTP 2.0的解析采用二进制，实现方便，健壮性更好。
- **多路复用**：每一个request对应一个id，一个连接上可以有多个request，每个连接的request可以随机混在一起，这样接收方可以根据request的id将request归属到各自不同的服务端请求里。
- **header压缩**：在HTTP 1.x中，header携带大量信息，并且每次都需要重新发送，HTTP 2.0采用编码的方式减小了header的大小，**同时通信双方各自缓存一份header fields表**，避免了header的重复传输。
- **服务端推送**：客户端在请求一个资源时，会把相关资源一起发给客户端，这样客户端就不需要再次发起请求。

#### HTTP报文头属性

[参考](https://blog.csdn.net/weixin_39307273/article/details/104681002)、[参考](https://zh.wikipedia.org/wiki/HTTP%E5%A4%B4%E5%AD%97%E6%AE%B5)

##### 请求头属性

**Content-Type**

MediaType，即是Internet Media Type，**互联网媒体类型**；也叫做MIME类型，在Http协议消息头中，使用Content-Type来表示具体请求中的媒体类型信息。

常见的：

1. application/json  ： JSON数据格式
2. application/x-www-form-urlencoded ： <form encType="">中默认的encType，form表单数据被编码为key/value格式发送到服务器（表单默认的提交数据的格式）
3. multipart/form-data ： 需要在表单中进行文件上传时，就需要使用该格式

| Header                | 解释                                                         | 示例                                                    |
| :-------------------- | :----------------------------------------------------------- | :------------------------------------------------------ |
| Accept                | 指定客户端能够接收的内容类型                                 | Accept: text/plain, text/html                           |
| Accept-Charset        | 浏览器可以接受的字符编码集。                                 | Accept-Charset: iso-8859-5                              |
| Accept-Encoding       | 指定浏览器可以支持的web服务器返回内容压缩编码类型。          | Accept-Encoding: compress, gzip                         |
| Accept-Language       | 浏览器可接受的语言                                           | Accept-Language: en,zh                                  |
| Accept-Ranges         | 可以请求网页实体的一个或者多个子范围字段                     | Accept-Ranges: bytes                                    |
| Authorization         | HTTP授权的授权证书                                           | Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==       |
| **Cache-Control**     | 指定请求和响应遵循的缓存机制                                 | Cache-Control: no-cache                                 |
| **Connection**        | 表示是否需要持久连接。（HTTP 1.1默认进行持久连接）           | Connection: close                                       |
| **Cookie**            | HTTP请求发送时，会把保存在该请求域名下的所有cookie值一起发送给web服务器。 | Cookie: $Version=1; Skin=new;                           |
| Content-Length        | 请求的内容长度                                               | Content-Length: 348                                     |
| **Content-Type**      | 请求的与实体对应的MIME信息                                   | Content-Type: application/x-www-form-urlencoded         |
| Date                  | 请求发送的日期和时间                                         | Date: Tue, 15 Nov 2010 08:12:31 GMT                     |
| Expect                | 请求的特定的服务器行为                                       | Expect: 100-continue                                    |
| From                  | 发出请求的用户的Email                                        | From: user@email.com                                    |
| **Host**              | 指定请求的服务器的域名和端口号                               | Host: www.zcmhi.com                                     |
| If-Match              | 只有请求内容与实体相匹配才有效                               | If-Match: “737060cd8c284d8af7ad3082f209582d”            |
| **If-Modified-Since** | 如果请求的部分在指定时间之后被修改则请求成功，未被修改则返回304代码 | If-Modified-Since: Sat, 29 Oct 2010 19:43:31 GMT        |
| **If-None-Match**     | 如果内容未改变返回304代码，参数为服务器先前发送的Etag，与服务器回应的Etag比较判断是否改变 | If-None-Match: “737060cd8c284d8af7ad3082f209582d”       |
| If-Range              | 如果实体未改变，服务器发送客户端丢失的部分，否则发送整个实体。参数也为Etag | If-Range: “737060cd8c284d8af7ad3082f209582d”            |
| If-Unmodified-Since   | 只在实体在指定时间之后未被修改才请求成功                     | If-Unmodified-Since: Sat, 29 Oct 2010 19:43:31 GMT      |
| Max-Forwards          | 限制信息通过代理和网关传送的时间                             | Max-Forwards: 10                                        |
| Pragma                | 用来包含实现特定的指令                                       | Pragma: no-cache                                        |
| Proxy-Authorization   | 连接到代理的授权证书                                         | Proxy-Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ== |
| Range                 | 只请求实体的一部分，指定范围                                 | Range: bytes=500-999                                    |
| Referer               | 先前网页的地址，当前请求网页紧随其后,即来路                  | Referer: http://www.zcmhi.com/archives/71.html          |
| TE                    | 客户端愿意接受的传输编码，并通知服务器接受接受尾加头信息     | TE: trailers,deflate;q=0.5                              |
| Upgrade               | 向服务器指定某种传输协议以便服务器进行转换（如果支持）       | Upgrade: HTTP/2.0, SHTTP/1.3, IRC/6.9, RTA/x11          |
| User-Agent            | User-Agent的内容包含发出请求的用户信息                       | User-Agent: Mozilla/5.0 (Linux; X11)                    |
| Via                   | 通知中间网关或代理服务器地址，通信协议                       | Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1)             |
| Warning               | 关于消息实体的警告信息                                       | Warn: 199 Miscellaneous warning                         |

##### 响应头字段属性

| Header             | 解释                                                         | 示例                                                  |
| :----------------- | :----------------------------------------------------------- | :---------------------------------------------------- |
| Accept-Ranges      | 表明服务器是否支持指定范围请求及哪种类型的分段请求           | Accept-Ranges: bytes                                  |
| Age                | 从原始服务器到代理缓存形成的估算时间（以秒计，非负）         | Age: 12                                               |
| Allow              | 对某网络资源的有效的请求行为，不允许则返回405                | Allow: GET, HEAD                                      |
| **Cache-Control**  | 告诉所有的缓存机制是否可以缓存及哪种类型                     | Cache-Control: no-cache                               |
| Content-Encoding   | web服务器支持的返回内容压缩编码类型。                        | Content-Encoding: gzip                                |
| Content-Language   | 响应体的语言                                                 | Content-Language: en,zh                               |
| Content-Length     | 响应体的长度                                                 | Content-Length: 348                                   |
| Content-Location   | 请求资源可替代的备用的另一地址                               | Content-Location: /index.htm                          |
| Content-MD5        | 返回资源的MD5校验值                                          | Content-MD5: Q2hlY2sgSW50ZWdyaXR5IQ==                 |
| Content-Range      | 在整个返回体中本部分的字节位置                               | Content-Range: bytes 21010-47021/47022                |
| **Content-Type**   | 返回内容的MIME类型                                           | Content-Type: text/html; charset=utf-8                |
| Date               | 原始服务器消息发出的时间                                     | Date: Tue, 15 Nov 2010 08:12:31 GMT                   |
| **ETag**           | 请求变量的实体标签的当前值                                   | ETag: “737060cd8c284d8af7ad3082f209582d”              |
| Expires            | 响应过期的日期和时间                                         | Expires: Thu, 01 Dec 2010 16:00:00 GMT                |
| **Last-Modified**  | 请求资源的最后修改时间                                       | Last-Modified: Tue, 15 Nov 2010 12:45:26 GMT          |
| Location           | 用来重定向接收方到非请求URL的位置来完成请求或标识新的资源    | Location: http://www.zcmhi.com/archives/94.html       |
| Pragma             | 包括实现特定的指令，它可应用到响应链上的任何接收方           | Pragma: no-cache                                      |
| Proxy-Authenticate | 它指出认证方案和可应用到代理的该URL上的参数                  | Proxy-Authenticate: Basic                             |
| refresh            | 应用于重定向或一个新的资源被创造，在5秒之后重定向（由网景提出，被大部分浏览器支持） | Refresh: 5; url=http://www.zcmhi.com/archives/94.html |
| Retry-After        | 如果实体暂时不可取，通知客户端在指定时间之后再次尝试         | Retry-After: 120                                      |
| Server             | web服务器软件名称                                            | Server: Apache/1.3.27 (Unix) (Red-Hat/Linux)          |
| **Set-Cookie**     | 设置Http Cookie                                              | Set-Cookie: UserID=JohnDoe; Max-Age=3600; Version=1   |
| Trailer            | 指出头域在分块传输编码的尾部存在                             | Trailer: Max-Forwards                                 |
| Transfer-Encoding  | 文件传输编码                                                 | Transfer-Encoding:chunked                             |
| Vary               | 告诉下游代理是使用缓存响应还是从原始服务器请求               | Vary: *                                               |
| Via                | 告知代理客户端响应是通过哪里发送的                           | Via: 1.0 fred, 1.1 nowhere.com (Apache/1.1)           |
| Warning            | 警告实体可能存在的问题                                       | Warning: 199 Miscellaneous warning                    |
| WWW-Authenticate   | 表明客户端请求实体应该使用的授权方案                         | WWW-Authenticate: Basic                               |

####  HTTP 缓存技术

[参考](https://xiaolincoding.com/network/2_http/http_interview.html#http-%E7%BC%93%E5%AD%98%E6%9C%89%E5%93%AA%E4%BA%9B%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F)

对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都**缓存在本地**，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升。

所以，避免发送 HTTP 请求的方法就是通过**缓存技术**，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的头部有不少是针对缓存的字段。

HTTP 缓存有两种实现方式，分别是**强制缓存和协商缓存**

### Session、Cookie和Token的主要区别　***

HTTP协议是无状态的，即服务器无法判断用户身份。Session和Cookie可以用来进行身份辨认。

- Cookie

  **Cookie是保存在客户端一个小数据块**，其中包含了用户信息。当客户端向服务端发起请求，服务端会像客户端浏览器发送一个Cookie，客户端会把Cookie存起来，当下次客户端再次请求服务端时，会携带上这个Cookie，服务端会通过这个Cookie来确定身份。

- Session

  Session是通过Cookie实现的，和Cookie不同的是，**Session是存在服务端的**。当客户端浏览器第一次访问服务器时，服务器会为浏览器创建一个sessionid，将sessionid放到Cookie中，存在客户端浏览器。比如浏览器访问的是购物网站，将一本《图解HTTP》放到了购物车，当浏览器再次访问服务器时，服务器会取出Cookie中的sessionid，并根据sessionid获取会话中的存储的信息，确认浏览器的身份是上次将《图解HTTP》放入到购物车那个用户。

  服务器使用session把用户的信息临时保存在了服务器上，用户离开网站后session会被销毁。这种用户信息存储方式相对cookie来说更安全，可是session有一个**缺陷**：如果web服务器做了负载均衡，那么下一个操作请求到了另一台服务器的时候session会丢失。

- Token

  **客户端在浏览器第一次访问服务端时，服务端生成的一串字符串作为Token发给客户端浏览器**，下次浏览器在访问服务端时携带token即可无需验证用户名和密码，省下来大量的资源开销。看到这里很多人感觉这不是和sessionid作用一样吗？其实是不一样的，但是本文章主要针对面试，知识点很多，篇幅有限，几句话也解释不清楚，大家可以看看这篇文章，我觉得说的非常清楚了。[cookie、session与token的真正区别](https://link.segmentfault.com/?enc=GrshIvnLLcEv71iAkclwMg%3D%3D.nQrOC%2BH9qvoTZhrXWFV4ZMAVGKc6nprghE4D1yrALbQ337UyBWFRjmZxFN5i6fuS6B07x1WbMeYHaDO5MtQ2Uw%3D%3D)

  下面为了方便记忆，做了一个表格进行对比。

  |         |   存放位置   | 占用空间 | 安全性 |      应用场景      |
  | :-----: | :----------: | :------: | :----: | :----------------: |
  | Cookie  | 客户端浏览器 |    小    |  较低  |  一般存放配置信息  |
  | Session |    服务端    |    多    |  较高  | 存放较为重要的信息 |

#### 如果客户端禁止 cookie 能实现 session 还能用吗？　＊

可以，Session的作用是在服务端来保持状态，通过sessionid来进行确认身份，但sessionid一般是通过Cookie来进行传递的。如果Cooike被禁用了，可以通过在URL中传递sessionid。

#### Token	 ***

在Web领域基于Token的身份验证随处可见。在大多数使用Web API的互联网公司中，tokens 是多用户下处理认证的最佳方式。

以下几点特性会让你在程序中使用基于Token的身份验证

1. 无状态、可扩展
2. 支持移动设备
3. 跨程序调用
4. 安全

##### **Token的起源**

在介绍基于Token的身份验证的原理与优势之前，不妨先看看之前的认证都是怎么做的。

**基于服务器的验证**

我们都是知道**HTTP协议是无状态的**，这种无状态意味着程序需要验证每一次请求，从而辨别客户端的身份。

在这之前，程序都是通过在服务端存储的登录信息来辨别请求的。这种方式一般都是通过存储Session来完成。

随着Web，应用程序，已经移动端的兴起，这种验证的方式逐渐暴露出了问题。尤其是在可扩展性方面。

**基于服务器验证方式暴露的一些问题**

1. **Seesion：**每次认证用户发起请求时，服务器需要去创建一个记录来存储信息。当越来越多的用户发请求时，内存的开销也会不断增加。
2. **可扩展性：**在服务端的内存中使用Seesion存储登录信息，伴随而来的是可扩展性问题。
3. **CORS(跨域资源共享)：**当我们需要让数据跨多台移动设备上使用时，跨域资源的共享会是一个让人头疼的问题。在使用Ajax抓取另一个域的资源，就可以会出现禁止请求的情况。
4. **CSRF(跨站请求伪造)：**用户在访问银行网站时，他们很容易受到跨站请求伪造的攻击，并且能够被利用其访问其他的网站。

在这些问题中，可扩展行是最突出的。因此我们有必要去寻求一种更有行之有效的方法。

**基于Token的验证原理**

基于Token的身份验证是无状态的，我们不将用户信息存在服务器或Session中。

这种概念解决了在服务端存储信息时的许多问题

> NoSession意味着你的程序可以根据需要去增减机器，而不用去担心用户是否登录。

基于Token的身份验证的过程如下:

1. 用户通过用户名和密码发送请求。
2. 程序验证。
3. 程序返回一个签名的token 给客户端。
4. 客户端储存token,并且每次用于每次发送请求。
5. 服务端验证token并返回数据。

每一次请求都需要token。**token应该在HTTP的头部发送从而保证了Http请求无状态**。我们同样通过设置服务器属性Access-Control-Allow-Origin:* ，让服务器能接受到来自所有域的请求。

需要主要的是，在ACAO头部标明(designating)*时，不得带有像HTTP认证，客户端SSL证书和cookies的证书。

实现思路：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20190509111955368.jpg)

1. 用户登录校验，校验成功后就返回Token给客户端。
2. 客户端收到数据后保存在客户端
3. 客户端每次访问API是携带Token到服务器端。
4. 服务器端采用filter过滤器校验。校验成功则返回请求数据，校验失败则返回错误码

当我们在程序中认证了信息并取得token之后，我们便能通过这个Token做许多的事情。

我们甚至能基于创建一个基于权限的token传给第三方应用程序，这些第三方程序能够获取到我们的数据（当然只有在我们允许的特定的token）

##### **Token的优势**

**无状态、可扩展**

在客户端存储的Tokens是无状态的，并且能够被扩展。基于这种无状态和不存储Session信息，负载负载均衡器能够将用户信息从一个服务传到其他服务器上。

如果我们将已验证的用户的信息保存在Session中，则每次请求都需要用户向已验证的服务器发送验证信息(称为Session亲和性)。用户量大时，可能会造成一些拥堵。

但是不要着急。使用tokens之后这些问题都迎刃而解，因为tokens自己hold住了用户的验证信息。

**安全性**

请求中发送token而不再是发送cookie能够防止CSRF(跨站请求伪造)。即使在客户端使用cookie存储token，cookie也仅仅是一个存储机制而不是用于认证。不将信息存储在Session中，让我们少了对session操作。

token是有时效的，一段时间之后用户需要重新验证。我们也不一定需要等到token自动失效，token有撤回的操作，通过token revocataion可以使一个特定的token或是一组有相同认证的token无效。

**可扩展性**

Tokens能够创建与其它程序共享权限的程序。例如，能将一个随便的社交帐号和自己的大号(Fackbook或是Twitter)联系起来。当通过服务登录Twitter(我们将这个过程Buffer)时，我们可以将这些Buffer附到Twitter的数据流上(we are allowing Buffer to post to our Twitter stream)。

使用tokens时，可以提供可选的权限给第三方应用程序。当用户想让另一个应用程序访问它们的数据，我们可以通过建立自己的API，得出特殊权限的tokens。

**多平台跨域**

我们提前先来谈论一下CORS(跨域资源共享)，对应用程序和服务进行扩展的时候，需要介入各种各种的设备和应用程序。

> Having our API just serve data, we can also make the design choice to serve assets from a CDN. This eliminates the issues that CORS brings up after we set a quick header configuration for our application.

只要用户有一个通过了验证的token，数据和资源就能够在任何域上被请求到。

Access-Control-Allow-Origin: *   

基于标准创建token的时候，你可以设定一些选项。我们在后续的文章中会进行更加详尽的描述，但是标准的用法会在JSON Web Tokens体现。

最近的程序和文档是供给JSON Web Tokens的。它支持众多的语言。这意味在未来的使用中你可以真正的转换你的认证机制。

### Servlet是线程安全的吗	*

Servlet不是线程安全的，多线程的读写会导致数据不同步的问题。



## 传输层

### TCP与UDP有什么区别	***

|      | 是否面向连接 | 可靠性 |  传输形式  | 传输效率 | 消耗资源 |   应用场景    | 首部字节 |
| :--: | :----------: | :----: | :--------: | :------: | :------: | :-----------: | :------: |
| TCP  |   面向连接   |  可靠  |   字节流   |    慢    |    多    | 文件/邮件传输 |  20~60   |
| UDP  |    无连接    | 不可靠 | 数据报文段 |    快    |    少    | 视频/语音传输 |    8     |

> 有时候面试还会问到TCP的首部都包含什么

#### TCP首部：

前20个字节是固定的，后面有4n个字节是根据需而增加的选项，所以TCP首部最小长度为20字节。

![在这里插入图片描述](https://segmentfault.com/img/remote/1460000038526734)

- **源、目标端口号字段：占16比特**。TCP协议通过使用"端口"来标识源端和目标端的应用进程。端口号可以使用0到65535之间的任何数字。在收到服务请求时，操作系统动态地为客户端的应用程序分配端口号。在服务器端，每种服务在"众所周知的端口"（Well-Know Port）为用户提供服务。
- 顺序号字段：占**32比特**。用来**标识从TCP源端向TCP目标端发送的数据字节流**，它表示在这个报文段中的第一个数据字节。
- 确认号字段：占**32比特**。**只有ACK标志为1时，确认号字段才有效**。它包含目标端所期望收到源端的下一个数据字节。
- 头部长度字段（数据偏移）：占**4比特**。**给出头部占32比特的数目**。没有任何选项字段的TCP头部长度为20字节；最多可以有60字节的TCP头部。
- 标志位字段（U、A、P、R、S、F）：占6比特。各比特的含义如下：
  - URG：紧急指针（urgent pointer）有效。
  - ACK：确认序号有效。
  - PSH：接收方应该尽快将这个报文段交给应用层。
  - RST：重建连接。
  - SYN：发起一个连接。
  - FIN：释放一个连接。
- 窗口大小字段：占**16比特**。此字段用来**进行流量控制**。**单位为字节数**，这个值是本机期望一次接收的字节数。
- TCP校验和字段：**占16比特**。对整个TCP报文段，即TCP头部和TCP数据进行校验和计算，并由目标端进行验证。
- 紧急指针字段：占16比特。它是一个偏移量，和序号字段中的值相加表示紧急数据最后一个字节的序号。
- 选项字段：**占32比特**。可能包括"窗口扩大因子"、"时间戳"等选项。

#### UDP首部

UDP的首部只有8个字节，**源端口号、目的端口号、长度和校验和各两个字节**。

![在这里插入图片描述](https://segmentfault.com/img/remote/1460000038526736)



### TCP协议如何保证可靠传输	***

> 主要有校验和、序列号、超时重传、流量控制及拥塞避免等几种方法。

- **校验和：在发送算和接收端分别计算数据的校验和**，如果两者不一致，则说明数据在传输过程中出现了差错，TCP将丢弃和不确认此报文段。
- **序列号：TCP会对每一个发送的字节进行编号**，接收方接到数据后，会对发送方发送确认应答(ACK报文)，并且这个ACK报文中带有相应的确认编号，告诉发送方，下一次发送的数据从编号多少开始发。如果发送方发送相同的数据，接收端也可以通过序列号判断出，直接将数据丢弃。如果

![](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000038526737)

- **超时重传**：在上面说了序列号的作用，但如果发送方在发送数据后一段时间内（可以设置重传计时器规定这段时间）没有收到确认序号ACK，那么发送方就会重新发送数据。

  这里**发送方没有收到ACK可以分两种情况**，**如果是发送方发送的数据包丢失了**，接收方收到发送方重新发送的数据包后会马上给发送方发送ACK；**如果是接收方之前接收到了发送方发送的数据包，而返回给发送方的ACK丢失了**，这种情况，发送方重传后，接收方会**直接丢弃发送方冲重传的数据包**，然后**再次发送ACK响应报文**。

  如果数据被重发之后**还是没有收到接收方的确认应答**，则进行再次发送。此时，**等待确认应答的时间将会以2倍、4倍的指数函数延长，直到最后关闭连接**。

- **流量控制**：如果发送端发送的数据太快，接收端来不及接收就会出现丢包问题。为了解决这个问题，**TCP协议利用了滑动窗口进行了流量控制**。在TCP首部有一个16位字段大小的窗口，**窗口的大小就是接收端接收数据缓冲区的剩余大小**。接收端会在收到数据包后发送ACK报文时，将自己的窗口大小填入ACK中，**发送方会根据ACK报文中的窗口大小进而控制发送速度**。如果窗口大小为零，发送方会停止发送数据。

- **拥塞控制**：如果网络出现拥塞，则会产生丢包等问题，这时发送方会将丢失的数据包继续重传，网络拥塞会更加严重，所以在网络出现拥塞时应注意控制发送方的发送数据，降低整个网络的拥塞程度。**拥塞控制主要有四部分组成：慢开始、拥塞避免、快重传、快恢复**。

  ![在这里插入图片描述](https://segmentfault.com/img/remote/1460000038526742)

这里的**发送方会维护一个拥塞窗口的状态变量（congestion window，cwnd）**，它和流量控制的滑动窗口是不一样的，滑动窗口是根据接收方数据缓冲区大小确定的，而**拥塞窗口是根据网络的拥塞情况动态确定的**，一般来说发送方真实的发送窗口为滑动窗口和拥塞窗口中的最小值。

1. **慢开始**：为了**避免一开始发送大量的数据而产生网络阻塞**，会先初始化cwnd为1，当收到ACK后到下一个传输轮次，cwnd为2，以此类推成指数形式增长。

2. **拥塞避免**：因为cwnd的数量在慢开始是指数增长的，为了防止cwnd数量过大而导致网络阻塞，会设置一个慢开始的门限值ssthresh，当cwnd>=ssthresh时，进入到拥塞避免阶段，cwnd每个传输轮次加1。但网络出现超时，会**将门限值ssthresh变为出现超时cwnd数值的一半**，**cwnd重新设置为1**，如上图，在第12轮出现超时后，cwnd变为1，ssthresh变为12。

3. **快重传**：在**网络中如果出现超时或者阻塞，则按慢开始和拥塞避免算法进行调整**。但**如果只是丢失某一个报文段**，如下图(图片来源于网络)，则使用快重传算法。

   ![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000038526741)

从上图可知，**接收方正确地接收到M1和M2，而M3丢失，由于没有接收到M3，在接收方收到M5、M6和M7时，并不会进行确认，也就是不会发送ACK。**这时根据前面说的保证TCP可靠性传输中的序列号的作用，接收方这时不会接收M5，M6，M7，接收方可以什么都不会，因为发送方长时间未收到M3的确认报文，会对M3进行重传。**除了这样，接收方也可以重复发送M2的确认报文**，这样发送端长时间未收到M3的确认报文也会继续发送M3报文。

**但是根据快重传算法，要求在这种情况下，需要快速向发送端发送M2的确认报文，在发送方收到三个M2的确认报文后，无需等待重传计时器所设置的时间，可直接进行M3的重传，这就是快重传。**(面试时说这一句就够了，前面是帮助理解)

4. **快恢复**：从上图可以看到，当发送收到三个重复的ACK，会进行快重传和快恢复。**快恢复是指将ssthresh设置为发生快重传时的cwnd数量的一半，而cwnd不是设置为1而是设置为为门限值ssthresh，并开始拥塞避免阶段**。

### TCP的三次握手及四次挥手	***

> 必考题

在介绍三次握手和四次挥手之前，先介绍一下TCP头部的一些常用字段。

- 序号：seq，占32位，用来标识从发送端到接收端发送的字节流。
- 确认号：ack，占32位，只有ACK标志位为1时，确认序号字段才有效，ack=seq+1。
- 标志位：
  - SYN：发起一个新连接。
  - FIN：释放一个连接。
  - ACK：确认序号有效。

#### 三次握手

> 三次握手的本质就是确定发送端和接收端具备收发信息的能力，在能流畅描述三次握手的流程及其中的字段含义作用的同时还需要记住每次握手时**接收端和发送端的状态**。这个比较容易忽略。

先看一张很经典的图（图片来源于网络），**发送端有CLOSED、SYN-SENT、ESTABLISHED**三种状态，**接收端有CLOSED、LISTEN、SYN-RCVD、ESTABLISHED四种状态**。![在这里插入图片描述](https://segmentfault.com/img/remote/1460000038526739)

假设发送端为客户端，接收端为服务端。开始时客户端和服务端的状态都是CLOSE。

- 第一次握手：客户端向服务端发起建立连接请求，**客户端会随机生成一个起始序列号x**，客户端向服务端发送的字段中包含标志位SYN=1，序列号seq=100。第一次握手前客户端的状态为CLOSE，第一次握手后客户端的状态为SYN-SENT。此时服务端的状态为LISTEN
- 第二次握手：服务端在收到客户端发来的报文后，会**随机生成一个服务端的起始序列号y**，然后给客户端回复一段报文，其中包括标志位SYN=1，ACK=1，序列号seq=y，确认号ack=x+1。第二次握手前服务端的状态为LISTEN，第二次握手后服务端的状态为SYN-RCVD，此时客户端的状态为SYN-SENT。（其中SYN=1表示要和客户端建立一个连接，ACK=1表示确认序号有效）
- 第三次握手：客户端收到服务端发来的报文后，会再向服务端发送报文，其中包含标志位ACK=1，序列号seq=x+1，确认号ack=y+1。第三次握手前客户端的状态为SYN-SENT，第三次握手后客户端和服务端的状态都为ESTABLISHED。

> 需要注意的一点是，第一次握手，**客户端向服务端发起建立连接报文，会占一个序列号**。但是第三次握手，同样是客户端向服务端发送报文，这次却不占序列号，所以建立连接后，**客户端向服务端发送的第一个数据的序列号为x+1**。

#### 四次挥手

客户端在四次挥手过程中有ESTABLISHED、FIN-WAIT-1、FIN-WAIT-2、TIME-WAIT、CLOSED等五个状态，服务端有ESTABLISHED、CLOSE-WAIT、LAST-ACK、CLOSED等四种状态。最好记住每次挥手时服务端和客户端的状态。

假设客户端首先发起的断开连接请求
![在这里插入图片描述](https://segmentfault.com/img/remote/1460000038526740)

- 第一次挥手：客户端向服务端发送的数据完成后，**向服务端发起释放连接报文**，报文包含标志位FIN=1，序列号seq=u。此时客户端只能接收数据，不能向服务端发送数据。
- 第二次挥手：服务端收到客户端的释放连接报文后，向客户端发送确认报文，包含标志位ACK=1，序列号seq=v，确认号ack=u+1。**此时客户端到服务端的连接已经释放掉，客户端不能向服务端发送数据，服务端也不能向客户端发送数据。**但服务端到客户端的**单向连接还能正常传输数据**。
- 第三次挥手：**服务端发送完数据后向客户端发出连接释放报文**，报文包含标志位FIN=1，标志位ACK=1，序列号seq=w，确认号ack=u+1。
- 第四次挥手：客户端收到服务端发送的释放连接请求，向服务端发送确认报文，包含标志位ACK=1，序列号seq=u+1，确认号ack=w+1。MSL（Maximum Segment Lifetime）表示为“最长报文段寿命”，它是任何报文在网络上存在的最长的最长时间，超过这个时间报文将被丢弃。

#### 为什么TCP连接的时候是3次？两次是否可以？

不可以，主要从以下两方面考虑（假设客户端是首先发起连接请求）：

1. 假设建立TCP连接仅需要两次握手，**那么如果第二次握手时，服务端返回给客户端的确认报文丢失了，客户端这边认为服务端没有和他建立连接，而服务端却以为已经和客户端建立了连接，**并且可能向服务端已经开始向客户端发送数据，但客户端并不会接收这些数据，浪费了资源。如果是三次握手，不会出现双方连接还未完全建立成功就开始发送数据的情况。
2. **如果服务端接收到了一个早已失效的来自客户端的连接请求报文**，会向客户端发送确认报文同意建立TCP连接。但因为客户端并不需要向服务端发送数据，所以此次TCP连接没有意义并且浪费了资源。

#### 为什么TCP连接的时候是3次，关闭的时候却是4次？

**因为需要确保通信双方都能通知对方释放连接，假设客服端发送完数据向服务端发送释放连接请求**，当客服端并不知道，服务端是否已经发送完数据，所以此次断开的是客服端到服务端的单向连接，服务端返回给客户端确认报文后，服务端还能继续单向给客户端发送数据。当服务端发送完数据后还需要向客户端发送释放连接请求，客户端返回确认报文，TCP连接彻底关闭。所以断开TCP连接需要客户端和服务端分别通知对方并分别收到确认报文，一共需要四次。

#### TIME_WAIT和CLOSE_WAIT的区别在哪?

默认客户端首先发起断开连接请求

- 从上图可以看出，**CLOSE_WAIT是被动关闭形成的**，当客户端发送FIN报文，服务端返回ACK报文后进入CLOSE_WAIT。
- **TIME_WAIT是主动关闭形成的**，当第四次挥手完成后，客户端进入TIME_WAIT状态。

#### 为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？

**MSL的意思是报文的最长寿命**，可以从两方面考虑：

1. 客户端发送第四次挥手中的报文后，再经过2MSL，**可使本次TCP连接中的所有报文全部消失**，不会出现在下一个TCP连接中。
2. 考虑丢包问题，**如果第四挥手发送的报文在传输过程中丢失了，那么服务端没收到确认ack报文就会重发第三次挥手的报文**。如果客户端发送完第四次挥手的确认报文后直接关闭，而这次报文又恰好丢失，则会造成服务端无法正常关闭。

#### 如果已经建立了连接，但是客户端突然出现故障了怎么办？

**如果TCP连接已经建立，在通信过程中，客户端突然故障，那么服务端不会一直等下去，过一段时间就关闭连接了**。具体原理是TCP有一个**保活机制，主要用在服务器端**，用于检测已建立TCP链接的客户端的状态，防止因客户端崩溃或者客户端网络不可达，而服务器端一直保持该TCP链接，占用服务器端的大量资源(因为Linux系统中可以创建的总TCP链接数是有限制的)。

**保活机制原理**：设置TCP保活机制的保活时间keepIdle，**即在TCP链接超过该时间没有任何数据交互时，发送保活探测报文**；设置保活探测报文的发送时间间隔keepInterval；设置保活探测报文的总发送次数keepCount。如果在keepCount次的保活探测报文均没有收到客户端的回应，则服务器端即关闭与客户端的TCP链接。

### TCP面向字节流和UDP面向报文的区别

[参考](https://www.cnblogs.com/Frank-Hong/p/13944392.html)

*“TCP是一种流模式的协议，UDP是一种数据报模式的协议”*

#### 1、TCP

打个比方比喻TCP，你家里有个蓄水池，你可以里面倒水，蓄水池上有个龙头，你可以通过龙头将水池里的水放出来，然后用各种各样的容器装（杯子、矿泉水瓶、锅碗瓢盆）接水。

上面的例子中，往水池里倒几次水和接几次水是没有必然联系的，也就是说你可以只倒一次水，然后分10次接完。另外，水池里的水接多少就会少多少；往里面倒多少水，就会增加多少水，但是不能超过水池的容量，多出的水会溢出。

结合TCP的概念，水池就好比接收缓存，倒水就相当于发送数据，接水就相当于读取数据。好比你通过TCP连接给另一端发送数据，你只调用了一次write，发送了100个字节，但是对方可以分10次收完，每次10个字节；你也可以调用10次write，每次10个字节，但是对方可以一次就收完。（假设数据都能到达）但是，你发送的数据量不能大于对方的接收缓存（流量控制），如果你硬是要发送过量数据，则对方的缓存满了就会把多出的数据丢弃。

#### 2、UDP

UDP和TCP不同，**发送端调用了几次write，接收端必须用相同次数的read读完**。UDP是基于报文的，在接收的时候，每次最多只能读取一个报文，报文和报文是不会合并的，如果缓冲区小于报文长度，则多出的部分会被丢弃。也就说，如果不指定MSG_PEEK标志，每次读取操作将消耗一个报文。

#### 3、为什么

其实，这种不同是由TCP和UDP的特性决定的。TCP是面向连接的，也就是说，在连接持续的过程中，socket中收到的数据都是由同一台主机发出的（劫持什么的不考虑），因此，知道保证数据是有序的到达就行了，至于每次读取多少数据自己看着办。

而UDP是无连接的协议，也就是说，只要知道接收端的IP和端口，且网络是可达的，任何主机都可以向接收端发送数据。这时候，如果一次能读取超过一个报文的数据，则会乱套。比如，主机A向发送了报文P1，主机B发送了报文P2，如果能够读取超过一个报文的数据，那么就会将P1和P2的数据合并在了一起，这样的数据是没有意义的。

### TCP粘包

[参考](https://mp.weixin.qq.com/s/YpQGsRyyrGNDu1cOuMy83w)

#### MTU 和MSS

- **MTU: Maximum Transmit Unit**，**最大传输单元**。 由**网络接口层（数据链路层）**提供给**网络层**最大一次传输数据的大小；一般 MTU=**1500 Byte**。
  假设IP层有 <= 1500 byte 需要发送，只需要一个 IP 包就可以完成发送任务；假设 IP 层有> 1500 byte 数据需要发送，需要分片才能完成发送，分片后的 IP Header ID 相同。
- **MSS：Maximum Segment Size** 。**TCP 提交给 IP 层最大分段大小**，不包含 TCP Header 和  TCP Option，只包含 TCP Payload ，**MSS 是 TCP 用来限制应用层最大的发送字节数。**
  假设 MTU= 1500 byte，那么 **MSS = 1500- 20(IP Header) -20 (TCP Header) = 1460 byte**，如果应用层有 **2000 byte** 发送，那么需要两个切片才可以完成发送，第一个 TCP 切片 = 1460，第二个 TCP 切片 = 540。

##### 如何查看MSS？

我们都知道TCP三次握手，而**`MSS`会在三次握手的过程中传递给对方，用于通知对端本地最大可以接收的TCP报文数据大小**（**不包含TCP和IP报文首部**）。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkibqoGDoqpbongQDojtvSCs52icGwf9AJgR2vZHVwWnO0EFTjqEFPib99LbM0QvwkIl9OWAvSwLtNmg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

比如上图中，B将自己的MSS发送给A，建议A在发数据给B的时候，采用`MSS=1420`进行分段。而B在发数据给A的时候，同样会带上`MSS=1372`。两者在对比后，会采用**小的**那个值（1372）作为通信的`MSS值`，这个过程叫`MSS协商`。

> 另外，一般情况下MSS + 20（TCP头）+ 20（IP头）= MTU，上面抓包的图里对应的MTU分别是1372+40 和 1420+40。同一个路径上，**MTU不一定是对称的**，也就是说A到B和B到A，两条路径上的MTU可以是不同的，对应的MSS也一样。

##### 三次握手中协商了MSS就不会改变了吗？

当然不是，每次执行TCP发送消息的函数时，**会重新计算一次MSS**，再进行分段操作。

##### 对端不传MSS会怎么样？

我们再看TCP的报头。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkibqoGDoqpbongQDojtvSCs7hENZsukAa4rLTicUiaIXluxYHCsJ1UCYd3zjGeCJl6uxXXC0icDNtxicA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

其实MSS是作为可选项引入的，只不过一般情况下MSS都会传，但是万一遇到了哪台机器的实现上比较调皮，**不传MSS**这个可选项。那对端该怎么办？

**如果没有接收到对端TCP的MSS，本端TCP默认采用MSS=536Byte**。

那为什么会是`536`？

```
536（data） + 20（tcp头）+20（ip头）= 576Byte
```

前面提到了IP会切片，那会切片，也就会重组，而这个**576正好是 IP 最小重组缓冲区的大小**。

##### 如何查看MTU

在`mac`控制台输入 `ifconfig`命令，可以看到MTU的值为多大。

```
$ ipconfig
lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> mtu 16384
    ...
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
    ...
p2p0: flags=8843<UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST> mtu 2304
    ...
```

可以看到这上面有好几个**MTU**，可以简单理解为每个网卡的处理能力不同，所以对应的MTU也不同。当然这个值是可以修改的，但不在今天的讨论范畴内，不再展开。

在一台机器的应用层到这台机器的网卡，**这条链路上**，基本上可以保证，`MSS < MTU`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkibqoGDoqpbongQDojtvSCsS4OPm5QBxRvGX1wUFlNnHyyagYF7j1bZ0ibP8ib6Byw8x14cO6G2iciayg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 为什么MTU一般是1500

这其实是**由传输效率决定**的。首先，虽然我们平时用的网络感觉挺稳定的，但其实这是因为TCP在背地里做了各种重传等保证了传输的可靠，其实背地里线路是动不动就丢包的，而越大的包，发生丢包的概率就越大。

那是不是包越小就越好？也不是

但是如果选择一个比较小的长度，假设选择`MTU`为`300Byte`，`TCP payload = 300 - IP Header - TCP Header = 300 - 20 - 20 = 260 byte`。那有效传输效率`= 260 / 300 = 86%`

而如果以太网长度为1500，那有效传输效率`= 1460 / 1500 = 96%` ，显然比 `86%` 高多了。

所以，**包越小越不容易丢包，包越大，传输效率又越高**，因此权衡之下，选了`1500`。

#### 粘包

**TCP，Transmission Control Protocol**。传输控制协议，是一种面向连接的、可靠的、基于**字节流**的传输层通信协议。

其中跟**粘包**关系最大的就是**基于字节流**这个特点。

字节流可以理解为一个双向的通道里流淌的数据，这个**数据**其实就是我们常说的二进制数据，简单来说就是一大堆 01 串。这些 01 串之间**没有任何边界**。

![图片](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/640)

应用层传到 TCP 协议的数据，不是以**消息报为单位**向目的主机发送，而是以**字节流**的方式发送到下游，这些数据可能被**切割和组装**成各种数据包，接收端收到这些数据包后没有正确还原原来的消息，因此出现粘包现象。

#### 为什么要组装发送的数据

上面提到 TCP **切割**数据包是为了能顺利通过网络这根水管。相反，还有一个**组装**的情况。如果前后两次 TCP 发的数据都远小于 MSS，比如就几个字节，每次都单独发送这几个字节，就比较**浪费**网络 io 。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61kkL00WTdryTicAMt4TdSic7YqticnYBgL53axVha21EfKzf2IQYa0TQeA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)正常发送数据包

比如小白爸让小白出门给买一瓶酱油，小白出去买酱油回来了。小白妈又让小白出门买一瓶醋回来。小白前后结结实实跑了两趟，影响了打游戏的时间。

优化的方法也比较简单。当小白爸让小白去买酱油的时候，小白先**等待**，继续打会游戏，这时候如果小白妈让小白买瓶醋回来，小白可以一次性带着两个需求出门，再把东西带回来。

上面说的其实就是`TCP`的 **Nagle 算法**优化，目的是**为了避免发送小的数据包**。

在 Nagle 算法开启的状态下，数据包在以下两个情况会被发送：

- 如果**包长度达到`MSS`（或含有`Fin`包），立刻发送**，否则**等待**下一个包到来；如果下一包到来后两个包的总长度超过`MSS`的话，就会进行拆分发送；
- **等待超时（一般为`200ms`），第一个包没到`MSS`长度**，但是又迟迟等不到第二个包的到来，则立即发送。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61dyXtL6zcgK4YAnuum27gKlobMGiawm6MtdYQKzyAPVCrYX4Cv3fmXPA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)Nagle2

- 由于启动了**Nagle算法**，msg1 小于 mss ，此时等待`200ms`内来了一个 msg2，msg1 + msg2 > MSS，因此把 msg2 分为 msg2(1) 和 msg2(2)，msg1 + msg2(1) 包的大小为`MSS`。此时发送出去。
- 剩余的 msg2(2) 也等到了 msg3，同样 msg2(2) + msg3 > MSS，因此把 msg3分为msg3(1) 和 msg3(2)，msg2(2) + msg3(1) 作为一个包发送。
- 剩余的 msg3(2) 长度不足`mss`，同时在`200ms`内没有等到下一个包，等待超时，直接发送。
- 此时三个包虽然在图里**颜色不同**，但是实际场景中，他们都是**一整个 01 串**，如果处理开发者把第一个收到的 msg1 + msg2(1) 就当做是一个完整消息进行处理，就会看上去就**像是两个包粘在一起**，就会导致**粘包问题**。

#### 关掉Nagle算法就不会粘包了吗？

**Nagle** 算法其实是个**有些年代**的东西了，诞生于 1984 年。对于应用程序一次发送一字节数据的场景，如果没有 Nagle 的优化，这样的包立马就发出去了，会导致网络由于太多的包而过载。

但是今天网络环境比以前好太多，Nagle 的优化帮助就没那么大了。而且它的延迟发送，有时候还可能导致调用延时变大，比如打游戏的时候，你操作如此丝滑，但却因为 Nagle 算法延迟发送导致慢了一拍，就问你难受不难受。

所以现在**一般也会把它关掉**。

看起来，Nagle 算法的优化作用貌似不大，还会导致**粘包"问题"**。那么是不是关掉这个算法就可以解决掉这个**粘包"问题"**呢？

```
TCP_NODELAY = 1
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61R86w2q8mo095f1xTofzpbkbx5wfJWpnLFkyByfo58avDfFdtKlpqcw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)关闭Nagle就不会粘包了吗

- 接受端应用层在收到 **msg1** 时立马就取走了，那此时 **msg1** 没粘包问题
- **msg2**到了后，应用层在忙，没来得及取走，就呆在 **TCP Recv Buffer** 中了
- **msg3**此时也到了，跟 **msg2** 和 **msg3** 一起放在了 **TCP Recv Buffer** 中
- 这时候应用层忙完了，来取数据，图里是两个颜色作区分，但实际场景中**都是 01 串**，此时一起取走，发现还是粘包。

因此，就算关闭 Nagle 算法，接收数据端的应用层没有及时读取 TCP Recv Buffer 中的数据，还是会发生粘包。

#### 怎么处理粘包

粘包出现的根本原因是不确定**消息的边界**。接收端在面对**"无边无际"的二进制流**的时候，根本不知道收了多少 01 才算**一个消息**。一不小心拿多了就说是**粘包**。其实粘包根本不是 TCP 的问题，是使用者对于 TCP 的理解有误导致的一个问题。

只要在发送端每次发送消息的时候给消息**带上识别消息边界的信息**，接收端就可以根据这些信息识别出消息的边界，从而区分出每个消息。

常见的方法有

- 加入特殊标志

  ![图片](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/sdasDSdseaf.png)

  可以通过特殊的标志作为头尾，比如当收到了`0xfffffe`或者回车符，则认为收到了新消息的头，此时继续取数据，直到收到下一个头标志`0xfffffe`或者尾部标记，才认为是一个完整消息。类似的像 HTTP 协议里当使用 **chunked 编码** 传输时，使用若干个 chunk 组成消息，最后由一个标明长度为 0 的 chunk 结束。

- 加入消息长度信息

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61icRxwJm1s37Nnf6C9U9aibfn4yfiaWCRAiaSfCC4CC7XZR9xagANA99TSA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这个一般配合上面的特殊标志一起使用，在收到头标志时，里面还可以带上消息长度，以此表明在这之后多少 byte 都是属于这个消息的。如果在这之后正好有符合长度的 byte，则取走，作为一个完整消息给应用层使用。在实际场景中，HTTP 中的`Content-Length`就起了类似的作用，当接收端收到的消息长度小于 Content-Length 时，说明还有些消息没收到。那接收端会一直等，直到拿够了消息或超时，关于这一点[上一篇文章](https://mp.weixin.qq.com/s?__biz=MzAwMDAxNjU4Mg==&mid=2247484204&idx=1&sn=0e83aabb2a48570b5bec563a777f4d26&scene=21#wechat_redirect)里有更详细的说明。

可能这时候会有朋友会问，采用`0xfffffe`标志位，用来标志一个数据包的开头，你就不怕你发的某个数据里正好有这个内容吗？

是的，**怕**，所以一般除了这个标志位，发送端在发送时还会加入各种校验字段（`校验和`或者对整段完整数据进行 `CRC` 之后获得的数据）放在标志位后面，在接收端拿到整段数据后校验下确保它就是发送端发来的完整数据。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61oPJj2EZC8s1ediavXCESb1ft8NZGcUricq98Zn9SYd5HYUyzMQXqSQiaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

#### UDP 会粘包吗

跟 `TCP` 同为传输层的另一个协议，**UDP，User Datagram Protocol**。用户数据包协议，是面向无连接，不可靠的，基于**数据报**的传输层通信协议。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61D67XP3SSnGYBVQwwKZNMIfKjL6LmlvADXtBduGibw3cMAPEuNm02lNQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

基于**数据报**是指无论应用层交给 UDP 多长的报文，UDP 都照样发送，即**一次发送一个报文**。至于如果数据包太长，需要分片，那也是IP层的事情，大不了效率低一些。**UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界**。而接收方在接收数据报的时候，也不会像面对 TCP 无穷无尽的二进制流那样不清楚啥时候能结束。正因为**基于数据报**和**基于字节流**的差异，**TCP 发送端发 10 次字节流数据，而这时候接收端可以分 100 次去取数据，每次取数据的长度可以根据处理能力作调整；但 UDP 发送端发了 10 次数据报，那接收端就要在 10 次收完，且发了多少，就取多少，确保每次都是一个完整的数据报。**

我们先看下**IP报头**

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt611ytG6aAibl9M6ZD0R8xtqL1fdcFvQPvmyyF612C1YgNLRTUcFBiaq0kg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

注意这里面是有一个 16 位的总长度的，意味着 IP 报头里记录了整个 IP 包的总长度。接着我们再看下 **UDP 的报头**。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61WTlkvANOtkJXYwSicaBFwiaXm8yttulVYoMnIkfLK9EibmVfQoh0ibDL2g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

在报头中有`16bit`用于指示 **UDP 数据报文的长度**，假设这个长度是 n ，以此作为**数据边界**。因此在接收端的应用层能清晰地将不同的数据报文区分开，从报头开始取 n 位，就是一个**完整的**数据报，从而避免粘包和拆包的问题。

当然，就算没有这个位（**16位 UDP 长度**），因为 IP 的头部已经包含了数据的**总长度**信息，此时如果 IP 包（网络层）里放的数据使用的协议是 UDP（传输层），那么这个**总长度**其实就包含了 UDP 的头部和 UDP 的数据。

因为 **UDP 的头部长度固定为 8 字节**（ 1 字节= 8 位，8 字节= 64 位，上图中除了`数据和选项`以外的部分），那么这样就很容易的算出 UDP 的数据的长度了。因此说 UDP 的长度信息其实是冗余的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61kBtYAia2mOw3zFXLowlSJa24UQmCtlKN1u9SAtHASsSnML1798VHxog/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

```
UDP Data 的长度 = IP 总长度 - IP Header 长度 - UDP Header 长度
```

可以再来看下 **TCP 的报头**

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61ZS5Iz893UNAcJjw7thoBCjMKf3v8hDQg3elYnqtYnp5ySxsxSnutbg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

TCP首部里是没有长度这个信息的，跟UDP类似，同样可以通过下面的公式获得当前包的TCP数据长度。

```
TCP Data 的长度 = IP 总长度 - IP Header 长度 - TCP Header 长度。
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt610LLpAibdk2ed4HwFM9V6WZ2TmyAXGYLwyDzH5w4RLicCM4d1u4icZAwTQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

跟 UDP 不同在于，TCP 发送端在发的时候就**不保证发的是一个完整的数据报**，仅仅看成一连串无结构的字节流，这串字节流在接收端收到时哪怕知道长度也没用，因为它很可能只是某个完整消息的一部分。

#### 为什么长度字段冗余还要加到 UDP 首部中

关于这一点，查了很多资料，`《 TCP-IP 详解（卷2）》`里说可能是因为要**用于计算校验和**。也有的说是因为UDP底层使用的可以不是IP协议，毕竟 IP 头里带了总长度，正好可以用于计算 UDP 数据的长度，万一 UDP 的底层不是IP层协议，而是其他网络层协议，就不能继续这么计算了。

但我觉得，最重要的原因是，IP 层是网络层的，而 UDP 是传输层的，到了传输层，数据包就已经不存在IP头信息了，那么此时的UDP数据会被放在 UDP 的  `Socket Buffer` 中。当应用层来不及取这个 UDP 数据报，那么两个数据报在数据层面其实都是一堆 01 串。此时读取第一个数据报的时候，会先读取到 UDP 头部，**如果这时候 UDP 头不含 UDP 长度信息，那么应用层应该取多少数据才算完整的一个数据报呢**？

因此 UDP 头的这个长度其实跟 TCP 为了防止粘包而在消息体里加入的边界信息是起一样的作用的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61koKBsrbuTI5telFZsQuXicUoDbaYeeHRs8ZRDUWL73svXic0vMiayLibBg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)为什么UDP要冗余一个长度字段

面试的时候咱就把这些全说出去，**显得**咱好像经过了深深的思考一样，面试官可能会觉得咱特别爱思考，**加分加分**。

如果我说错了，请把我的这篇文章转发给更多的人，让大家记住这个满嘴胡话的人，在关注之后狠狠的私信骂我，拜托了！

#### IP 层有粘包问题吗

IP 层会对大包进行切片，是不是也有粘包问题？

先说结论，不会。首先前文提到了，粘包其实是由于使用者无法正确区分消息边界导致的一个问题。

先看看 IP 层的切片分包是怎么回事。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlnuLFPj8NI57ZdUQiaibNt61a7Ab5RT2ia1mFSibx8IDGgvmx0bicc2JBn6XMblwITFg283KLA5dAaM4g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 如果消息过长，`IP层`会按 **MTU 长度**把消息分成 **N 个切片**，每个切片带有自身在**包里的位置（offset）**和**同样的IP头信息**。
- 各个切片在网络中进行传输。每个数据包切片可以在不同的路由中流转，然后**在最后的终点汇合后再组装**。
- 在接收端收到第一个切片包时会申请一块新内存，创建IP包的数据结构，等待其他切片分包数据到位。
- 等消息全部到位后就把整个消息包给到上层（传输层）进行处理。

可以看出整个过程，`IP 层`从按长度切片到把切片组装成一个数据包的过程中，都只管运输，都不需要在意消息的边界和内容，都不在意消息内容了，那就不会有粘包一说了。

`IP 层`表示：我只管把发送端给我的数据传到接收端就完了，我也不了解里头放了啥东西。

听起来就像 “**我不管产品的需求傻不傻X，我实现了就行，我不问，也懒得争了**”，这思路值得每一位优秀的划水程序员学习，**respect**。

#### 为什么IP层会分片，TCP还要分段

假设有一份数据，较大，且在TCP层不分段，如果这份数据在发送的过程中出现**丢包**现象，TCP会发生重传，那么重传的就是这一大份数据（虽然IP层会把数据切分为MTU长度的N多个小包，但是**TCP重传的单位却是那一大份数据**）。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnkibqoGDoqpbongQDojtvSCs6bjuZcQgyicI10Dc8p45lT1RKY3oBRplZlUSWFQeoB97h66vOVRqcicQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

如果TCP把这份数据，分段为N个小于等于MSS长度的数据包，到了IP层后加上IP头和TCP头，还是小于MTU，那么IP层也不会再进行分包。此时在传输路上发生了丢包，那么TCP重传的时候也只是重传那一小部分的MSS段。效率会比TCP不分段时更高。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnkibqoGDoqpbongQDojtvSCsFGj3A8szIvsAIpPaVz0gJO1AmicT3Ppk8cczFvqDuQqIeZ8rjfqiaZfQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

类似的，传输层除了TCP外，还有UDP协议，但UDP本身不会分段，所以当数据量较大时，只能交给IP层去分片，然后传到底层进行发送。

也就是说，正常情况下，在一台机器的传输层到网络层**这条链路上**，如果传输层对数据做了分段，那么IP层就不会再分片。如果传输层没分段，那么IP层就可能会进行分片。

说白了，**数据在TCP分段，就是为了在IP层不需要分片，同时发生重传的时候只重传分段后的小份数据**。

#### 总结

粘包这个问题的根因是由于开发人员没有正确理解 TCP 面向字节流的数据传输方式，本身并不是 TCP 的问题，是开发者的问题。

- 数据在TCP分段，在IP层就不需要分片，同时发生重传的时候只重传分段后的小份数据
- TCP分段时使用MSS，IP分片时使用MTU
- MSS是通过MTU计算得到，在三次握手和发送消息时都有可能产生变化。
- IP分片是**不得已**的行为，尽量不在IP层分片，尤其是链路上中间设备的IP分片。因此，在IPv6中已经禁止中间节点设备对IP报文进行分片，分片只能在链路的最开头和最末尾两端进行。
- 建立连接后，路径上节点的MTU值改变时，可以通过PMTU发现更新发送端MTU的值。这种情况下，PMTU发现通过浪费N次发送机会来换取的PMTU，TCP因为有重传可以保证可靠性，在UDP就相当于消息直接丢了。

- TCP 不管发送端要发什么，都基于字节流把数据发到接收端。这个字节流里可能包含上一次想要发的数据的部分信息。接收端根据需要在消息里加上识别消息边界的信息。不加就可能出现粘包问题。
- TCP 粘包跟Nagle算法有关系，但关闭 Nagle 算法并不解决粘包问题。
- UDP 是基于数据报的传输协议，不会有粘包问题。
- **IP 层也切片，但是因为不关心消息里有啥，因此有不会有粘包问题**。
- `TCP` 发送端可以发 `10 次`字节流数据，接收端可以分 `100 次`去取；`UDP` 发送端发了 `10 次`数据报，那接收端就要在 `10 次`收完。

数据包也只是按着 TCP 的方式进行组装和拆分，**如果数据包有错，那数据包也只是犯了每个数据包都会犯的错而已**。

## 网络层

### 概述

因为网络层是整个互联网的核心，因此应当让网络层尽可能简单。网络层向上**只提供简单灵活的、无连接的、尽最大努力交互的数据报服务**。

使用 IP 协议，可以把异构的物理网络连接起来，使得在网络层看起来好像是一个统一的网络。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f38643737396162372d666663632d343763362d393065632d6564653832363062323336382e706e67)



与 IP 协议配套使用的还有三个协议：

- 地址解析协议 ARP（Address Resolution Protocol）
- 网际控制报文协议 ICMP（Internet Control Message Protocol）
- 网际组管理协议 IGMP（Internet Group Management Protocol）

### IP 数据报格式

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f38356330356662312d353534362d346335302d393232312d3231663233316364633863352e6a7067)



- **版本** : 有 4（IPv4）和 6（IPv6）两个值；
- **首部长度** : 占 4 位，因此**最大值为 15**。值为 1 表示的是 1 个 32 位字的长度，也就是 4 字节。因为**固定部分长度为 20 字节**，因此该值**最小为 5**。如果可选字段的长度不是 4 字节的整数倍，就用尾部的填充部分来填充。
- **区分服务** : 用来获得更好的服务，一般情况下不使用。
- **总长度** : **包括首部长度和数据部分长度**。
- **生存时间** ：TTL（time to live），它的**存在是为了防止无法交付的数据报在互联网中不断兜圈子**。以**路由器跳数为单位**，当 TTL 为 0 时就丢弃数据报。
- **上层协议标识** ：指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP、UDP 等。
- **首部检验和** ：因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。
- **标识** : 在**数据报长度过长从而发生分片的情况下，相同数据报的不同分片具有相同的标识符**。
- **片偏移** : 和标识符一起，用于发生分片的情况。片偏移的单位为 8 字节。

![img](https://camo.githubusercontent.com/5a2935c7e397acf88b53a3b9ff7ad79a254cc814c581d667673a55cd4387c020/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f32336261383930652d653131632d343565322d613230632d3634643231376638333433302e706e67)



### IP 地址编址方式

IP 地址的编址方式经历了三个历史阶段：

- 分类
- 子网划分
- 无分类

#### 1. 分类

由两部分组成，网络号和主机号，其中不同分类具有不同的网络号长度，并且是固定的。

IP 地址 ::= {< 网络号 >, < 主机号 >}

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f63626635306562382d323262342d343532382d613265372d6431383731343364353766372e706e67)

![一文看懂IP地址：含义、分类、子网划分、查与改、路由器与IP地址](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/eb33cd41f526b4eaadd87f6a02e50f8a.jpeg)

#### 2. 子网划分

通过在主机号字段中拿一部分作为子网号，把两级 IP 地址划分为三级 IP 地址。

IP 地址 ::= {< 网络号 >, < 子网号 >, < 主机号 >}

要使用子网，必须配置子网掩码。一个 B 类地址的默认子网掩码为 255.255.0.0，如果 B 类地址的子网占两个比特，那么子网掩码为 11111111 11111111 11000000 00000000，也就是 255.255.192.0。

注意，外部网络看不到子网的存在。

#### 3. 无分类

**无分类编址 CIDR 消除了传统 A 类、B 类和 C 类地址以及划分子网的概念，使用网络前缀和主机号来对 IP 地址进行编码，网络前缀的长度可以根据需要变化**。

IP 地址 ::= {< 网络前缀号 >, < 主机号 >}

CIDR 的记法上采用**在 IP 地址后面加上网络前缀长度**的方法，例如 128.14.35.7/20 表示前 20 位为网络前缀。

CIDR 的地址掩码可以继续称为子网掩码，子网掩码首 1 长度为网络前缀的长度。

一个 CIDR 地址块中有很多地址，一个 CIDR 表示的网络就可以表示原来的很多个网络，并且在路由表中只需要一个路由就可以代替原来的多个路由，减少了路由表项的数量。把这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为 **构成超网** 。

在路由表中的项目由“网络前缀”和“下一跳地址”组成，在查找时可能会得到不止一个匹配结果，应当采用最长前缀匹配来确定应该匹配哪一个。

#### 子网掩码

子网掩码(subnet mask)又叫网络掩码、地址掩码、子网络遮罩，它是一种**用来指明一个IP地址的哪些位标识的是主机所在的子网，以及哪些位标识的是主机的位掩码**。子网掩码不能单独存在，它必须结合IP地址一起使用。子网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分。

它的主要作用有两个，一是**用于屏蔽IP地址的一部分以区别网络标识和主机标识，并说明该IP地址是在局域网上**，还是在远程网上。二是用于将一个大的IP网络划分为若干小的子网络。

**例子**

**1、255.255.255.0**

**子网掩码由连续的1和0组成**，**连续的1表示网络地址**，**连续的0表示主机地址**，**通过0的个数可以计算出子网的容量**（子网中主机的IP地址范围）。首先来看看默认的子网掩码255.255.255.0是怎么划分子网的，将该子网掩码的二进制由24个1和8个0组成，8个0表示该子网掩码划分出的子网容量为256（2的8次方），也就是说192.168.1.0-255都在同一个子网中，这256个地址中可用地址只有254个，因为**规定每个子网的第一个IP地址为网段地址，最后一个IP地址为广播地址，都不可用**。举例说明：对于网段192.168.1.0，如果子网掩码设置255.255.255.0，192.168.1.1-192.168.1.254为可用IP地址，设置这个范围内的IP地址，计算机之间能正常联网。

![img](https://pic4.zhimg.com/v2-141ad9160a6ad61a371d19c6d07e22d3_r.jpg)

**2、255.255.255.252**

当然上面是默认的情况，也是最简单的情况。下面我们分析子网掩码255.255.255.252是怎么划分子网的。将该子网掩码转换成二进制为30个1和2个0，表示每个子网中只有4个IP地址（2的2次方），192.168.1.0-255的地址段共可划分64个子网，第一个子网的地址范围是192.168.1.0-192.168.1.3，第二个子网的地址范围是192.168.1.4-192.168.1.7，依次类推。**其中每个子网第一个和最后一个IP地址不可用**，可用的只有2个IP地址。也就是说：如果子网掩码设置为255.255.255.252，那么该子网只能容纳两台电脑，而且这两台电脑的IP必须在一个子网内才能正常联网，例如一台电脑的IP设为192.168.1.10，另外一台电脑的IP必须设置为192.168.1.9。

![img](https://pic1.zhimg.com/v2-0c0f5766af51e86d46309413859569a4_r.jpg)


**3、子网划分实战**

通过以上两个例子读者应该明白子网掩码的作用了，下面通过一个实际的例子检验刚才的学习成果。某个小型公司有四个部门：行政、研发、营销、售后，每个部门各40台计算机接入公司局域网交换机，如果要在192.168.1.0网段为每个部门划分子网，子网掩码应该怎么设置，每个子网的地址范围分别是什么？

192.168.1.0网段共256个地址，划分4个子网，每个子网需要64个地址；64是2的6次方，子网掩码应该以6个0结尾，剩下的用1补齐，由26个1和6个0组成，转换成十进制是255.255.255.192；每个子网共64个IP地址，掐头去尾后可用地址只有62个，第1个子网的可用IP地址范围是：192.168.1.1-62，第2个子网可用IP地址范围是192.168.1.65-126，第3个子网的可用IP地址范围是：192.168.1.129-190，第4个子网可用IP地址范围是192.168.1.193-254；该公司各部门计算机按照3中的IP地址范围进行设置，所有计算机的子网掩码都必须设置为255.255.255.192，设置完毕后各部门内的计算机能正常联网，不同部门间的计算机无法直接联通。

### 有了IP地址，为什么还要用MAC地址？　＊＊

> 简单来说，标识网络中的一台计算机，比较常用的就是IP地址和MAC地址，但计算机的IP地址可由用户自行更改，管理起来相对困难，而MAC地址不可更改，所以一般会**把IP地址和MAC地址组合起来使用**。具体是如何组合使用的在上面的ARP协议中已经讲的很清楚了。
>
> 那只用MAC地址不用IP地址可不可以呢？其实也是不行的，因为在最早就是MAC地址先出现的，并且当时并不用IP地址，只用MAC地址，后来随着网络中的设备越来越多，整个路由过程越来越复杂，便出现了子网的概念。对于目的地址在其他子网的数据包，路由只需要将数据包送到那个子网即可，这个过程就是上面说的ARP协议。
>
> 那为什么要用IP地址呢？是因为IP地址是和地域相关的，对于同一个子网上的设备，IP地址的前缀都是一样的，这样路由器通过IP地址的前缀就知道设备在在哪个子网上了，而只用MAC地址的话，路由器则需要记住每个MAC地址在哪个子网，这需要路由器有极大的存储空间，是无法实现的。
>
> **IP地址可以比作为地址，MAC地址为收件人**，在一次通信过程中，两者是缺一不可的。

### 说一下ping的过程　＊＊

ping是ICMP(网际控制报文协议)中的一个重要应用，ICMP是网络层的协议。ping的作用是测试两个主机的连通性。

ping的工作过程：

1. 向目的主机发送多个ICMP回送请求报文
2. 根据目的主机返回的回送报文的时间和成功响应的次数估算出数据包往返时间及丢包率。

### 路由器和交换机的区别？　＊

|        | 所属网络模型的层级 |                             功能                             |
| :----: | :----------------: | :----------------------------------------------------------: |
| 路由器 |       网络层       | 识别IP地址并根据IP地址转发数据包，维护数据表并基于数据表进行最佳路径选择 |
| 交换机 |     数据链库层     |              识别MAC地址并根据MAC地址转发数据帧              |

### ICMP

[ICMP](https://zhuanlan.zhihu.com/p/369623317)

[ICMP](https://zhuanlan.zhihu.com/p/369623317)是（Internet Control Message Protocol）Internet控制 [报文](http://baike.baidu.com/view/175122.htm) 协议。它是 [TCP/IP协议族](http://baike.baidu.com/view/2221037.htm) 的一个子协议，用于在IP [主机](http://baike.baidu.com/view/23880.htm) 、 [路由](http://baike.baidu.com/view/18655.htm) 器之间传递控制消息。控制消息是指 [网络通](http://baike.baidu.com/view/8079702.htm) 不通、 [主机](http://baike.baidu.com/view/23880.htm) 是否可达、 [路由](http://baike.baidu.com/view/18655.htm) 是否可用等网络本身的消息

在IPv4协议中最常用的ICMP消息类型有以下几种:

- **回显应答(类型0)和回显请求(类型8):这是Ping程序发送的信息。**
- **目标不可达(类型3)**
- 源抑制(类型4):这是一种用于通知发送者路由器或者主机出现阻塞现象的ICMP消息，发送者需要降低发送速度。
- 重定向(类型5):这个消息用来向可以访问两台路由器的主机说“请使用另一台路由器”。
- 路由器信息应答(类型9)和路由器信息请求(类型10)
- 超时(类型11):这个消息有两种用途。第一，当超过IP生存期时向发送系统发出错误信息。第二，如果分段的IP数据报没有在某种时限内重新组合，这个消息将通知发送系统。

### IGMP

**互联网组管理协议**（**I**nternet **G**roup **M**anagement **P**rotocol，[IGMP](https://network.51cto.com/article/590835.html)）是用于管理网路协议[多播](https://zh.wikipedia.org/wiki/多播)组成员的一种通信协议。IP主机和相邻的路由器利用IGMP来创建多播组的组成员。像[ICMP](https://zh.wikipedia.org/wiki/ICMP)用于单播连接一样，IGMP也是IP多播说明的一个完整部分。 IGMP为互联网协议的一种，属于开放系统链接(OSI) 模块的第三层协议，I**P主机用它将主机的多点发送成员人数报告给临近的多点发送路由器**。

### ARP协议＊＊

[参考](https://www.cnblogs.com/jmilkfan-fanguiju/p/12789810.html)

[地址解析协议](https://www.cnblogs.com/cxuanBlog/p/14265315.html)（Address Resolution Protocol，  ARP）, **ARP协议属于网络层的协议**，主要作用是实现从IP地址转换为MAC地址。在每个主机或者路由器中都建有一个ARP缓存表，表中有IP地址及IP地址对应的MAC地址。先来看一下什么时IP地址和MAC地址。

- IP地址：**IP地址是指互联网协议地址**，IP地址是IP协议提供的一种统一的地址格式，它为互联网上的每一个网络和每一台主机分配一个逻辑地址，以此来屏蔽物理地址的差异。
- MAC地址：**MAC地址又称物理地址**，由网络设备制造商生产时写在硬件内部，不可更改，并且每个以太网设备的MAC地址都是唯一的。

ARP 实现由 IP 地址得到 MAC 地址。

[![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f62396437396135612d653761662d343939622d623938392d6631303438336537316238622e6a7067)](https://camo.githubusercontent.com/81c45941a689239d8a0ef8c121b17a5a82ba3a626e92a7bf5c3cf53c6035ea26/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f62396437396135612d653761662d343939622d623938392d6631303438336537316238622e6a7067)

数据在传输过程中，会先从高层传到底层，然后在通信链路上传输。从下图可以看到TCP报文在网络层会被封装成IP数据报，在数据链路层被封装成MAC帧，然后在通信链路中传输。**在网络层使用的是IP地址，在数据据链路层使用的是MAC地址**。MAC帧在传送时的源地址和目的地址使用的都是MAC地址，在**通信链路上的主机或路由器也都是根据MAC帧首部的MAC地址接收MAC帧**。并且**在数据链路层是看不到IP地址**的，只有当数据传到网络层时去掉MAC帧的首部和尾部时才能在IP数据报的首部中找到源IP地址和目的地址。

![在这里插入图片描述](https://segmentfault.com/img/remote/1460000038526732)

**网络层实现的是主机之间的通信**，而**链路层实现的是链路之间的通信**，所以从下图可以看出，在数据传输过程中，IP数据报的源地址(IP1)和目的地址(IP2)是一直不变的，而MAC地址(硬件地址)却一直随着链路的改变而改变。

![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1460000038526738)

ARP的工作流程(面试时问ARP协议主要说这个就可以了)：

1. 在局域网内，主机A要向主机B发送IP数据报时，首先会在主机A的**ARP缓存表**中查找是否有IP地址及其对应的MAC地址，如果有，则将MAC地址写入到MAC帧的首部，并通过局域网将该MAC帧发送到MAC地址所在的主机B。
2. 如果主机A的ARP缓存表中没有主机B的IP地址及所对应的MAC地址，**主机A会在局域网内广播发送一个ARP请求分组**。局域网内的所有主机都会收到这个ARP请求分组。这个 ARP 请求包中包含了主机 A 想要知道的主机 B 的 IP 地址的 MAC 地址。
3. 主机B在看到主机A发送的ARP请求分组中有自己的IP地址，会**向主机A以单播的方式发送一个带有自己MAC地址的响应分组**。
4. 主机A收到主机B的ARP响应分组后，会在ARP缓存表中写入主机B的IP地址及其IP地址对应的MAC地址。
5. 如果主机A和主机B不在同一个局域网内，即使知道主机B的MAC地址也是不能直接通信的，必须通过路由器转发到主机B的局域网才可以通过主机B的MAC地址找到主机B。并且主机A和主机B已经可以通信的情况下，**主机A的ARP缓存表中寸的并不是主机B的IP地址及主机B的MAC地址，而是主机B的IP地址及该通信链路上的下一跳路由器的MAC地址**。这就是上图中的源IP地址和目的IP地址一直不变，而MAC地址却随着链路的不同而改变。
6. 如果主机A和主机B不在同一个局域网，参考上图中的主机H1和主机H2，这时主机H1需要先广播找到路由器R1的MAC地址，再由R1广播找到路由器R2的MAC地址，最后R2广播找到主机H2的MAC地址，建立起通信链路。

### ARP 的数据包结构

下图为以太网数据帧的数据结构：

![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20191109134810910.png)

- 以太网首部
  - **以太网目的地址和以太网源地址**：表示目标设备和发送端设备的以太网地址。其中，目的以太网地址全为 1，即 FF:FF:FF:FF:FF:FF，为广播地址，在本地局域网内，所有的以太网接口都要接收这个数据帧。
  - **帧类型**：表示以太网数据帧的数据类型。对于 ARP 来说这里是 0x0806，对于 IP 数据报文是 0x0800，还有 RARP（逆地址解析协议）是 0x8035。

**NOTE**：如果以太网数据帧的帧类型是 ARP，那么数据部分就会拥有 ARP 首部。

- 以太网 ARP 字段
  - **ARP 首部**：
    - **硬件类型和协议类型**：这两个字段用来描述 ARP 分组。前者表示 ARP 工作在什么类型的网络中，需要什么样的物理地址；后者则表示需要映射的协议地址类型。例如：用来描述 IPv4 时，则协议类型为 IP，硬件类型为以太网的物理地址。所以协议类型为 0x0800，硬件类型为 1，表示以太网地址。
    - **硬件地址长度和协议地址长度**：用来描述 IPv4 时，分别表示 MAC 地址和 IP 地址的长度，分别为 6 字节和 4 字节。
    - **操作码**：表示 ARP 数据包的操作类型。
      - 1：ARP 请求
      - 2：ARP 应答
      - 3：RARP 请求
      - 4：RARP 应答
  - **源硬件地址和目标硬件地址**：与以太网首部中的目的以太网地址和源以太网地址重叠。
  - **源协议地址和目标协议地址**：用来描述 IPv4 时，分别表示源设备 IP 地址和目的设备 IP 地址。

**NOTE 1**：在发送 ARP 请求时，目标硬件地址是空着的（全为 0），因为 ARP 请求要请求的就是它的值，当目标设备请求收到后，就会把自己的硬件地址写到这个字段，并把操作码改成 2，再应答回去。

**NOTE 2**：在 ARP 中，有效数据的长度为 28 个字节，不足以太网的最小长度 46 字节长度。所以需要填充字节，填充字节最小长度为 18 个字节。

### RARP

**逆地址解析协议**（Reverse Address Resolution Protocol，RARP），是一种[网络协议](https://zh.wikipedia.org/wiki/网络协议)，[互联网工程任务组](https://zh.wikipedia.org/wiki/互联网工程任务组)（IETF）在RFC903中描述了RARP。RARP使用与ARP相同的报头结构，作用与[ARP](https://zh.wikipedia.org/wiki/ARP)相反。RARP用于将[MAC地址](https://zh.wikipedia.org/wiki/MAC地址)转换为[IP地址](https://zh.wikipedia.org/wiki/IP地址)。其因为较限于IP地址的运用以及其他的一些缺点，因此渐为更新的[BOOTP](https://zh.wikipedia.org/wiki/BOOTP)或[DHCP](https://zh.wikipedia.org/wiki/DHCP)所取代。

从 MAC 地址定位 IP 地址的一种协议，将打印机服务器等小型嵌入式设备接入网络时会使用到。

##### 工作原理

1. 发送主机发送一个本地的RARP广播，在此广播包中，声明自己的MAC地址并且请求任何收到此请求的RARP服务器分配一个IP地址；
2. 本地网段上的RARP服务器收到此请求后，检查其RARP列表，查找该MAC地址对应的IP地址；
3. 如果存在，RARP服务器就给源主机发送一个响应数据包并将此IP地址提供给对方主机使用；
4. 如果不存在，RARP服务器对此不做任何的响应；
5. 源主机收到从RARP服务器的响应信息，就利用得到的IP地址进行通讯；如果一直没有收到RARP服务器的响应信息，表示初始化失败。

## 数据链路层

![在这里插入图片描述](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/20191109140531920.png)

- **数据帧首部**：占 14 字节，包含有目标主机网卡 MAC 地址、源主机网卡 MAC 地址以及数据帧类型标识。
- **数据**：从上层（网络层）传递下来的数据包，范围在 [46, 1500] 个字节之间。
- **数据帧尾部**：占 4 个字节，是 CRC 校验序列，用来确定数据帧在传输过程中是否有损坏。

## 物理层

### 通信方式

根据信息在传输线上的传送方向，分为以下三种通信方式：

- 单工通信：单向传输
- 半双工通信：双向交替传输
- 全双工通信：双向同时传输

### 带通调制

模拟信号是连续的信号，数字信号是离散的信号。带通调制把数字信号转换为模拟信号。

[![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f63333466343530332d663632632d343034332d396463362d3365303332383836353764662e6a7067)](https://camo.githubusercontent.com/59e58745fe8f7b2ec9020e6fb10da969ee0ecc5ba40bb04318690171449586bc/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f63333466343530332d663632632d343034332d396463362d3365303332383836353764662e6a7067)

## Socket 编程

> 针对 TCP 应该如何 Socket 编程？

![基于 TCP 协议的客户端和服务器工作](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM0LmpwZw?x-oss-process=image/format,png)

- 服务端和客户端初始化 `socket`，得到文件描述符；
- 服务端调用 `bind`，将**绑定在 IP 地址和端口**;
- 服务端调用 `listen`，**进行监听端口**；
- 服务端调用 `accept`，**接受该套接字连接请求**；
- 客户端调用 `connect`，**向服务器端的地址和端口发起连接请求**；
- 服务端 `accept` 返回用于传输的 `socket` 的文件描述符；
- 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；
- 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。

这里需要注意的是，服务端调用 `accept` 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。

所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。

成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。

> listen 时候参数 backlog 的意义？

Linux内核中会维护两个队列：

- 半连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态；
- 全连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态；

![ SYN 队列 与 Accpet 队列 ](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM1LmpwZw?x-oss-process=image/format,png)

```c
int listen (int socketfd, int backlog)
```

- 参数一 socketfd 为 socketfd 文件描述符
- 参数二 backlog，这参数在历史版本有一定的变化

在早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。

在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，**所以现在通常认为 backlog 是 accept 队列。**

**但是上限值是内核参数 somaxconn 的大小，也就说 accpet 队列长度 = min(backlog, somaxconn)。**

想详细了解 TCP 半连接队列和全连接队列，可以看这篇：[TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？(opens new window)](https://mp.weixin.qq.com/s/2qN0ulyBtO2I67NB_RnJbg)

> accept 发生在三次握手的哪一步？

我们先看看客户端连接服务端时，发送了什么？

![socket 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/%E7%BD%91%E7%BB%9C/socket%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png)

- 客户端的协议栈向服务器端发送了 SYN 包，并告诉服务器端当前发送序列号 client_isn，客户端进入 SYN_SENT 状态；
- 服务器端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 client_isn+1，表示对 SYN 包 client_isn 的确认，同时服务器也发送一个 SYN 包，告诉客户端当前我的发送序列号为 server_isn，服务器端进入 SYN_RCVD 状态；
- 客户端协议栈收到 ACK 之后，使得应用程序从 `connect` 调用返回，表示客户端到服务器端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务器端的 SYN 包进行应答，应答数据为 server_isn+1；
- ACK 应答包到达服务器端后，服务器端的 TCP 连接进入 ESTABLISHED 状态，同时服务器端协议栈使得 `accept` 阻塞调用返回，这个时候服务器端到客户端的单向连接也建立成功。至此，客户端与服务端两个方向的连接都建立成功。

从上面的描述过程，我们可以得知**客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。**

> 客户端调用 close 了，连接是断开的流程是什么？

我们看看客户端主动调用了 `close`，会发生什么？

![客户端调用 close 过程](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzM3LmpwZw?x-oss-process=image/format,png)

- **客户端调用 `close`，表明客户端没有数据需要发送了**，则此时会向服务端发送 FIN 报文，进入 FIN_WAIT_1 状态；
- 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 `EOF` 到接收缓冲区中，应用程序可以通过 `read` 调用来感知这个 FIN 包。这个 `EOF` 会被**放在已排队等候的其他已接收的数据之后**，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态；
- 接着，当处理完数据后，自然就会读到 `EOF`，于是也调用 `close` 关闭它的套接字，这会使得服务端发出一个 FIN 包，之后处于 LAST_ACK 状态；
- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；
- 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；
- 客户端经过 `2MSL` 时间之后，也进入 CLOSE 状态；

## 网络攻击

### XSS

#### 是什么

XSS攻击：跨站脚本攻击(Cross-Site Scripting)，为了不和层叠样式表(Cascading Style Sheets, CSS)的缩写混淆，故将跨站脚本攻击缩写为XSS。XSS是一种常见的web安全漏洞，它**允许攻击者将恶意代码植入到提供给其它用户使用的页面**中。不同于大多数攻击(一般只涉及攻击者和受害者)，XSS涉及到三方，即攻击者、客户端与Web应用。XSS的攻击目标是**为了盗取存储在客户端的cookie或者其他网站用于识别客户端身份的敏感信息**。一旦获取到合法用户的信息后，攻击者甚至可以假冒合法用户与网站进行交互。

XSS通常可以分为两大类：

1. 存储型XSS，主要出现在让用户输入数据，供其他浏览此页的用户进行查看的地方，包括留言、评论、博客日志和各类表单等。应用程序从数据库中查询数据，在页面中显示出来，攻击者在相关页面输入恶意的脚本数据后，用户浏览此类页面时就可能受到攻击。这个流程简单可以描述为：`恶意用户的Html输入Web程序->进入数据库->Web程序->用户浏览器`。
2. 反射型XSS，主要做法是将脚本代码加入URL地址的请求参数里，请求参数进入程序后在页面直接输出，用户点击类似的恶意链接就可能受到攻击。

比如说我写了一个网站，然后攻击者在上面发布了一个文章，内容是这样的 `<script>alert(document.cookie)</script>`,如果我没有对他的内容进行处理，直接存储到数据库，那么下一次当其他用户访问他的这篇文章的时候，服务器从数据库读取后然后响应给客户端，浏览器执行了这段脚本，就会将cookie展现出来，这就是典型的存储型XSS。

#### 如何预防XSS

答案很简单，坚决不要相信用户的任何输入，并过滤掉输入中的所有特殊字符。这样就能消灭绝大部分的XSS攻击。

目前防御XSS主要有如下几种方式：

1. **过滤特殊字符**
   避免XSS的方法之一主要是将用户所提供的内容进行过滤(如上面的`script`标签)。
2. **使用HTTP头指定类型**
   `w.Header().Set("Content-Type","text/javascript")`
   这样就可以让浏览器解析javascript代码，而不会是html输出。
3. 对于链接跳转，如`<a href="xxx"` 等，要校验内容，禁止以script开头的非法链接。
4. 限制输入长度

###  SQL注入

#### 什么是SQL注入

**攻击者成功的向服务器提交恶意的SQL查询代码**，程序在接收后错误的将攻击者的输入作为查询语句的一部分执行，导致原始的查询逻辑被改变，额外的执行了攻击者精心构造的恶意代码。

举例：`' OR '1'='1`

这是最常见的 SQL注入攻击，当我们输如用户名 admin ，然后密码输如`' OR '1'=1='1`的时候，我们在查询用户名和密码是否正确的时候，本来要执行的是`SELECT * FROM user WHERE username='' and password=''`,经过参数拼接后，会执行 SQL语句 `SELECT * FROM user WHERE username='' and password='' OR '1'='1'`，这个时候1=1是成立，自然就跳过验证了。

#### 如何预防SQL注入

- 在Java中，我们可以使用预编译语句(PreparedStatement)，这样的话即使我们使用 SQL语句伪造成参数，到了服务端的时候，这个伪造 SQL语句的参数也只是简单的字符，并不能起到攻击的作用。

- 对进入数据库的特殊字符（`'"\尖括号&*`;等）进行转义处理，或编码转换。

- 在应用发布之前建议使用专业的SQL注入检测工具进行检测，以及时修补被发现的SQL注入漏洞。网上有很多这方面的开源工具，例如sqlmap、SQLninja等。

- 避免网站打印出SQL错误信息，比如类型错误、字段不匹配等，把代码里的SQL语句暴露出来，以防止攻击者利用这些错误信息进行SQL注入。

- **使用#{}而不是 ${}**

  在MyBatis中,使用`#{}`而不是`${}`，可以很大程度防止sql注入。

  - 因为`#{}`是一个参数占位符，对于字符串类型，会自动加上""，其他类型不加。由于Mybatis采用**预编译**，其后的参数不会再进行SQL编译，所以一定程度上防止SQL注入。
  - `${}`是一个简单的字符串替换，字符串是什么，就会解析成什么，存在SQL注入风险

- **不要暴露一些不必要的日志或者安全信息，比如避免直接响应一些sql异常信息。**如果SQL发生异常了，不要把这些信息暴露响应给用户，可以自定义异常进行响应

- **不相信任何外部输入参数，过滤参数中含有的一些数据库关键词关键词**

  可以加个参数校验过滤的方法，过滤`union，or`等数据库关键词

- **适当的权限控制**

  在你查询信息时，先校验下当前用户是否有这个权限。比如说，实现代码的时候，可以让用户多传一个企业Id什么的，或者获取当前用户的session信息等，在查询前，先校验一下当前用户是否是这个企业下的等等，是的话才有这个查询员工的权限。

### DoS

**拒绝服务攻击**（Denial-of Service （ DoS） attack）

- 弱点攻击。这涉及向一台目标主机上运行的易受攻击的应用程序或操作系统**发送制作精细的报文**。如果适当顺序的多个分组发送给一个易受攻击的应用程序或操作系统，该服务器可能停止运行，或者更糟糕的是主机可能崩溃。 

- 带宽洪泛。攻击者向目标主机**发送大量的分组**，分组数量之多使得目标的接入链路变得拥塞，**使得合法的分组无法到达服务器**。 

- 连接洪泛。攻击者在目标主机中**创建大量的半开或全开TCP连接**。该主机因这些伪造的连接而陷入困境，并停止接受合法的连接

#### 什么是DDOS

DDOS：分布式拒绝服务攻击（Distributed Denial of Service），简单说就是发送大量请求是使服务器瘫痪。DDos攻击是在DOS攻击基础上的，可以通俗理解，dos是单挑，而ddos是群殴，因为现代技术的发展，dos攻击的杀伤力降低，所以出现了DDOS，攻击者借助公共网络，将大数量的计算机设备联合起来，向一个或多个目标进行攻击。

在技术角度上，DDoS攻击可以针对网络通讯协议的各层，手段大致有：**TCP类的SYN Flood、ACK Flood**，UDP类的Fraggle、Trinoo，DNS Query Flood，ICMP Flood，Slowloris类等等。一般会根据攻击目标的情况，针对性的把技术手法混合，以达到最低的成本最难防御的目的，并且可以进行合理的节奏控制，以及隐藏保护攻击资源。

下面介绍一下TCP协议中的SYN攻击。

#### SYN攻击

在三次握手过程中，服务器发送 `SYN-ACK` 之后，收到客户端的 `ACK` 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 `SYN_RCVD` 状态。当收到 ACK 后，服务器才能转入 `ESTABLISHED`状态.

`SYN`攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送`SYN`包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的`SYN`包将长时间占用未连接队列，正常的`SYN`请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。

#### 预防SYN攻击

##### SYN Cookie

[参考](https://zh.m.wikipedia.org/zh-hans/SYN_cookie)

此策略要求服务器创建 Cookie。为避免在填充积压工作时断开连接，服务器使用 SYN-ACK 数据包响应每一项连接请求，而后从积压工作中删除 SYN 请求，同时从内存中删除请求，保证端口保持打开状态并做好重新建立连接的准备。如果连接是合法请求并且已将最后一个 ACK 数据包从客户端机器发回服务器，服务器将重建（存在一些限制）SYN 积压工作队列条目。虽然这项缓解措施势必会丢失一些 TCP 连接信息，但好过因此导致对合法用户发起拒绝服务攻击。

发起一个 TCP 连接时，客户端将一个 TCP SYN 包发送给服务器。作为响应，服务器将 TCP SYN + ACK 包返回给客户端。**此数据包中有一个[序号](https://zh.m.wikipedia.org/wiki/传输控制协议#.E5.BA.8F.E5.88.97.E5.8F.B7.E5.92.8C.E7.A1.AE.E8.AE.A4)（sequence number，TCP头中的第二个32 bit），它被 TCP 用来重新组装数据流**。根据 TCP 规范，由端点发送的第一个序号可以是由该端点决定的任何值。

根据 TCP 规范，**当客户端发回 TCP ACK 包给服务器以响应服务器的 SYN + ACK 包时，客户端必须使用由服务器发送的初始序号加1作为数据包中的确认号。服务器接着从确认号中减去 1 以便还原向客户端发送的原始 SYN Cookie。**

分组嗅探器(packet sniffer) 

IP哄骗（IP spoofing）

### DNS 劫持

[参考](https://xxxixxxx.github.io/2021/02/12/1000-009DNS%20%E6%9F%A5%E8%AF%A2%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9FDNS%20%E5%8A%AB%E6%8C%81%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/)

**在 DNS 的解析中返回了一个假的 IP 地址给客户端**，从而促使用户访问了错误的网站。

#### DNS 劫持的方法

**1. 本机 DNS 劫持**

攻击者通过某些手段使用户的计算机感染上木马病毒，或者恶意软件之后，恶意修改本地 DNS 配置，比如修改本地 hosts 文件，缓存等

**2. 路由 DNS 劫持**

很多用户默认路由器的默认密码，攻击者可以侵入到路由管理员账号中，修改路由器的默认配置

**3.攻击 DNS 服务器**

直接攻击 DNS 服务器，例如对 DNS 服务器进行 DDOS 攻击，可以是 DNS 服务器宕机，出现异常请求，还可以利用某些手段感染 dns 服务器的缓存，使给用户返回来的是恶意的 ip 地址

### CSRF攻击

**思路:** 这道题考察的知识点是**CSRF攻击**，它是属于网络安全这块的知识点，还有**Xss攻击、SQL注入、DDoS**等这些常见的网络攻击，我们都需要知道攻击的流程哈。

**什么是CSRF 攻击？**

> ★
>
> CSRF，**跨站请求伪造**（英文全称是Cross-site request forgery），是一种**挟制用户在当前已登录的Web应用程序上执行非本意的操作的攻击方法**。
>
> ”

**CSRF是如何攻击的呢？**

来看一个来自百度百科的例子哈：

![img](http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-33e6252a833c8a51353bbbff2422a993.png)



1. Tom 登陆银行，没有退出，浏览器包含了Tom在银行的身份认证信息。
2. 黑客Jerry将伪造的转账请求，包含在在帖子
3. Tom在银行网站保持登陆的情况下，浏览帖子
4. 将伪造的转账请求连同身份认证信息，发送到银行网站
5. 银行网站看到身份认证信息，以为就是Tom的合法操作，最后造成Tom资金损失。

**怎么解决CSRF攻击呢？**

- 检查Referer字段（先前网页的地址，当前请求网页紧随其后,即来路）。
- 添加校验token。

### ARP 欺骗



## Linux 系统是如何收发网络包的

### 网络模型

为了使得多种设备能通过网络相互通信，和为了解决各种不同设备在网络互联中的兼容性问题，国际标准化组织制定了开放式系统互联通信参考模型（*Open System Interconnection Reference Model*），也就是 OSI 网络模型，该模型主要有 7 层，分别是应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层。

每一层负责的职能都不同，如下：

- 应用层，负责给应用程序提供统一的接口；
- 表示层，负责把数据转换成兼容另一个系统能识别的格式；
- 会话层，负责建立、管理和终止表示层实体之间的通信会话；
- 传输层，负责端到端的数据传输；
- 网络层，负责数据的路由、转发、分片；
- 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；
- 物理层，负责在物理网络中传输数据帧；

由于 OSI 模型实在太复杂，提出的也只是概念理论上的分层，并没有提供具体的实现方案。

事实上，我们比较常见，也比较实用的是四层模型，即 TCP/IP 网络模型，Linux 系统正是按照这套网络模型来实现网络协议栈的。

TCP/IP 网络模型共有 4 层，分别是应用层、传输层、网络层和网络接口层，每一层负责的职能如下：

- 应用层，负责向用户提供一组应用程序，比如 HTTP、DNS、FTP 等;
- 传输层，负责端到端的通信，比如 TCP、UDP 等；
- 网络层，负责网络包的封装、分片、路由、转发，比如 IP、ICMP 等；
- 网络接口层，负责网络包在物理网络中的传输，比如网络包的封帧、 MAC 寻址、差错检测，以及通过网卡传输网络帧等；

TCP/IP 网络模型相比 OSI 网络模型简化了不少，也更加易记，它们之间的关系如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/OSI%E4%B8%8ETCP.png)

不过，我们常说的七层和四层负载均衡，是用 OSI 网络模型来描述的，七层对应的是应用层，四层对应的是传输层。

------

### Linux 网络协议栈

我们可以把自己的身体比作应用层中的数据，打底衣服比作传输层中的 TCP 头，外套比作网络层中 IP 头，帽子和鞋子分别比作网络接口层的帧头和帧尾。

在冬天这个季节，当我们要从家里出去玩的时候，自然要先穿个打底衣服，再套上保暖外套，最后穿上帽子和鞋子才出门，这个过程就好像我们把 TCP 协议通信的网络包发出去的时候，会把应用层的数据按照网络协议栈层层封装和处理。

你从下面这张图可以看到，应用层数据在每一层的封装格式。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E5%B0%81%E8%A3%85.png)

其中：

- 传输层，给应用数据前面增加了 TCP 头；
- 网络层，给 TCP 数据包前面增加了 IP 头；
- 网络接口层，给 IP 数据包前后分别增加了帧头和帧尾；

这些新增的头部和尾部，都有各自的作用，也都是按照特定的协议格式填充，这每一层都增加了各自的协议头，那自然网络包的大小就增大了，但物理链路并不能传输任意大小的数据包，所以在以太网中，规定了最大传输单元（MTU）是 `1500` 字节，也就是规定了单次传输的最大 IP 包大小。

当网络包超过 MTU 的大小，就会在网络层分片，以确保分片后的 IP 包不会超过 MTU 大小，如果 MTU 越小，需要的分包就越多，那么网络吞吐能力就越差，相反的，如果 MTU 越大，需要的分包就越少，那么网络吞吐能力就越好。

知道了 TCP/IP 网络模型，以及网络包的封装原理后，那么 Linux 网络协议栈的样子，你想必猜到了大概，它其实就类似于 TCP/IP 的四层结构：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E5%8D%8F%E8%AE%AE%E6%A0%88.png" alt="img" style="zoom:50%;" />

从上图的的网络协议栈，你可以看到：

- **应用程序需要通过系统调用，来跟 Socket 层进行数据交互**；
- Socket 层的下面就是传输层、网络层和网络接口层；
- 最下面的一层，则是网卡驱动程序和硬件网卡设备；

### Linux 接收网络包的流程

网卡是计算机里的一个硬件，专门负责接收和发送网络包，当网卡接收到一个网络包后，会通过 DMA 技术，将网络包写入到指定的内存地址，也就是写入到 Ring Buffer ，这个是一个环形缓冲区，接着就会告诉操作系统这个网络包已经到达。

> 那应该怎么告诉操作系统这个网络包已经到达了呢？

最简单的一种方式就是触发中断，也就是每当网卡收到一个网络包，就触发一个中断告诉操作系统。

但是，这存在一个问题，在高性能网络场景下，网络包的数量会非常多，那么就会触发非常多的中断，要知道当 CPU 收到了中断，就会停下手里的事情，而去处理这些网络包，处理完毕后，才会回去继续其他事情，那么频繁地触发中断，则会导致 CPU 一直没完没了的处理中断，而导致其他任务可能无法继续前进，从而影响系统的整体效率。

所以为了解决频繁中断带来的性能开销，Linux 内核在 2.6 版本中引入了 **NAPI 机制**，它是混合「中断和轮询」的方式来接收网络包，它的核心概念就是**不采用中断的方式读取数据**，而是**首先采用中断唤醒数据接收的服务程序，然后 `poll` 的方法来轮询数据**。

因此，当有网络包到达时，会通过 DMA 技术，将网络包写入到指定的内存地址，接着网卡向 CPU 发起硬件中断，当 CPU 收到硬件中断请求后，根据中断表，调用已经注册的中断处理函数。

硬件中断处理函数会做如下的事情：

- 需要先「暂时屏蔽中断」，表示已经知道内存中有数据了，告诉网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停的被中断。
- 接着，发起「软中断」，然后恢复刚才屏蔽的中断。

至此，硬件中断处理函数的工作就已经完成。

硬件中断处理函数做的事情很少，主要耗时的工作都交给软中断处理函数了。

> 软中断的处理

内核中的 ksoftirqd 线程专门负责软中断的处理，当 ksoftirqd 内核线程收到软中断后，就会来轮询处理数据。

ksoftirqd 线程会从 Ring Buffer 中获取一个数据帧，用 sk_buff 表示，从而可以作为一个网络包交给网络协议栈进行逐层处理。

> 网络协议栈

首先，会先进入到网络接口层，在这一层会检查报文的合法性，如果不合法则丢弃，合法则会找出该网络包的上层协议的类型，比如是 IPv4，还是 IPv6，接着再去掉帧头和帧尾，然后交给网络层。

到了网络层，则取出 IP 包，判断网络包下一步的走向，比如是交给上层处理还是转发出去。当确认这个网络包要发送给本机后，就会从 IP 头里看看上一层协议的类型是 TCP 还是 UDP，接着去掉 IP 头，然后交给传输层。

传输层取出 TCP 头或 UDP 头，根据四元组「源 IP、源端口、目的 IP、目的端口」 作为标识，找出对应的 Socket，并把数据放到 Socket 的接收缓冲区。

最后，应用层程序调用 Socket 接口，将内核的 Socket 接收缓冲区的数据「拷贝」到应用层的缓冲区，然后唤醒用户进程。

至此，一个网络包的接收过程就已经结束了，你也可以从下图左边部分看到网络包接收的流程，右边部分刚好反过来，它是网络包发送的流程。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%B5%AE%E7%82%B9/%E6%94%B6%E5%8F%91%E6%B5%81%E7%A8%8B.png" alt="img" style="zoom: 33%;" />

### Linux 发送网络包的流程

如上图的右半部分，发送网络包的流程正好和接收流程相反。

首先，应用程序会调用 Socket 发送数据包的接口，由于这个是系统调用，所以会从用户态陷入到内核态中的 Socket 层，内核会申请一个内核态的 sk_buff 内存，**将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区**。

接下来，网络协议栈从 Socket 发送缓冲区中取出 sk_buff，并按照 TCP/IP 协议栈从上到下逐层处理。

如果使用的是 TCP 传输协议发送数据，那么**先拷贝一个新的 sk_buff 副本** ，这是因为 sk_buff 后续在调用网络层，最后到达网卡发送完成的时候，这个 sk_buff 会被释放掉。而 TCP 协议是支持丢失重传的，在收到对方的 ACK 之前，这个 sk_buff 不能被删除。所以内核的做法就是每次调用网卡发送的时候，实际上传递出去的是 sk_buff 的一个拷贝，等收到 ACK 再真正删除。

接着，对 sk_buff 填充 TCP 头。这里提一下，**sk_buff 可以表示各个层的数据包**，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。

你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。

于是，为了在层级之间传递数据时，不发生拷贝，**只用 sk_buff 一个结构体来描述所有的网络包**，那它是如何做到的呢？是通过调整 sk_buff 中 `data` 的指针，比如：

- 当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb->data 的值，来逐步剥离协议首部。
- 当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb->data 的值来增加协议首部。

你可以从下面这张图看到，当发送报文时，data 指针的移动过程。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/sk_buff.jpg)

至此，传输层的工作也就都完成了。

然后交给网络层，在网络层里会做这些工作：选取路由（确认下一跳的 IP）、填充 IP 头、netfilter 过滤、对超过 MTU 大小的数据包进行分片。处理完这些工作后会交给网络接口层处理。

网络接口层会通过 ARP 协议获得下一跳的 MAC 地址，然后对 sk_buff 填充帧头和帧尾，接着将 sk_buff 放到网卡的发送队列中。

这一些工作准备好后，会触发「软中断」告诉网卡驱动程序，这里有新的网络包需要发送，驱动程序会从发送队列中读取 sk_buff，将这个 sk_buff 挂到 RingBuffer 中，接着将 sk_buff 数据映射到网卡可访问的内存 DMA 区域，最后触发真实的发送。

当数据发送完成以后，其实工作并没有结束，因为内存还没有清理。当发送完成的时候，网卡设备会触发一个硬中断来释放内存，主要是释放 sk_buff 内存和清理 RingBuffer 内存。

最后，**当收到这个 TCP 报文的 ACK 应答时，传输层就会释放原始的 sk_buf**f 。

> 发送网络数据的时候，涉及几次内存拷贝操作？

第一次，调用发送数据的系统调用的时候，内核会申请一个内核态的 sk_buff 内存，将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区。

第二次，在使用 TCP 传输协议的情况下，从传输层进入网络层的时候，每一个 sk_buff 都会被克隆一个新的副本出来。副本 sk_buff 会被送往网络层，等它发送完的时候就会释放掉，然后原始的 sk_buff 还保留在传输层，目的是为了实现 TCP 的可靠传输，等收到这个数据包的 ACK 时，才会释放原始的 sk_buff 。

第三次，当 IP 层发现 sk_buff 大于 MTU 时才需要进行。会再申请额外的 sk_buff，并将原来的 sk_buff 拷贝为多个小的 sk_buff。

### 总结

电脑与电脑之间通常都是通过话网卡、交换机、路由器等网络设备连接到一起，那由于网络设备的异构性，国际标准化组织定义了一个七层的 OSI 网络模型，但是这个模型由于比较复杂，实际应用中并没有采用，而是采用了更为简化的 TCP/IP 模型，Linux 网络协议栈就是按照了该模型来实现的。

TCP/IP 模型主要分为应用层、传输层、网络层、网络接口层四层，每一层负责的职责都不同，这也是 Linux 网络协议栈主要构成部分。

当应用程序通过 Socket 接口发送数据包，数据包会被网络协议栈从上到下进行逐层处理后，才会被送到网卡队列中，随后由网卡将网络包发送出去。

而在接收网络包时，同样也要先经过网络协议栈从下到上的逐层处理，最后才会被送到应用程序。

### Linux IO

#### IO 多路复用

[参考](https://zhuanlan.zhihu.com/p/530567976)、[参考](https://developer.aliyun.com/article/763247)

select，poll和epoll其实都是操作系统中IO多路复用实现的方法。

##### **select**

**select方法本质其实就是维护了一个文件描述符（fd）数组**，以此为基础，实现IO多路复用的功能。这个fd数组有长度限制，在32位系统中，最大值为1024个，而在64位系统中，最大值为2048个。

select方法被调用，**首先需要将fd_set从用户空间拷贝到内核空间，然后内核用poll机制**（此poll机制非IO多路复用的那个poll方法）**直到有一个fd活跃，或者超时了，方法返回**。

- 如果返回值为-1，表明发生了错误
- 如果返回值为0，表明超时了
- 如果返回值为正数，表明有n个fd准备就绪了

**select方法返回后，需要轮询fd_set，以检查出发生IO事件的fd**。这样一套下来，select方法的缺点就很明显了：

- fd_set在用户空间和内核空间的频繁复制，效率低
- 单个进程可监控的fd数量有限制，无论是1024还是2048，对于很多情景来说都是不够用的。
- 基于轮询来实现，效率低

##### **poll**

poll本质上和select没有区别，依然需要进行数据结构的复制，**依然是基于轮询来实现**，但区别就是，select使用的是fd数组，而**poll则是维护了一个链表**，所以从理论上，poll方法中，单个进程能监听的fd不再有数量限制。但是轮询，复制等select存在的问题，poll依然存在

##### **epoll**

epoll就是对select和poll的改进了。它的核心思想是**基于事件驱动来实现的**，实现起来也并不难，就是**给每个fd注册一个回调函数，当fd对应的设备发生IO事件时，就会调用这个回调函数，将该fd放到一个链表中，然后由客户端从该链表中取出一个个fd，以此达到O（1）的时间复杂度**

epoll操作实际上对应着有三个函数：epoll_create，epoll_ctr，epoll_wait

**epoll_create**

epoll_create相当于在内核中创建一个存放fd的数据结构。在select和poll方法中，内核都没有为fd准备存放其的数据结构，只是简单粗暴地把数组或者链表复制进来；而epoll则不一样，epoll_create会在内核建立一颗专门用来存放fd结点的红黑树，后续如果有新增的fd结点，都会注册到这个epoll红黑树上。

**epoll_ctr**

另一点不一样的是，select和poll会一次性将监听的所有fd都复制到内核中，而epoll不一样，当需要添加一个新的fd时，会调用epoll_ctr，给这个fd注册一个回调函数，然后将该fd结点注册到内核中的红黑树中。当该fd对应的设备活跃时，会调用该fd上的回调函数，将该结点存放在一个就绪链表中。这也解决了在内核空间和用户空间之间进行来回复制的问题。

**epoll_wait**

epoll_wait的做法也很简单，其实直接就是从就绪链表中取结点，这也解决了轮询的问题，时间复杂度变成O(1)

所以综合来说，epoll的优点有：

- 没有最大并发连接的限制，远远比1024或者2048要大。（江湖传言1G的内存上能监听10W个端口）
- 效率变高。epoll是基于事件驱动实现的，不会随着fd数量上升而效率下降
- 减少内存拷贝的次数

**水平触发和边缘触发**

简单理解下

水平触发的意思就是说，**只要条件满足，对应的事件就会一直被触发**。所以如果条件满足了但未进行处理，那么就会一直被通知

边缘触发的意思就是说，**条件满足后，对应的事件只会被触发一次**，无论是否被处理，都只会触发一次。

而对于select和poll来说，其触发都是水平触发。而epoll则有两种模式：·EPOLLLT和EPOLLET

- EPOLLLT（默认状态）：也就是水平触发。在该模式下，只要这个fd还有数据可读，那么epoll_wait函数就会返回该fd
- EPOLLET（高速模式）：也就是边缘触发。在该模式下，当被监控的fd上有可读写事件发生时，epoll_wait会通知程序去读写，若本次读写没有读完所有数据，或者甚至没有进行处理，那么下一次调用epoll_wait时，也不会获取到该fd。这种效率比水平触发的要高，系统中不会充斥着大量程序不感兴趣的fd，不感兴趣直接忽视就行，下次不会再触发

##### **总结**

- **select，poll是基于轮询实现的**，将fd_set从用户空间复制到内核空间，然后让内核空间以poll机制来进行轮询，一旦有其中一个fd对应的设备活跃了，那么就把整个fd_set返回给客户端（复制到用户空间），再由客户端来轮询每个fd的，找出发生了IO事件的fd
- **epoll是基于事件驱动实现的**，加入一个新的fd，会调用epoll_ctr函数为该fd注册一个回调函数，然后将该fd结点注册到内核中的epoll红黑树中，当IO事件发生时，就会调用回调函数，将该fd结点放到就绪链表中，epoll_wait函数实际上就是从这个就绪链表中获取这些fd。
- epoll分为EPOLLLT（**水平触发**，默认状态）和EPOLLET（**边缘触发**，效率高）
- 并不是所有的情况中epoll都是最好的，比如当fd数量比较小的时候，epoll不见得就一定比select和poll好

### Linux 的 IO模型有哪些

[参考](https://github.com/Simin-hub/Learning-Programming/blob/main/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md#linux-%E7%9A%84-io%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B)

五种IO模型包括：**阻塞IO、非阻塞IO、IO多路复用、信号驱动IO、异步IO**。

## 面试题

[参考](https://www.eet-china.com/mp/a68780.html)

### 1. 说说HTTP常用的状态码及其含义？

**思路:** 这道面试题主要考察候选人，是否掌握HTTP状态码这个基础知识点。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-de0ffa968b5a19102cdb73625f149d5f.png)

**不管是不是面试需要，我们都要知道，日常开发中的这几个状态码的含义哈：**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-0953009dec9311039e7eedcd69dd6f07.png)

### 2. HTTP 常用的请求方式，区别和用途？

**思路:** 这道题主要考察候选人，是否掌握**HTTP请求方式**这个基础知识点，我们用得比较多就是**GET和POST**啦。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-24c57f6fb43b3c2766849173af549dee.png)

### 3. 请简单说一下你了解的端口及对应的服务？

![img](http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-b651e46b88d855f852c6a8f0d93fcc08.png)

### 4. 说下计算机网络体系结构

**思路:** 这道题主要考察候选人，**计算机网络体系结构**这个基础知识点。计算机网路体系结构呢，有三层：ISO七层模型、TCP/IP四层模型、五层体系结构。大家可以记住这个图，如下

![img](http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-7943d7dc8a2afb50c58f3467d45fa768.png)计算机网络体系结构

#### 4.1 ISO七层模型

ISO七层模型是国际标准化组织（International Organization for Standardization）制定的一个用于计算机或通信系统间互联的标准体系。

> ★
>
> - 应用层：网络服务与最终用户的一个接口，常见的协议有：**HTTP FTP  SMTP SNMP DNS**.
> - 表示层：数据的表示、安全、压缩。，确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。
> - 会话层：建立、管理、终止会话,对应主机进程，指本地主机与远程主机正在进行的会话.
> - 传输层：定义传输数据的协议端口号，以及流控和差错校验,协议有**TCP UDP**.
> - 网络层：进行逻辑地址寻址，实现不同网络之间的路径选择,协议有**ICMP IGMP IP等**.
> - 数据链路层：在物理层提供比特流服务的基础上，建立相邻结点之间的数据链路。
> - 物理层：建立、维护、断开物理连接。
>
> ”

#### 4.2 TCP/IP 四层模型

> ★
>
> - 应用层：对应于OSI参考模型的（应用层、表示层、会话层）。
> - 传输层: 对应OSI的传输层，为应用层实体提供端到端的通信功能，保证了数据包的顺序传送及数据的完整性。
> - 网际层：对应于OSI参考模型的网络层，主要解决主机到主机的通信问题。
> - 网络接口层：与OSI参考模型的数据链路层、物理层对应。
>
> ”

#### 4.3 五层体系结构

> ★
>
> - 应用层：对应于OSI参考模型的（应用层、表示层、会话层）。
> - 传输层：对应OSI参考模型的的传输层
> - 网络层：对应OSI参考模型的的网络层
> - 数据链路层：对应OSI参考模型的的数据链路层
> - 物理层：对应OSI参考模型的的物理层。
>
> ”

### 两台主机之间的通信

两台主机通信分为：同[网段](https://so.csdn.net/so/search?q=网段&spm=1001.2101.3001.7020)下通信和不同网段下通信

#### 1、同网段下：

![两台主机之间如何进行通信](https://s1.51cto.com/images/blog/201902/27/437dc1561ccfe55382bba923666af2df.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)
**应用层：**主机通过tcp/ip协议中的tcp应用端口进行通信要求，主机通过打开一个会话应用窗口，通过逻辑软件的链接和相互配合（检查[ARP](https://so.csdn.net/so/search?q=ARP&spm=1001.2101.3001.7020)缓存），将数据传到下一层；
**传输层：**进行分段，添加tcp报头（源端口，目的端口），分段为了节省时间、传输占用率和解决错误麻烦率以及解决传输单元限制问题；
**网络层：**给数据添加IP报头（源IP，目的IP）封装成数据包，将封装数据包又传输到下一层；
**数据链路层：**在数据包前加入数据帧头（源物理地址，目的物理地址），后加入校验位封装成数据帧，将数据帧传输到下一层；
**物理层：**将数据帧转换成可以在物流链路进行传输的电子信号并送达交换机，**交换机：**通过数据流里数据帧的物理地址地址找寻目的主机的地址，将数据发送到目的主机；
**目的主机：**通过比对目的物理地址,确认为自己接收数据，则拆去数据帧头，发往[网络层](https://so.csdn.net/so/search?q=网络层&spm=1001.2101.3001.7020)，网络层比对目的IP，相同则拆包发往传输层，传输层再比对目的端口，相同则拆去数据段交给应用程序进行数据组装，完成本次同网段下通信。
![两台主机之间如何进行通信](https://s1.51cto.com/images/blog/201903/01/949ef90d81653a423dcbb798cdaad409.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)

#### 2、不同网段下：

![两台主机之间如何进行通信](https://s1.51cto.com/images/blog/201902/27/3fae235b9d907fee72fccb68a3f7f637.png?x-oss-process=image/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=)
主机在**应用层—网络层—数据链路层—物理层—交换机**时需要像在同网段一样进行传输，当数据帧到达交换机时，需将数据帧传输到三层路由器，通过路由器的找寻目的主机物理地址所处的路由器，在经过路由器找寻所处主机物理地址进行分配，将数据帧根据目的主机物理地址传输，然后进行拆包等一系列的过程传输到目的主机。
![两台主机之间如何进行通信](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/5d7b9c35c2dba0ddd4259f47c1225326.png)

### 路由器和交换机区别

- 因为**路由器**是基于 IP 设计的，俗称**三层**网络设备，**路由器的各个端口都具有 MAC 地址和 IP 地址**；
- 而**交换机**是基于以太网设计的，俗称**二层**网络设备，**交换机的端口不具有 MAC 地址**。



###   HTTP协议格式、请求方法

[参考](https://www.jianshu.com/p/8fe93a14754c)

#### 一、URI结构

`HTTP`使用统一资源标识符（`URI`）来传输数据和建立连接。`URL`（统一资源定位符）是一种特殊种类的`URI`，包含了用于查找的资源的足够的信息，我们一般常用的就是`URL`，而一个完整的`URL`包含下面几部分：

[http://www.fishbay.cn:80/mix/76.html?name=kelvin&password=123456#first](https://link.jianshu.com/?t=http://www.fishbay.cn:80/mix/76.html?name=kelvin&password=123456#first)

1.协议部分

该`URL`的协议部分为`http:`，表示网页用的是`HTTP`协议，后面的`//`为分隔符

2.域名部分

域名是`www.fishbay.cn`，发送请求时，需要向`DNS`服务器解析`IP`。如果为了优化请求，可以直接用`IP`作为域名部分使用

3.端口部分

域名后面的`80`表示端口，和域名之间用`:`分隔，端口不是一个`URL`的必须的部分。如果端口是`80`，也可以省略不写

4.虚拟目录部分

从域名的第一个`/`开始到最后一个`/`为止，是虚拟目录的部分。其中，虚拟目录也不是`URL`必须的部分，本例中的虚拟目录是`/mix/`

5.文件名部分

从域名最后一个`/`开始到`?`为止，是文件名部分；如果没有`?`，则是从域名最后一个`/`开始到`#`为止，是文件名部分；如果没有`?`和`#`，那么就从域名的最后一个`/`从开始到结束，都是文件名部分。本例中的文件名是`76.html`，文件名也不是一个`URL`的必须部分，如果没有文件名，则使用默认文件名

6.锚部分

从`#`开始到最后，都是锚部分。本部分的锚部分是`first`，锚也不是一个`URL`必须的部分

7.参数部分

从`?`开始到`#`为止之间的部分是参数部分，又称为搜索部分、查询部分。本例中的参数是`name=kelvin&password=123456`，如果有多个参数，各个参数之间用`&`作为分隔符。

#### Request

HTTP的请求包括：请求行(request line)、请求头部(header)、空行 和 请求数据 四个部分组成。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1843940-d3214aa6ebf47292.png)

#### response

一般情况下，服务器收到客户端的请求后，就会有一个`HTTP`的响应消息，HTTP响应也由`4`部分组成，分别是：状态行、响应头、空行 和 响应体。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1843940-9161c0c67fb3bad1.jpg)

### 5 如何理解HTTP协议是无状态的

**思路:** 这道题主要考察候选人，是否理解Http协议，它为什么是无状态的呢？如何使它有状态呢？

如何理解无状态这个词呢？

> ★
>
> 当浏览器第一次发送请求给服务器时，服务器响应了；如果同个浏览器发起第二次请求给服务器时，它还是会响应，但是呢，服务器不知道你就是刚才的那个浏览器。简言之，服务器不会去记住你是谁，所以是无状态协议。
>
> ”

可以通过一个生活中的例子，来更好理解并记住它：

**有状态场景：**

- 小红：今天吃啥子？
- 小明：罗非鱼~
- 小红：味道怎么样呀？
- 小明：还不错，好香。

**无状态的场景：**

- 小红：今天吃啥子？
- 小明：罗非鱼~
- 小红：味道怎么样呀？
- 小明：？啊？你说啥？什么鬼？什么味道怎么样？

**Http加了Cookie的话**：

- 小红：今天吃啥子？
- 小明：罗非鱼~
- 小红：你今天吃的罗非鱼，味道怎么样呀？
- 小明：还不错，好香。

### 6.从浏览器地址栏输入url到显示主页的过程

**思路:** 这道题主要考察的知识点是HTTP的请求过程，**DNS解析，TCP三次握手，四次挥手这几个要点**，我们都可以讲下。

1. DNS解析，查找域名对应的IP地址。
2. 与服务器通过三次握手，建立TCP连接
3. 向服务器发送HTTP请求
4. 服务器处理请求，返回网页内容
5. 浏览器解析并渲染页面
6. TCP四次挥手，连接结束

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-99845059382a3c88ea6190ef462d5432.png)

- **根据域名和 DNS 解析到服务器的IP地址 (DNS + CDN)**
- **通过ARP协议获得IP地址对应的物理机器的MAC地址**
- 浏览器对服务器发起 TCP 3 次握手
- 建立 TCP 连接后发起 HTTP 请求报文
- 服务器响应 HTTP 请求，将响应报文返回给浏览器
- 短连接情况下，请求结束则通过 TCP 四次挥手关闭连接，长连接在没有访问服务器的若干时间后，进行连接的关闭
- 浏览器得到响应信息中的 HTML 代码， 并请求 HTML 代码中的资源（如js、css、图片等）
- 浏览器对页面进行渲染并呈现给用户

### 7. 说下HTTP/1.0，1.1，2.0的区别

**思路:** 这道题主要考察的知识点是HTTP几个版本的区别，我们记住**HTTP/1.0**默认是短连接，可以强制开启，HTTP/1.1默认长连接，HTTP/2.0采用**多路复用**就差不多啦。

**HTTP/1.0**

- 默认使用**短连接**，**每次请求都需要建立一个TCP连接**。它可以设置`Connection: keep-alive` 这个字段，强制开启长连接。

**HTTP/1.1**

- **引入了持久连接**，即TCP连接默认不关闭，可以被多个请求复用。
- **分块传输编码**，即服务端每产生一块数据，就发送一块，用”流模式”取代”缓存模式”， 即在请求头中加入 `Transfer-Encoding`字段，这时`Content-Length`为null。[参考](https://blog.csdn.net/qq_14920635/article/details/115512962)
- **管道机制**，即在同一个TCP连接里面，客户端可以同时发送多个请求。

**HTTP/2.0**

- **二进制协议**，1.1版本的头信息是文本（ASCII编码），数据体可以是文本或者二进制；2.0中，**头信息和数据体都是二进制**。
- **完全多路复用**，在一个连接里，客户端和浏览器都可以**同时发送多个请求或回应**，而且不用按照顺序一一对应。
- **报头压缩**，HTTP协议不带有状态，每次请求都必须附上所有信息。Http/2.0引入了头信息压缩机制，使用gzip或compress压缩后再发送。
- **服务端推送**，允许服务器未经请求，主动向客户端发送资源。

### 8.  POST和GET有哪些区别？

[参考](https://segmentfault.com/a/1190000023940344)、[参考](https://learnku.com/articles/25881)

**思路:** 这道题主要考察的知识点是POST和GET的区别，可以从**数据包、编码方式、请求参数、收藏为书签、历史记录、安全性**等几方面去回答哈。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-1d6684f23ca9fad8e14a636e9d2b3d36.png)

### post和put的区别

在http中，put被定义为幂等的方法，post不是幂等的方法。

#### post

用于提交请求，可以更新或者创建资源，是非幂等的

在用户注册时，每次提交都是创建一个用户账号，此时用post

#### put

用于向指定的`url`传送**更新资源**，是**幂等**的

还是用户模块，比如修改密码，虽然提交的还是账户名和密码，但是每次提交都只是更新该用户密码，每次请求都只是覆盖原型的值，此时用put

用post还是put

如果该更新对应的url多次调用的结果一致，用put

如果每次提交相同的内容，最终结果不一致，用post

### 9. 在交互过程中如果数据传送完了，还不想断开连接怎么办，怎么维持？

这个问题记住`keep-alive`就好，也就是说，在HTTP中响应体的**Connection**字段指定为`keep-alive`即可

### 10. HTTP 如何实现长连接？在什么时候会超时？

**思路:** 这道题实际上是考察TCP长连接的知识点，HTTP的长连接实质是指TCP的长连接。至于什么时候超时，我们记住这几个参数如**tcp_keepalive_time**、**tcp_keepalive_probes**就好啦

**什么是HTTP的长连接？**

HTTP分为长连接和短连接，**本质上说的是TCP的长短连接**。TCP连接是一个双向的通道，它是可以保持一段时间不关闭的，因此TCP连接才具有真正的长连接和短连接这一说法哈。

TCP长连接可以复用一个TCP连接，来发起多次的HTTP请求，这样就可以减少资源消耗，比如一次请求HTML，如果是短连接的话，可能还需要请求后续的JS/CSS。

**如何设置长连接？**

通过在头部（请求和响应头）设置**Connection**字段指定为`keep-alive`，HTTP/1.0协议支持，但是是默认关闭的，从HTTP/1.1以后，连接默认都是长连接。

**在什么时候会超时呢？**

> ★
>
> - HTTP一般会有httpd守护进程，里面可以设置**keep-alive timeout**，当tcp连接闲置超过这个时间就会关闭，也可以在HTTP的header里面设置超时时间
> - TCP 的**keep-alive**包含三个参数，支持在系统内核的net.ipv4里面设置；当 TCP 连接之后，闲置了**tcp_keepalive_time**，则会发生侦测包，如果没有收到对方的ACK，那么会每隔 tcp_keepalive_intvl再发一次，直到发送了**tcp_keepalive_probes**，就会丢弃该连接。
>
> ”

```
1. tcp_keepalive_intvl = 15
2. tcp_keepalive_probes = 5
3. tcp_keepalive_time = 1800
```

### 11. HTTP 与 HTTPS 的区别。

**思路:** 这道题实际上考察的知识点是HTTP与HTTPS的区别，这个知识点非常重要，可以**从安全性、数据是否加密、默认端口**等这几个方面去回答哈。其实，当你理解HTTPS的整个流程，就可以很好回答这个问题啦。

**我的答案如下**：

HTTP，即超文本传输协议，是一个基于TCP/IP通信协议来传递明文数据的协议。HTTP会存在这**几个问题**：

- 请求信息是明文传输，容易被窃听截取。
- 没有验证对方身份，存在被冒充的风险
- 数据的完整性未校验，容易被中间人篡改

为了解决Http存在的问题，Https出现啦。

**Https是什么？**

**HTTPS= HTTP+SSL/TLS**，可以理解Https是身披SSL(Secure Socket Layer，安全套接层)的HTTP。

它们主要区别如下：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-9c2878c02a4e5545681e7c7b478e7b8c.png)

### 12 . Https流程是怎样的？

**思路:** 这道题实际上考察的知识点是HTTPS的工作流程，大家需要回答这几个要点，**公私钥、数字证书、加密、对称加密、非对称加密**。

- HTTPS = HTTP + SSL/TLS，也就是用SSL/TLS对数据进行加密和解密，Http进行传输。
- SSL，即Secure Sockets Layer（安全套接层协议），是**网络通信提供安全及数据完整性的一种安全协议**。
- TLS，即Transport Layer Security(安全传输层协议)，它是SSL3.0的后续版本。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-2fc53ca265032a799cb042bef2328eff.png)Https工作流程

1. 客户端发起Https请求，连接到服务器的443端口。
2. 服务器必须要有一套数字证书（证书内容有公钥、证书颁发机构、失效日期等）。
3. 服务器将自己的数字证书发送给客户端（公钥在证书里面，私钥由服务器持有）。
4. 客户端收到数字证书之后，会验证证书的合法性。如果证书验证通过，就会生成一个**随机的对称密钥**，用证书的公钥加密。
5. 客户端将公钥加密后的密钥发送到服务器。
6. 服务器接收到客户端发来的密文密钥之后，用自己之前保留的**私钥对其进行非对称解密**，解密之后就得到客户端的密钥，然后用客户端密钥对返回数据进行对称加密，这样传输的数据都是密文啦。
7. 服务器将加密后的密文返回到客户端。
8. 客户端收到后，用自己的密钥对其进行对称解密，得到服务器返回的数据。

### 13. 说说HTTP的状态码，301和302的区别？

**思路:** 这道题考查的知识点，也是HTTP状态码，302和301都有重定向的含义，但是它们也是有区别的。

- 301：（永久性转移）**请求的网页已被永久移动到新位置**。服务器返回此响应时，会自动将请求者转到新位置。
- 302：（暂时性转移）**服务器目前正从不同位置的网页响应请**求，但请求者应继续使用原有位置来进行以后的请求。此代码与响应GET和HEAD请求的301代码类似，会自动将请求者转到不同的位置。

网上有个很**形象的例子**比喻：

> ★
>
> 当一个网站或者网页24—48小时内临时移动到一个新的位置，这时候就要进行302跳转，打个比方说，我有一套房子，但是最近走亲戚去亲戚家住了，过两天我还回来的。而使用301跳转的场景就是之前的网站因为某种原因需要移除掉，然后要到新的地址访问，是永久性的，就比如你的那套房子其实是租的，现在租期到了，你又在另一个地方找到了房子，之前租的房子不住了。
>
> ”

### 14. 说说什么是数字签名？什么是数字证书？

**思路:** 这道题考查的知识点，不仅仅是数字签名，数字证书，很可能面试官也会问你https的原理的，因为https原理跟数字证书有关的哈，大家需要掌握https原理哦。

**数字证书是指在互联网通讯中标志通讯各方身份信息的一个数字认证**，人们可以在网上用它来识别对方的身份。它的出现，是为了避免身份被篡改冒充的。比如Https的数字证书，就是为了避免公钥被中间人冒充篡改：![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-203531ea8afd45f9e5977a413fdf0d39.png)

**数字证书构成**

- 公钥和个人等信息，**经过Hash摘要算法加密**，形成消息摘要；**将消息摘要拿到拥有公信力的认证中心（CA）**，用它的私钥对消息摘要加密，形成**数字签名**。
- 公钥和个人信息、数字签名共同构成**数字证书**。

### 15. 对称加密与非对称加密有什么区别

**思路:** 这道题考察的知识点是对称加密与非对称加密算法，什么是对称加密，什么是非对称加密呢？

对称加密：**指加密和解密使用同一密钥**，优点是运算速度较快，缺点是如何安全将密钥传输给另一方。常见的对称加密算法有：DES、AES等。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-5160c7c63880378968c9482fbd50b965.png)

非对称加密：**指的是加密和解密使用不同的密钥（即公钥和私钥）**。**公钥与私钥是成对存在的**，如果用公钥对数据进行加密，只有对应的私钥才能解密。常见的非对称加密算法有RSA。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-4c5bb2ec3c707d21fa13bed41beb9675.png)

### 16. 说说DNS的解析过程？

**思路:** 这道题考察的知识点是**DNS域名解析**，http请求的过程，是涉及到DNS域名解析的，这道面试题也挺经典的，大家可以看下《图解HTTP》那本书哈。

> ★
>
> DNS，英文全称是**domain name system**，域名解析系统，是Internet上作为域名和IP相互映射的一个分布式数据库。它的作用很明确，就是可以根据域名查出对应的IP地址。在浏览器缓存、本地DNS服务器、根域名服务器都是怎么查找的，大家回答的时候都可以说下哈。
>
> ”

DNS的解析过程如下图：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-7f57724c1058b99576f01aa357d09e4c.png)

假设你要查询**www.baidu.com**的IP地址:

> ★
>
> - 首先会查找浏览器的缓存,看看是否能找到**www.baidu.com**对应的IP地址，找到就直接返回；否则进行下一步。
> - 将请求发往给本地DNS服务器，如果查找到也直接返回，否则继续进行下一步；
> - 本地DNS服务器向**根域名服务器**发送请求，根域名服务器返回负责`.com`的顶级域名服务器的IP地址的列表。
> - 本地DNS服务器再向其中一个负责`.com`的顶级域名服务器发送一个请求，返回负责`.baidu`的权威域名服务器的IP地址列表。
> - 本地DNS服务器再向其中一个权威域名服务器发送一个请求，返回**www.baidu.com**所对应的IP地址。
>
> ”

**查询顺序**：浏览器缓存–> 操作系统缓存–> 本地 host 文件 –> 路由器缓存–> ISP DNS 缓存 –> 根 DNS 服务器->顶级 DNS 服务器->权限域名 DNS 服务器

#### DNS用的是TCP协议还是UDP协议

[参考](https://www.cnblogs.com/MrLiuZF/p/15203919.html)

DNS占用53号端口，同时使用TCP和UDP协议。那么DNS在什么情况下使用这两种协议？

DNS在**区域传输的时候使用TCP协议**，**其他时候（如域名解析）使用UDP协议**。

DNS区域传输的时候使用TCP协议：

1. **辅域名服务器会定时（一般3小时）向主域名服务器进行查询以便了解数据是否有变动**。如有变动，会执行一次区域传送，进行数据同步。区域传送使用TCP而不是UDP，因为数据同步传送的数据量比一个请求应答的数据量要多得多。

2. TCP是一种可靠连接，保证了数据的准确性。

域名解析时使用UDP协议：

**客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可**。不用经过三次握手，这样DNS服务器负载更低，响应更快。理论上说，客户端也可以指定向DNS服务器查询时用TCP，但事实上，**很多DNS服务器进行配置的时候，仅支持UDP查询**包。

### 什么是 DNS 劫持？

[参考](https://xxxixxxx.github.io/2021/02/12/1000-009DNS%20%E6%9F%A5%E8%AF%A2%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9FDNS%20%E5%8A%AB%E6%8C%81%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/)

**在 DNS 的解析中返回了一个假的 IP 地址给客户端**，从而促使用户访问了错误的网站。

#### DNS 劫持的方法

**1. 本机 DNS 劫持**

攻击者通过某些手段使用户的计算机感染上木马病毒，或者恶意软件之后，恶意修改本地 DNS 配置，比如修改本地 hosts 文件，缓存等

**2. 路由 DNS 劫持**

很多用户默认路由器的默认密码，攻击者可以侵入到路由管理员账号中，修改路由器的默认配置

**3.攻击 DNS 服务器**

直接攻击 DNS 服务器，例如对 DNS 服务器进行 DDOS 攻击，可以是 DNS 服务器宕机，出现异常请求，还可以利用某些手段感染 dns 服务器的缓存，使给用户返回来的是恶意的 ip 地址

### 17. 什么是CSRF攻击，如何避免

**思路:** 这道题考察的知识点是**CSRF攻击**，它是属于网络安全这块的知识点，还有**Xss攻击、SQL注入、DDoS**等这些常见的网络攻击，我们都需要知道攻击的流程哈。

**什么是CSRF 攻击？**

> ★
>
> CSRF，**跨站请求伪造**（英文全称是Cross-site request forgery），是一种**挟制用户在当前已登录的Web应用程序上执行非本意的操作的攻击方法**。
>
> ”

**CSRF是如何攻击的呢？**

来看一个来自百度百科的例子哈：

![img](http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-33e6252a833c8a51353bbbff2422a993.png)



1. Tom 登陆银行，没有退出，浏览器包含了Tom在银行的身份认证信息。
2. 黑客Jerry将伪造的转账请求，包含在在帖子
3. Tom在银行网站保持登陆的情况下，浏览帖子
4. 将伪造的转账请求连同身份认证信息，发送到银行网站
5. 银行网站看到身份认证信息，以为就是Tom的合法操作，最后造成Tom资金损失。

**怎么解决CSRF攻击呢？**

- 检查Referer字段（先前网页的地址，当前请求网页紧随其后,即来路）。
- 添加校验token。

### 18. 聊聊五层计算机网络体系结构中，每一层对应的网络协议有哪些？

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-ba1e2d8a01c7492d49d39cb93378550f.png)

### 19. 说说 WebSocket与socket的区别

**思路:** 这是一个比较基础的知识点，经常有小伙伴会搞混。

- Socket其实就是等于**IP地址 + 端口 + 协议**。

> ★
>
> 具体来说，Socket是一套标准，它完成了对TCP/IP的高度封装，屏蔽网络细节，以方便开发者更好地进行网络编程。
>
> ”

- **WebSocket是一个持久化的协议**，它是伴随H5而出的协议，用来解决**http不支持持久化连接**的问题。
- Socket一个是**网络编程的标准接口**，而WebSocket则是**应用层通信协议**。

### 20. 什么是DoS、DDoS、DRDoS攻击？

**思路:** 这是涉及网络安全的一个知识点，DDos还会挺常见的，如SYN Flood。

> ★
>
> - **DOS**: (Denial of Service),翻译过来就是**拒绝服务**,一切能引起DOS行为的攻击都被称为DOS攻击。最常见的DoS攻击就有**计算机网络宽带攻击**、**连通性攻击**。
> - **DDoS**: (Distributed Denial of Service),翻译过来是**分布式拒绝服务**。是指处于不同位置的多个攻击者同时向一个或几个目标发动攻击，或者一个攻击者控制了位于不同位置的多台机器并利用这些机器对受害者同时实施攻击。常见的DDos有**SYN Flood、Ping of Death、ACK Flood、UDP Flood**等。
> - **DRDoS**: (Distributed Reflection Denial of Service)，中文是**分布式反射拒绝服务**，该方式靠的是**发送大量带有被害者IP地址的数据包给攻击主机**，然后攻击主机对IP地址源做出大量回应，从而形成拒绝服务攻击。
>
> ”

### 21. 什么是XSS攻击，如何避免?

**思路:** XSS攻击也是比较常见，XSS，叫**跨站脚本攻击（Cross-Site Scripting）**，因为会与层叠样式表(Cascading Style Sheets, CSS)的缩写混淆，因此有人将跨站脚本攻击缩写为XSS。它指的是**恶意攻击者往Web页面里插入恶意html代码，当用户浏览该页之时，嵌入其中Web里面的html代码会被执行，从而达到恶意攻击用户的特殊目的**。XSS攻击一般分三种类型：**存储型 、反射型 、DOM型XSS**

#### 21.1 XSS是如何攻击的呢？

拿反射型举个例子吧，流程图如下：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-31e42588913eadf80cb3e3e4e758f522.png)

#### 21.2 如何解决XSS攻击问题？

- 对输入进行过滤，过滤标签等，只允许合法值。
- HTML转义
- 对于链接跳转，如`<a href="xxx"` 等，要校验内容，禁止以script开头的非法链接。
- 限制输入长度

### 22. Http请求的过程与原理

**思路:** HTTP请求，一个非常非常基础的知识点，一定需要掌握的。其实觉得跟浏览器地址栏输入url到显示主页这道题有点类似。

**我的答案如下**：

HTTP是一个基于TCP/IP协议来传递数据的超文本传输协议，传输的数据类型有HTML,图片等。以访问百度有例子，看下一次Http的请求过程吧

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-d58966fe4854169509511d6bfc90de9d.png)Http请求过程

1. 客户端进行DNS域名解析，得到对应的IP地址
2. 根据这个IP，找到对应的服务器建立连接（三次握手）
3. 建立TCP连接后发起HTTP请求（一个完整的http请求报文）
4. 服务器响应HTTP请求，客户端得到html代码
5. 客户端解析html代码，用html代码中的资源(如js,css,图片等等)渲染页面。
6. 服务器关闭TCP连接（四次挥手）

### 23.  forward和redirect的区别？

**思路:** 这道题有点偏Java web方向的。以前记得刚出来实习找工作的时候，面试官可喜欢问这道题啦，当时我记的答案就是，forward是转发，redirect是重定向。

**我的答案如下**：

> ★
>
> - **直接转发方式（Forward）** ，客户端和浏览器只发出一次请求，Servlet、HTML、JSP或其它信息资源，由第二个信息资源响应该请求，在请求对象request中，保存的对象对于每个信息资源是共享的。
> - **间接转发方式（Redirect）** 实际是两次HTTP请求，服务器端在响应第一次请求的时候，让浏览器再向另外一个URL发出请求，从而达到转发的目的。
>
> ”

举个通俗的例子：

> ★
>
> - 直接转发就相当于：“A找B借钱，B说没有，B去找C借，借到借不到都会把消息传递给A”；
> - 间接转发就相当于："A找B借钱，B说没有，让A去找C借"。**
>
> ”

看下这两个图，可以更容易理解一些:

- Redirect 的工作原理：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-cefd503ecf09d013ccca358f11037b83.png)

- forward 的工作原理

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-715a25ba54d8cfa72f52e05bc456f6fc.png)

### 24. 聊聊SQL注入？

**思路:** SQL注入是最经典的安全问题。无论你是前端开发还是后端开发，都必须掌握的。

> ★
>
> SQL注入是一种代码注入技术，一般被应用于攻击web应用程序。它**通过在web应用接口传入一些特殊参数字符，来欺骗应用服务器，执行恶意的SQL命令**，以达到非法获取系统信息的目的。它目前是黑客对数据库进行攻击的最常用手段之一。
>
> ”

#### 24.1 SQL注入是如何攻击的？

举个常见的**业务场景**：在web表单搜索框输入员工名字，然后后台查询出对应名字的员工。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-7fd7ced8659cbab7f5b89fc5dd808e62.png)

这种场景下，一般都是前端页面,把一个名字参数name传到后台，然后后台通过SQL把结果查询出来

```
name = "田螺"; //前端传过来的

SQL= "select * from staff where name=" + name;  //根据前端传过来的name参数，查询数据库员工表staff
```

因为SQL是直接拼接的，如果我们完全信任前端传的参数的话。假如前端传这么一个参数时`'' or '1'='1'`，SQL就变成酱紫的啦。

```
select * from staff where name='' or '1'='1';
```

这个SQL会把所有的员工信息全都查出来了，酱紫就请求用户已经越权啦。请求者可以获取所有员工的信息，信息已经暴露了啦。

#### 24.2 如何预防SQL注入问题

**1). 使用#{}而不是 ${}**

在MyBatis中,使用`#{}`而不是`${}`，可以很大程度防止sql注入。

- 因为`#{}`是一个参数占位符，对于字符串类型，会自动加上""，其他类型不加。由于Mybatis采用**预编译**，其后的参数不会再进行SQL编译，所以一定程度上防止SQL注入。
- `${}`是一个简单的字符串替换，字符串是什么，就会解析成什么，存在SQL注入风险

**2). 不要暴露一些不必要的日志或者安全信息，比如避免直接响应一些sql异常信息。**

如果SQL发生异常了，不要把这些信息暴露响应给用户，可以自定义异常进行响应

**3). 不相信任何外部输入参数，过滤参数中含有的一些数据库关键词关键词**

可以加个参数校验过滤的方法，过滤`union，or`等数据库关键词

**4). 适当的权限控制**

在你查询信息时，先校验下当前用户是否有这个权限。比如说，实现代码的时候，可以让用户多传一个企业Id什么的，或者获取当前用户的session信息等，在查询前，先校验一下当前用户是否是这个企业下的等等，是的话才有这个查询员工的权限。

### 25. Session和Cookie的区别。

**我们先来看Session和Cookie的概念吧：**

- Cookie是保存在客户端的一小块文本串的数据。客户端向服务器发起请求时，服务端会向客户端发送一个Cookie，客户端就把Cookie保存起来。在客户端下次向同一服务器再发起请求时，Cookie被携带发送到服务器。服务器就是根据这个Cookie来确认身份的。
- session指的就是**服务器和客户端一次会话的过程**。Session利用Cookie进行信息处理的，当用户首先进行了请求后，服务端就在用户浏览器上创建了一个Cookie，当这个Session结束时，其实就是意味着这个Cookie就过期了。Session对象存储着特定用户会话所需的属性及配置信息。

**Session 和Cookie的区别主要有这些：**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-1a7151f7c2bb75d9f9f56a0e5184c1c1.png)

来看个图吧：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-644bd68fcbe1dfa2f637b894aac49579.png)

> ★
>
> - 用户第一次请求服务器时，服务器根据用户提交的信息，创建对应的Session，请求返回时将此Session的唯一标识信息SessionID返回给浏览器，浏览器接收到服务器返回的SessionID信息后，会将此信息存入Cookie中，同时Cookie记录此SessionID是属于哪个域名。
> - 当用户第二次访问服务器时，请求会自动判断此域名下是否存在Cookie信息，如果存在，则自动将Cookie信息也发送给服务端，服务端会从Cookie中获取SessionID，再根据 SessionID查找对应的 Session信息，如果没有找到，说明用户没有登录或者登录失效，如果找到Session证明用户已经登录可执行后面操作。
>
> ”

### 26. IP地址有哪些分类？

一般可以这么认为，IP地址=网络号+主机号。

1. 网络号：它标志主机所连接的网络地址表示属于互联网的哪一个网络。
2. 主机号：它标志主机地址表示其属于该网络中的哪一台主机。

IP地址分为A，B，C，D，E五大类：

- A类地址(1~126)：**以0开头，网络号占前8位**，主机号占后面24位。
- B类地址(128~191)：**以10开头，网络号占前16位**，主机号占后面16位。
- C类地址(192~223)：**以110开头，网络号占前24位**，主机号占后面8位。
- D类地址(224~239)：以1110开头，保留位多播地址。
- E类地址(240~255)：以11110开头，保留位为将来使用

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-faa187d21caaf4e7283eb57118ab21dc.png)

### **IP 无类别域间路由**（Classless Inter-Domain Routing、**CIDR**）

**IP地址的网络部分称为网络地址，网络地址用于唯一地标识一个网段**，或者若干网段的聚合，同一网段中的网络设备有同样的网络地址。**IP地址的主机部分称为主机地址，主机地址用于唯一的标识同一网段内的网络设备**。例如，前面所述的A类IP地址：10.110.192.111，网络部分地址为10，主机部分地址为110.192.111。

**无类别域间路由**（Classless Inter-Domain Routing、**CIDR**）是一个用于给用户分配[IP地址](https://zh.m.wikipedia.org/wiki/IP地址)以及在[互联网](https://zh.m.wikipedia.org/wiki/互联网)上有效地路由IP[数据包](https://zh.m.wikipedia.org/wiki/数据包)的对IP地址进行归类的方法。

在[域名系统](https://zh.m.wikipedia.org/wiki/域名系统)出现之后的第一个十年里，基于[分类网络](https://zh.m.wikipedia.org/wiki/分类网络)进行地址分配和路由IP数据包的设计就已明显显得[可扩充性](https://zh.m.wikipedia.org/wiki/可扩放性)不足（参见RFC 1517）。为了解决这个问题，[互联网工程工作小组](https://zh.m.wikipedia.org/wiki/互联网工程工作小组)在1993年发布了一新系列的标准——RFC 1518和RFC 1519——以定义新的分配IP地址块和路由[IPv4](https://zh.m.wikipedia.org/wiki/IPv4)数据包的方法。

一个IP地址包含两部分：标识网络的**前缀**和紧接着的在这个网络内的**主机地址**。在之前的[分类网络](https://zh.m.wikipedia.org/wiki/分类网络)中，IP地址的分配把IP地址的32位按每8位为一段分开。这使得**前缀必须为8，16或者24位**。因此，可分配的最小的地址块有256（24位前缀，8位主机地址，$2^{8}=256$)个地址，而这对大多数企业来说太少了。大一点的地址块包含65536（16位前缀，16位主机，$2^{16}=65536$）个地址，而这对大公司来说都太多了。这导致不能充分使用IP地址和在路由上的不便，因为大量的需要单独路由的小型网络（C类网络）因在地域上分得很开而很难进行[聚合路由](https://zh.m.wikipedia.org/w/index.php?title=聚合路由&action=edit&redlink=1)，于是给路由设备增加了很多负担。

无类别域间路由是基于**可变长子网掩码（VLSM）**来进行任意长度的前缀的分配的。在RFC 950（1985）中有关于可变长子网掩码的说明。CIDR包括：

- 指定任意长度的前缀的可变长子网掩码技术。遵从CIDR规则的地址有一个后缀说明前缀的位数，例如：192.168.0.0/16。这使得对日益缺乏的IPv4地址的使用更加有效。
- 将多个连续的前缀聚合成**[超网](https://zh.m.wikipedia.org/w/index.php?title=超网&action=edit&redlink=1)**，以及，在互联网中，只要有可能，就显示为一个聚合的网络，因此在总体上可以减少路由表的表项数目。聚合使得互联网的路由表不用分为多级，并通过VLSM逆转“划分子网”的过程。
- 根据机构的实际需要和短期预期需要而不是分类网络中所限定的过大或过小的地址块来管理IP地址的分配的过程。

CIDR表示方法：IP地址/网络ID的位数，比如192.168.23.35/21，其中用21位表示网络ID。

例1：192.168.23.35/21，子网掩码：11111111 11111111 11111000 00000000则为255.255.248.0

计算步骤：

1. 根据网络ID的位数，计算单个子网的IP数，21位网络ID，在第三个字节，借出三位给网络号
2. 子网掩码与IP地址相与得到网络地址，192.168.16.0， 或者通过主机地址全部置为0，就得到了网络地址
3. 起始IP地址：192.168.16.1（主机ID不能全为0，全为0表示网络ID最后一位为1）
4. 结束IP地址：192.168.00010111.11111110（主机ID不能全为1，全为1表示本地广播）则结束IP地址为：192.168.23.254。

[CIDR 练习题](https://blog.csdn.net/weixin_43095238/article/details/112572107#_CIDR__93)

#### IP地址计算方式

##### 1、利用子网数目计算子网掩码

把B类地址172.16.0.0划分成30个子网络，它的子网掩码是多少？

①将子网络数目30转换成二进制表示11110

②统计一下这个二进制的数共有5位

③注意：当二进制数中只有一个1的时候，所统计的位数需要减1（例如：10000要统计为4位）

④将B类地址的子网掩码255.255.0.0主机地址部分的前5位变成1

⑤这就得到了所要的子网掩码（11111111.11111111.11111000.00000000）255.255.248.0。



##### 2、利用主机数目计算子网掩码

把B类地址172.16.0.0划分成若干子网络，每个子网络能容纳500台主机，它的子网掩码是多少？

①把500转换成二进制表示111110100

②统计一下这个二进制的数共有9位

③将子网掩码255.255.255.255从后向前的9位变成0

④这就得到了所要的子网掩码（11111111.11111111.11111110.00000000）255.255.254.0。

 

##### 3、利用子网掩码计算最大有效子网数

A类IP地址，子网掩码为255.224.0.0，它所能划分的最大有效子网数是多少？

①将子网掩码转换成二进制表示11111111.11100000.00000000.00000000

②统计一下它的网络位共有11位

③A类地址网络位的基础数是8，二者之间的位数差是3

④最大有效子网数就是2的3次方，即最多可以划分8个子网络。

 

##### 4、利用子网掩码计算最大可用主机数

A类IP地址，子网掩码为255.252.0.0，将它划分成若干子网络，每个子网络中可用主机数有多少？

①将子网掩码转换成二进制表示11111111.11111100.00000000.00000000

②统计一下它的主机位共有18位

③最大可用主机数就是2的18次方减2（除去全是0的网络地址和全是1广播地址），即每个子网络最多有262142台主机可用。

 

##### 5、利用子网掩码确定子网络的起止地址

B类IP地址172.16.0.0，子网掩码为255.255.192.0，它所能划分的子网络起止地址是多少？

①利用子网掩码计算，最多可以划分4个子网络

②利用子网掩码计算，每个子网络可容纳16384台主机（包括网络地址和广播地址）

③用16384除以256（网段内包括网络地址和广播地址的全部主机数），结果是64

④具体划分网络起止方法如下：

172.16.0.0～172.16.63.255

172.16.64.0～172.16.127.255

172.16.128.0～172.16.191.255

172.16.192.0～172.16.255.255

##### 6、计算子网数

假设网络地址为192.168.35.0，子网掩码为/28，则这个网络中可用的子网数和主机数分别是

通过网络地址可知，这是c类地址，所以网络号前有24位，又因为掩码28位，所以子网4位，主机4位，扣掉全1全0就是14位

### 27. 说下ARP 协议的工作过程？

[参考](https://zh.wikipedia.org/wiki/%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%8D%8F%E8%AE%AE)、[参考](https://info.support.huawei.com/info-finder/encyclopedia/zh/ARP.html)

ARP 协议协议，**Address Resolution Protocol**，**地址解析协议**，它是用于实现IP地址到MAC地址的映射。

> ★
>
> 1. 首先，每台主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址的对应关系。
> 2. 当源主机需要将一个数据包要发送到目的主机时，会首先检查自己的ARP列表，是否存在该IP地址对应的MAC地址；如果有﹐就直接将数据包发送到这个MAC地址；如果没有，就向**本地网段发起一个ARP请求的广播包**，查询此目的主机对应的MAC地址。此ARP请求的数据包里，包括源主机的IP地址、硬件地址、以及目的主机的IP地址。
> 3. 网络中所有的主机收到这个ARP请求后，会检查数据包中的目的IP是否和自己的IP地址一致。如果不相同，就会忽略此数据包；如果相同，该主机首先将发送端的MAC地址和IP地址添加到自己的ARP列表中，如果ARP表中已经存在该IP的信息，则将其覆盖，然后给源主机发送一个 ARP响应数据包，告诉对方自己是它需要查找的MAC地址。
> 4. 源主机收到这个ARP响应数据包后，将得到的目的主机的IP地址和MAC地址添加到自己的ARP列表中，并利用此信息开始数据的传输。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。
>
> ”

### 28. 有了IP地址，为什么还要用MAC地址？

> ★
>
> - 简而言之，标识网络中的一台计算机，比较常用的就是**IP地址和MAC地址**，但计算机的IP地址可由用户自行更改，管理起来就相对困难，而MAC地址不可更改，所以一般会把IP地址和MAC地址组合起来使用。
> - 那只使用MAC地址不用IP地址行不行呢？不行的！因为最早就是MAC地址先出现的，并且当时并不用IP地址，只用MAC地址，后来随着网络中的设备越来越多，整个路由过程越来越复杂，便出现了子网的概念。对于目的地址在其他子网的数据包，路由只需要将数据包送到那个子网即可。
> - 那为什么要用IP地址呢？是因为IP地址是和地域相关的，对于同一个子网上的设备，IP地址的前缀都是一样的，这样路由器通过IP地址的前缀就知道设备在在哪个子网上了，而只用MAC地址的话，路由器则需要记住每个MAC地址在哪个子网，这需要路由器有极大的存储空间，是无法实现的。
> - I**P地址可以比作为地址，MAC地址为收件人**，在一次通信过程中，两者是缺一不可的。
>
> ”

### 29. TCP 和 UDP 分别对应的常见应用层协议有哪些？

**基于TCP的应用层协议有：HTTP、FTP、SMTP、TELNET、SSH**

- **HTTP**：HyperText Transfer Protocol（超文本传输协议），默认端口80
- **FTP**: File Transfer Protocol (文件传输协议), 默认端口(20用于传输数据，21用于传输控制信息)
- **SMTP**: Simple Mail Transfer Protocol (简单邮件传输协议) ,默认端口25
- **TELNET**: Teletype over the Network (网络电传), 默认端口23
- **SSH**：Secure Shell（安全外壳协议），默认端口 22

**基于UDP的应用层协议：DNS、TFTP、SNMP**

- **DNS** : Domain Name Service (域名服务),默认端口 53
- **TFTP**: Trivial File Transfer Protocol (简单文件传输协议)，默认端口69
- **SNMP**：Simple Network Management Protocol（简单网络管理协议），通过UDP端口161接收，只有Trap信息采用UDP端口162。

### 32. URI和URL的区别

- URI，全称是Uniform Resource Identifier)，中文翻译是**统一资源标志符**，主要作用是唯一标识一个资源。
- URL，全称是Uniform Resource Location)，中文翻译是**统一资源定位符**，主要作用是提供资源的路径。打个经典比喻吧，URI像是身份证，可以唯一标识一个人，而URL更像一个住址，可以通过URL找到这个人。

### 33. ICMP协议的功能

[参考](https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E6%8E%A7%E5%88%B6%E6%B6%88%E6%81%AF%E5%8D%8F%E8%AE%AE)、[参考](https://zhuanlan.zhihu.com/p/369623317)

[ICMP](https://zhuanlan.zhihu.com/p/369623317), (Internet Control Message Protocol) ,Internet控制消息协议。

- ICMP协议是一种**面向无连接**的协议，**用于传输出错报告控制信息**。
- 它是一个非常重要的协议，它对于网络安全具有极其重要的意义。它属于**网络层协议**，主要用于在主机与路由器之间传递控制信息，包括**报告错误、交换受限控制和状态信息**等。
- 当遇到IP数据无法访问目标、IP路由器无法按当前的传输速率转发数据包等情况时，会自动发送ICMP消息。

比如我们日常使用得比较多的**ping**，就是基于ICMP的。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/v2-ef5b42aff92acc5a8873d946d48177e4_r.jpg)

### 35. 说下ping的原理

> ★
>
> ping，**Packet Internet Groper**，是一种因特网包探索器，用于测试网络连接量的程序。Ping是工作在TCP/IP网络体系结构中应用层的一个服务命令， 主要是**向特定的目的主机发送ICMP**（Internet Control Message Protocol 因特网报文控制协议） 请求报文，测试目的站是否可达及了解其有关状态
>
> ”

一般来说，ping可以用来检测网络通不通。它是基于`ICMP`协议工作的。假设**机器A** ping**机器B**，工作过程如下：

1. ping通知系统，新建一个固定格式的ICMP请求数据包
2. ICMP协议，将该数据包和目标机器B的IP地址打包，一起转交给IP协议层
3. IP层协议将本机IP地址为源地址，机器B的IP地址为目标地址，加上一些其他的控制信息，构建一个IP数据包
4. 先获取目标机器B的MAC地址。
5. 数据链路层构建一个数据帧，目的地址是IP层传过来的**MAC地址**，源地址是本机的**MAC地址**
6. 机器B收到后，对比目标地址，和自己本机的MAC地址是否一致，符合就处理返回，不符合就丢弃。
7. 根据目的主机返回的ICMP回送回答报文中的时间戳，从而计算出往返时间
8. 最终显示结果有这几项：发送到目的主机的IP地址、发送 & 收到 & 丢失的分组数、往返时间的最小、最大& 平均值

### 36. 请详细介绍一下TCP 的三次握手机制

**思路:** TCP连接的三次握手机制，最重要的知识点，必须得会，通讯过程以及客户端、服务器的对应的状态都需要记住哈。

TCP提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的就是同步连接双方的序列号和确认号并交换TCP窗口大小信息。我们一起来看下流程图哈：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-eaaf3fdfd9c7a2b0390fa7a7973f2218.png)

- 第一次握手(SYN=1, seq=x)，发送完毕后，客户端就进入SYN_SEND状态
- 第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1)， 发送完毕后，服务器端就进入SYN_RCV状态。
- 第三次握手(ACK=1，ACKnum=y+1)，发送完毕后，客户端进入ESTABLISHED状态，当服务器端接收到这个包时，也进入ESTABLISHED状态。

##### 为什么TCP建立一定要三次呢？两次不行吗？

> **原因**：
>
> - 双方要明确对方接收能力都是正常的，（客户端发之后，服务端可以确定客户端发送能力正常，服务端发送给客户端，客户端可以确定服务端的接收和发送能力正常，最后客户端发送确认，来确定客户端的接收能力。
> - **为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误**”。

### 37. TCP握手为什么是三次，为什么不能是两次？不能是四次？

**思路:** TCP握手为什么不能是两次，为什么不能是四次呢？为了方便理解，我们以男孩子和女孩子谈恋爱为例子：两个人能走到一起，最重要的事情就是相爱，就是**我爱你，并且我知道，你也爱我**，接下来我们以此来模拟三次握手的过程：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-f795189dd1254b2b5765b0a617198b9c.png)

**为什么握手不能是两次呢？**

如果只有两次握手，女孩子可能就不知道，她的那句**我也爱你**，男孩子是否**收到**，恋爱关系就不能愉快展开。

**为什么握手不能是四次呢？**

因为握手不能是四次呢？因为三次已经够了，三次已经能让双方都知道：你爱我，我也爱你。而四次就多余了。

### 38. 说说TCP四次挥手过程

**思路:** TCP的四次挥手，也是最重要的知识点，一般跟三次握手会一起考的，必须得记住。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-8d9d9e5c274d6b4cc3deb1767d67d1f8.png)TCP四次挥手过程

1. 第一次挥手(FIN=1，seq=u)，发送完毕后，客户端进入**FIN_WAIT_1**状态。
2. 第二次挥手(ACK=1，ack=u+1,seq =v)，发送完毕后，服务器端进入**CLOSE_WAIT**状态，客户端接收到这个确认包之后，进入**FIN_WAIT_2**状态。
3. 第三次挥手(FIN=1，ACK1,seq=w,ack=u+1)，发送完毕后，服务器端进入**LAST_ACK**状态，等待来自客户端的最后一个ACK。
4. 第四次挥手(ACK=1，seq=u+1,ack=w+1)，客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入TIME_WAIT状态，**等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后**，没有收到服务器端的ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入CLOSED状态。服务器端接收到这个确认包之后，关闭连接，进入CLOSED状态。

### 39. TCP挥手为什么需要四次呢？

**思路：** TCP挥手为什么需要四次呢？为了方便大家理解，再举个生活的例子吧。

> ★
>
> 小明和小红打电话聊天，通话差不多要结束时，小红说，“我没啥要说的了”。小明回答，“我知道了”。但是小明可能还有要说的话，小红不能要求小明跟着她自己的节奏结束通话，于是小明可能又叽叽歪歪说了一通，最后小明说，“我说完了”，小红回答，“我知道了”，这样通话才算结束。
>
> ”

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-d1851a1a8ede59ee850fd57e2f09b05b.png)

正常情况下。只要数据传输完了，**不管是客户端还是服务端，都可以主动发起四次挥手**，释放连接。

### 40. TCP四次挥手过程中，为什么需要等待2MSL,才进入CLOSED关闭状态

**思路：** 这个**问得频率特别高**。去面试前，一定要把这道题拿下哈。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-44ee0cb73ea8397feb1977d138f8f995.png)

2MSL，**two Maximum Segment Lifetime**，即两个最大段生命周期。假设**主动发起挥手的是客户端**，那么需要2MSL的原因是：

> ★
>
> - **1.为了保证客户端发送的最后一个ACK报文段能够到达服务端。** 这个ACK报文段有可能丢失，因而使处在**LAST-ACK**状态的服务端就收不到对已发送的**FIN + ACK**报文段的确认。服务端会超时重传这个FIN+ACK 报文段，而客户端就能在 2MSL 时间内（**超时 + 1MSL 传输**）收到这个重传的 FIN+ACK 报文段。接着客户端重传一次确认，重新启动2MSL计时器。最后，客户端和服务器都正常进入到**CLOSED**状态。
> - **2. 防止已失效的连接请求报文段出现在本连接中**。客户端在发送完最后一个ACK报文段后，再经过时间2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。
>
> ”

### FIN一定要程序执行close()或shutdown()才能发出吗？

**不一定**。一般情况下，通过对`socket`执行 `close()` 或 `shutdown()` 方法会发出`FIN`。但实际上，**只要应用程序退出**，不管是**主动**退出，还是**被动**退出（因为一些莫名其妙的原因被`kill`了）, **都会**发出 `FIN`。

> **FIN 是指"我不再发送数据"**，因此`shutdown()` 关闭读不会给对方发FIN, 关闭写才会发FIN。

### 聊聊保活计时器的作用

除时间等待计时器外，TCP 还有一个保活计时器（keepalive timer）。设想这样的场景：客户已主动与服务器建立了TCP连接。但后来客户端的主机突然发生故障。显然，服务器以后就不能再收到客户端发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就需要使用保活计时器了。

服务器每收到一次客户的数据，就**重新设置保活计时器，时间的设置通常是两个小时**。若两个小时都没有收到客户端的数据，服务端就发送一个探测报文段，以后则每隔 75秒钟发送一次。若连续发送10个探测报文段后仍然无客户端的响应，服务端就认为客户端出了故障，接着就关闭这个连接。

### 如果服务器出现了大量CLOSE_WAIT状态如何解决。

我们先来复习下TCP的四次挥手

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-44ee0cb73ea8397feb1977d138f8f995.png)

- 服务器端收到客户端发送的`FIN`后，TCP协议栈就会**自动发送ACK**，接着进入**CLOSE_WAIT**状态。
- 但是**如果服务器端不执行socket的close()操作（即不发送FIN=1， ACK=1，seq=w,  ack=u+1  第三次挥手）**，那么就没法进入LAST_ACK,导致大量连接处于CLOSE_WAIT状态
- 所以，如果服务器出现了大量**CLOSE_WAIT**状态，一般是**程序Bug，或者关闭socket不及时**。

### 大量的 TIME_WAIT 状态连接怎么处理？

几个方面：

1. 问题描述：什么现象？什么影响？
2. 问题分析
3. 解决方案
4. 底层原理

#### 1.问题描述

模拟高并发的场景，会出现批量的 `TIME_WAIT` 的 TCP 连接：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/cs2wu427ys.png)

短时间后，所有的 `TIME_WAIT` 全都消失，被回收，端口包括服务，均正常。

即，在高并发的场景下，`TIME_WAIT` 连接存在，属于正常现象。

线上场景中，持续的高并发场景

- 一部分 `TIME_WAIT` 连接被回收，但新的 `TIME_WAIT` 连接产生；
- 一些极端情况下，会出现**大量**的 `TIME_WAIT` 连接。

Think：

> 上述大量的 `TIME_WAIT` 状态 TCP 连接，有什么业务上的影响吗？

Nginx 作为反向代理时，大量的短链接，可能导致 Nginx 上的 TCP 连接处于 `time_wait` 状态：

- 每一个 time_wait 状态，都会占用一个「本地端口」，上限为 `65535`(16 bit，2 Byte)；
- 当大量的连接处于 `time_wait` 时，新建立 TCP 连接会出错，**address already in use : connect** 异常

统计 TCP 连接的状态：

```javascript
1.  `// 统计：各种连接的数量`
2.  `$ netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'`
3.  `ESTABLISHED 1154`
4.  `TIME_WAIT 1645`
```

Tips：

> TCP 本地端口数量，上限为 `65535`（6.5w），这是因为 TCP 头部使用 `16 bit`，存储「**端口号**」，因此约束上限为 `65535`。

#### **2.问题分析**

大量的 `TIME_WAIT` 状态 TCP 连接存在，其本质原因是什么？

- 大量的**短连接**存在
- 特别是 HTTP 请求中，如果 `connection` 头部取值被设置为 `close` 时，基本都由「**服务端**」发起**主动关闭连接**
- 而，`TCP 四次挥手`关闭连接机制中，为了保证 `ACK 重发`和`丢弃延迟数据`，设置 `time_wait` 为 2 倍的 `MSL`（报文最大存活时间）

TIME_WAIT 状态：

- TCP 连接中，**主动关闭连接**的一方出现的状态；（收到 FIN 命令，进入 TIME_WAIT 状态，并返回 ACK 命令）
- 保持 2 个 `MSL` 时间，即，`4 分钟`；（MSL 为 2 分钟）

#### **3.解决办法**

解决上述 `time_wait` 状态大量存在，导致新连接创建失败的问题，一般解决办法：

1、**客户端**，HTTP 请求的头部，connection 设置为 keep-alive，保持存活一段时间：现在的浏览器，一般都这么进行了

2、[**服务器**](https://cloud.tencent.com/product/cvm?from=10680)**端**，

- 允许 `time_wait` 状态的 socket 被**重用**
- 缩减 `time_wait` 时间，设置为 `1 MSL`（即，2 mins）

#### **结论**：

几个核心要点

1、 **time_wait 状态的影响**：

- TCP 连接中，「主动发起关闭连接」的一端，会进入 time_wait 状态
- time_wait 状态，默认会持续 `2 MSL`（报文的最大生存时间），一般是 2x2 mins
- time_wait 状态下，TCP 连接占用的端口，无法被再次使用
- TCP 端口数量，上限是 6.5w（`65535`，16 bit）
- 大量 time_wait 状态存在，会导致新建 TCP 连接会出错，**address already in use : connect** 异常

2、 **现实场景**：

- 服务器端，一般设置：**不允许**「主动关闭连接」
- 但 HTTP 请求中，http 头部 connection 参数，可能设置为 close，则，服务端处理完请求会主动关闭 TCP 连接
- 现在浏览器中， HTTP 请求 `connection` 参数，一般都设置为 `keep-alive`
- Nginx 反向代理场景中，可能出现大量短链接，服务器端，可能存在

3、 **解决办法：服务器端**，

- 允许 `time_wait` 状态的 socket 被重用
- 缩减 `time_wait` 时间，设置为 `1 MSL`（即，2 mins）

### 服务器出现大量的TIME_WAIT状态或CLOSE_WAIT状态

服务器出现异常，百分之八九十都是下面两种情况：

- 服务器保持了大量TIME_WAIT状态；
- 服务器保持了大量CLOSE_WAIT状态；

因为linux分配给一个用户的文件句柄是有限的，而TIME_WAIT和CLOSE_WAIT两种状态如果一直被保持，那么意味着对应数目的通道就一直被占着，而且是“占着茅坑不使劲”，一旦达到句柄数上限，新的请求就无法被处理了，接着就是大量Too Many Open Files异常，tomcat崩溃。

在服务器的日常维护过程中，会经常用到下面的命令：

```
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
```

![image.png](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/1628729346351202130.png)

#### 服务器保持了大量TIME_WAIT状态

从上面的示意图可以看得出来，**TIME_WAIT是主动关闭连接的一方保持的状态**，对于爬虫服务器来说它本身就是客户端，在完成一个爬取任务之后，它就会发起主动关闭连接，从而进入TIME_WAIT的状态。然后在保持这个状态2MSL（max segment lifetime）时间之后，彻底关闭回收资源。

为什么要这么做？明明就已经主动关闭连接了为啥还要保持资源一段时间呢？

这个是TCP/IP的设计者规定的，主要出于以下两个方面的考虑：

1. 防止上一次连接中的包，迷路后重新出现，影响新连接（经过2MSL，上一次连接中所有的重复包都会消失）。

2. 可靠的关闭TCP连接。在主动关闭方发送的最后一个 ack(fin) ，有可能丢失，这时被动方会重新发fin，如果这时主动方处于 CLOSED 状态 ，就会响应 rst 而不是 ack。所以主动方要处于 TIME_WAIT 状态，而不能是 CLOSED 。另外这么设计TIME_WAIT 会定时的回收资源，并不会占用很大资源的，除非短时间内接受大量请求或者受到攻击。

大量的 `TIME_WAIT` 状态 TCP 连接存在，其本质原因是什么？

- 大量的**短连接**存在
- 特别是 HTTP 请求中，如果 `connection` 头部取值被设置为 `close` 时，基本都由「**服务端**」发起**主动关闭连接**
- 而，`TCP 四次挥手`关闭连接机制中，为了保证 `ACK 重发`和`丢弃延迟数据`，设置 `time_wait` 为 2 倍的 `MSL`（报文最大存活时间）

**time_wait 状态的影响**：

- time_wait 状态下，TCP 连接占用的端口，无法被再次使用
- TCP 端口数量，上限是 6.5w（`65535`，16 bit）
- 大量 time_wait 状态存在，会导致新建 TCP 连接会出错，**address already in use : connect** 异常

**解决办法**

解决上述 `time_wait` 状态大量存在，导致新连接创建失败的问题，一般解决办法：

- **客户端**，HTTP 请求的头部，connection 设置为 keep-alive，保持存活一段时间：现在的浏览器，一般都这么进行了 

- [**服务器**](https://cloud.tencent.com/product/cvm?from=10680)**端**，

  - 允许 `time_wait` 状态的 socket 被**重用**

  - 缩减 `time_wait` 时间，设置为 `1 MSL`（即，2 mins）

#### 服务器保持了大量CLOSE_WAIT状态

TIME_WAIT状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的。

但是CLOSE_WAIT就不一样了，从上面的图可以看出来，如果一直保持在CLOSE_WAIT状态，那么**只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号**。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。

close_wait是被动关闭连接是形成的，根据TCP状态机，服务器端收到客户端发送的FIN，TCP协议栈会自动发送ACK，链接进入close_wait状态。但**如果服务器端不执行socket的close()操作**，状态就不能由close_wait迁移到last_ack，则系统中会存在很多close_wait状态的连接

执行如下命令：

```
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
```

发现CLOSE_WAIT的数量始终在400以上，一直没降过。

个人觉得这种情况，通过服务器内核参数也没办法解决，服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行。

如果你使用的是HttpClient并且你遇到了大量CLOSE_WAIT的情况，那么这篇日志也许对你有用：

HttpClient连接池抛出大量
ConnectionPoolTimeoutException: Timeout waiting for connection异常排查

http://blog.csdn.net/shootyou/article/details/6615051

在那边日志里头我举了个场景，来说明CLOSE_WAIT和TIME_WAIT的区别，这里重新描述一下：

服务器A是一台爬虫服务器，它使用简单的HttpClient去请求资源服务器B上面的apache获取文件资源，正常情况下，如果请求成功，那么在抓取完资源后，服务器A会主动发出关闭连接的请求，这个时候就是主动关闭连接，服务器A的连接状态我们可以看到是TIME_WAIT。

如果一旦发生异常呢？假设请求的资源服务器B上并不存在，那么这个时候就会由服务器B发出关闭连接的请求，服务器A就是被动的关闭了连接，如果服务器A被动关闭连接之后程序员忘了让HttpClient释放连接，那就会造成CLOSE_WAIT的状态了。

所以如果将大量CLOSE_WAIT的解决办法总结为一句话那就是：查代码。因为问题出在服务器程序里头。

### 如何优化 TIME_WAIT？

这里给出优化 TIME-WAIT 的几个方式，都是有利有弊：

- 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；
- net.ipv4.tcp_max_tw_buckets
- 程序中使用 SO_LINGER ，应用强制使用 RST 关闭。

*方式一：net.ipv4.tcp_tw_reuse 和 tcp_timestamps*

如下的 Linux 内核参数开启后，则可以**复用处于 TIME_WAIT 的 socket 为新的连接所用**。

有一点需要注意的是，**tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。**

```shell
net.ipv4.tcp_tw_reuse = 1
```

使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即

```text
net.ipv4.tcp_timestamps=1（默认即为 1）
```

这个时间戳的字段是在 TCP 头部的「选项」里，它由一共 8 个字节表示时间戳，其中第一个 4 字节字段用来保存发送该数据包的时间，第二个 4 字节字段用来保存最近一次接收对方发送到达数据的时间。

由于引入了时间戳，我们在前面提到的 `2MSL` 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。

*方式二：net.ipv4.tcp_max_tw_buckets*

这个值默认为 18000，**当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置**，这个方法比较暴力。

*方式三：程序中使用 SO_LINGER*

我们可以通过设置 socket 选项，来设置调用 close 关闭连接行为。

```c
struct linger so_linger;
so_linger.l_onoff = 1;
so_linger.l_linger = 0;
setsockopt(s, SOL_SOCKET, SO_LINGER, &so_linger,sizeof(so_linger));
```

如果`l_onoff`为非 0， 且`l_linger`值为 0，那么调用`close`后，会立该发送一个`RST`标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了`TIME_WAIT`状态，直接关闭。

但这为跨越`TIME_WAIT`状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。

前面介绍的方法都是试图越过 `TIME_WAIT`状态的，这样其实不太好。虽然 TIME_WAIT 状态持续的时间是有一点长，显得很不友好，但是它被设计来就是用来避免发生乱七八糟的事情。

《UNIX网络编程》一书中却说道：**TIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它**。

**如果服务端要避免过多的 TIME_WAIT 状态的连接，就永远不要主动断开连接，让客户端去断开，由分布在各处的客户端去承受 TIME_WAIT**。

### 如果已经建立了连接，但是客户端突然出现故障了怎么办？

TCP 有一个机制是**保活机制**。这个机制的原理是这样的：

**定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用**，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序。

在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：

```shell
net.ipv4.tcp_keepalive_time=7200
net.ipv4.tcp_keepalive_intvl=75  
net.ipv4.tcp_keepalive_probes=9
```

- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制
- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；
- tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。

也就是说在 Linux 系统中，最少需要经过 2 小时 11 分 15 秒才可以发现一个「死亡」连接。

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9jZG4uanNkZWxpdnIubmV0L2doL3hpYW9saW5jb2Rlci9JbWFnZUhvc3QyLyVFOCVBRSVBMSVFNyVBRSU5NyVFNiU5QyVCQSVFNyVCRCU5MSVFNyVCQiU5Qy9UQ1AtJUU0JUI4JTg5JUU2JUFDJUExJUU2JThGJUExJUU2JTg5JThCJUU1JTkyJThDJUU1JTlCJTlCJUU2JUFDJUExJUU2JThDJUE1JUU2JTg5JThCLzMzLmpwZw?x-oss-process=image/format,png" alt="img" style="zoom:50%;" />

注意，应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制。

如果开启了 TCP 保活，需要考虑以下几种情况：

- 第一种，**对端程序是正常工作的**。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
- 第二种，**对端程序崩溃并重启**。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，**会产生一个 RST 报文**，这样很快就会发现 TCP 连接已经被重置。
- 第三种，**是对端程序崩溃，或对端由于其他原因导致报文不可达**。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。

TCP 保活的这个机制检测的时间是有点长，我们可以自己在应用层实现一个心跳机制。

比如，web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**

<img src="https://img-blog.csdnimg.cn/img_convert/2d872f947dedd24800a1867dc4f8b9ce.png" alt="web 服务的 心跳机制" style="zoom:50%;" />

### 第二次挥手丢失了，会发生什么？

当服务端收到客户端的第一次挥手后，就会先回一个 ACK 确认报文，此时服务端的连接进入到 `CLOSE_WAIT` 状态。

在前面我们也提了，**ACK 报文是不会重传的，所以如果服务端的第二次挥手丢失了，客户端就会触发超时重传机制，重传 FIN 报文，直到收到服务端的第二次挥手，或者达到最大的重传次数**。

这里提一下，当客户端收到第二次挥手，也就是收到服务端发送的 ACK 报文后，客户端就会处于 `FIN_WAIT2` 状态，在这个状态需要等服务端发送第三次挥手，也就是服务端的 FIN 报文。

对于 close 函数关闭的连接，由于无法再发送和接收数据，所以`FIN_WAIT2` 状态不可以持续太久，而 `tcp_fin_timeout` 控制了这个状态下连接的持续时长，默认值是 60 秒。

这意味着**对于调用 close 关闭的连接，如果在 60 秒后还没有收到 FIN 报文，客户端（主动关闭方）的连接就会直接关闭**。

但是注意，如果主动关闭方使用 shutdown 函数关闭连接，指定了只关闭发送方向，而接收方向并没有关闭，那么意味着主动关闭方还是可以接收数据的。此时，如果主动关闭方一直没收到第三次挥手，那么主动关闭方的连接将会一直处于 `FIN_WAIT2` 状态（`tcp_fin_timeout` 无法控制 shutdown 关闭的连接）。

### 如果机器上FIN-WAIT-2状态特别多，是为什么

根据上面的四次挥手图，可以看出，`FIN-WAIT-2`是**主动方**那边的状态。

处于这个状态的程序，一直在等**第三次挥手**的`FIN`。而**第三次挥手需要由被动方在代码里执行`close()` 发出**。

因此当机器上`FIN-WAIT-2`状态特别多，那一般来说，另外一台机器上会有大量的 `CLOSE_WAIT`。需要检查有大量的 `CLOSE_WAIT`的那台机器，为什么迟迟不愿调用`close()`关闭连接。

所以，如果机器上`FIN-WAIT-2`状态特别多，**一般是因为对端一直不执行`close()`方法发出第三次挥手**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIian6HJics3mtJEayjn3gtKefbNANxQEkLjYnrkzqffGLTCajoQ7S4vutjicDbB6BUdU4XNW1VLrdaGWg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 主动方在close之后收到的数据，会怎么处理

从源码的角度提到了，**一般情况下**，程序主动执行`close()`的时候；

- 如果当前连接对应的`socket`的**接收缓冲区**有数据，会发`RST`。
- 如果**发送缓冲区**有数据，那会等待发送完，再发第一次挥手的`FIN`。

大家知道，TCP是**全双工通信**，意思是发送数据的同时，还可以接收数据。

`Close()`的含义是，此时要同时**关闭发送和接收**消息的功能。

也就是说，虽然**理论上**，第二次和第三次挥手之间，被动方是可以传数据给主动方的。

但如果 主动方的四次挥手是通过 `close()` 触发的，那主动方是不会去收这个消息的。而且还会回一个 `RST`。直接结束掉这次连接。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIian6HJics3mtJEayjn3gtKefbdP9AfzmQSP8DhQhz0RJD6r5uxGARhadESqjcY1M9vz2w8njmn2Wssg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 第二第三次挥手之间，不能传输数据吗？

也不是。前面提到`Close()`的含义是，要同时**关闭发送和接收**消息的功能。

那如果能做到**只关闭发送消息**，**不关闭接收消息**的功能，那就能继续收消息了。这种 `half-close` 的功能，通过调用`shutdown()` 方法就能做到。

```
int shutdown(int sock, int howto);
```

> 其中 howto 为断开方式。有以下取值：
>
> - SHUT_RD：关闭读。这时应用层不应该再尝试接收数据，内核协议栈中就算接收缓冲区收到数据也会被丢弃。
> - SHUT_WR：关闭写。如果发送缓冲区中还有数据没发，会将将数据传递到目标主机。
> - SHUT_RDWR：关闭读和写。相当于`close()`了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIian6HJics3mtJEayjn3gtKefbwXdVdlRCcvIhvDAELQ2ZoMRQFhN2Xa7HP6NWFR8DEwRBFhlDQALOVw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 怎么知道对端socket执行了close还是shutdown

不管**主动**关闭方调用的是`close()`还是`shutdown()`，对于被动方来说，收到的就只有一个`FIN`。

**被动**关闭方**就懵了**，"我怎么知道对方让不让我继续发数据？"

其实，大可不必纠结，该发就发。

第二次挥手和第三次挥手之间，如果**被动**关闭方想发数据，那么在代码层面上，就是执行了 `send()` 方法。

```
int send( SOCKET s,const char* buf,int len,int flags);
```

`send()` 会把数据拷贝到本机的**发送缓冲区**。如果发送缓冲区没出问题，都能拷贝进去，所以正常情况下，`send()`**一般**都会返回成功。

然后**被动方**内核协议栈会把数据发给**主动**关闭方。

- 如果上一次**主动**关闭方调用的是`shutdown(socket_fd, SHUT_WR)`。那此时，**主动关闭方**不再发送消息，但能接收**被动方**的消息，一切如常，皆大欢喜。
- 如果上一次**主动**关闭方调用的是`close()`。那**主动方**在收到**被动方**的数据后会直接**丢弃**，然后回一个`RST`。

针对第二种情况。

被动方**内核协议栈**收到了`RST`，会把连接关闭。但内核连接关闭了，应用层也不知道（除非被通知）。

此时被动方**应用层**接下来的操作，无非就是**读或写**。

- 如果是读，则会返回`RST`的报错，也就是我们常见的`Connection reset by peer`。
- 如果是写，那么程序会产生`SIGPIPE`信号，应用层代码可以捕获并处理信号，如果不处理，则默认情况下进程会终止，异常退出。

**总结一下**，当被动关闭方 `recv()` 返回`EOF`时，说明主动方通过 `close()`或 `shutdown(fd, SHUT_WR)` 发起了第一次挥手。

如果此时被动方执行**两次** `send()`。

- 第一次`send()`, 一般会成功返回。
- 第二次`send()`时。如果主动方是通过 `shutdown(fd, SHUT_WR)` 发起的第一次挥手，那此时`send()`还是会成功。如果主动方通过 `close()`发起的第一次挥手，那此时会产生`SIGPIPE`信号，进程默认会终止，异常退出。不想异常退出的话，记得捕获处理这个信号。

### 如果被动方一直不发第三次挥手，会怎么样

[参考](https://mp.weixin.qq.com/s/Z0EqSihRaRbMscrZJl-zxQ)

第三次挥手，是由**被动方**主动触发的，比如调用`close()`。

如果由于代码错误或者其他一些原因，被动方就是不执行第三次挥手。

这时候，主动方会根据自身第一次挥手的时候用的是 `close()` 还是 `shutdown(fd, SHUT_WR)` ，有不同的行为表现。

- 如果是 `shutdown(fd, SHUT_WR)` ，说明主动方其实只关闭了写，但还可以读，此时会一直处于 `FIN-WAIT-2`， 死等被动方的第三次挥手。
- 如果是 `close()`， 说明主动方读写都关闭了，这时候会处于 `FIN-WAIT-2`一段时间，这个时间由 `net.ipv4.tcp_fin_timeout` 控制，一般是 `60s`，这个值正好跟`2MSL`一样 。**超过这段时间之后，状态不会变成 `TIME-WAIT`，而是直接变成`CLOSED`。**

```
# cat /proc/sys/net/ipv4/tcp_fin_timeout
60
```



![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIian6HJics3mtJEayjn3gtKefbK95HxzvkOlIxiafFnopktbvbd9UllwfhBlCrNJ3yicbvS1Oxn9uib8Xng/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 收到RST，就一定会断开TCP连接吗？

[参考](https://mp.weixin.qq.com/s/Fr6o6gRiIUIspV9-jR9snw)、[参考](https://zhuanlan.zhihu.com/p/30791159)

TCP正常情况下断开连接是用四次挥手，那是**正常时候**的优雅做法。

但**异常情况**下，收发双方都不一定正常，连挥手这件事本身都可能做不到，所以就需要一个机制去强行关闭连接。

**RST** 就是用于这种情况，一般用来**异常地**关闭一个连接。它是一个TCP包头中的**标志位**。

**正常情况下**，不管是**发出**，还是**收到**置了这个标志位的数据包，**相应的内存、端口等连接资源都会被释放**。**从效果上来看就是TCP连接被关闭了**。

而接收到 RST的一方，一般会看到一个 `connection reset` 或  `connection refused` 的报错。

> 在TCP协议中，rst段标识复位，用来异常的关闭连接。在TCP的设计中它是不可或缺的，发送rst段关闭连接时，**不必等缓冲区的数据都发送出去，直接丢弃缓冲区中的数据**。而接收端收到rst段后，也不必发送ack来确认。

**RST**一般出现于异常情况，归类为 **对端的端口不可用** 和 **socket提前关闭**。

- **情况1 目标端口未监听**
- **情况2 目的主机或者网络路径中防火墙拦截**
- **情况3 socket接收缓冲取Recv-Q中的数据未完全被应用程序读取时关闭该socket**
- **情况4 向已关闭的socket发送数据**
- **情况5 向已关闭的连接发送FIN**
- **情况6 向已经消逝的连接中发送数据**

总结

- RST其实是TCP包头里的一个标志位，目的是为了在**异常情况**下关闭连接。
- 内核收到RST后，应用层只能通过调用读/写操作来感知，此时会对应获得 **Connection reset by peer**（接收端） 和**Broken pipe** （发送端）报错。
- 发出RST后不需要得到对方的ACK确认包，因此RST丢失后对方不能立刻感知，但是通过下一次**重传**数据或keepalive**心跳包**可以导致RST重传。

### 41. TCP的粘包和拆包

TCP是面向流，没有界限的一串数据。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为，一**个完整的包可能会被TCP拆分成多个包进行发送**，**也有可能把多个小的包封装成一个大的数据包发送**，这就是所谓的TCP粘包和拆包问题。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-8dec9e57094f083991b19a6ba95c49fe.png)TCP的粘包和拆包

**为什么会产生粘包和拆包呢?**

- **要发送的数据小于TCP发送缓冲区的大小**，TCP将多次写入缓冲区的数据一次发送出去，将会发生粘包；
- 接收数据端的**应用层没有及时读取接收缓冲区中的数据**，将发生粘包；
- **要发送的数据大于TCP发送缓冲区剩余空间大小**，将会发生拆包；
- 待发送数据大于MSS（最大报文长度），TCP在传输前将进行拆包。即TCP报文长度-TCP头部长度>MSS。

**解决方案：**

- 发送端将每个数据包封装为**固定长度**
- 在数据尾部**增加特殊字符**进行分割
- **将数据分为两部分**，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小。

### 42. 聊聊TCP的流量控制

TCP三次握手，发送端和接收端进入到ESTABLISHED状态，它们即可以愉快地传输数据啦。

但是发送端不能疯狂地向接收端发送数据，因为接收端接收不过来的话，接收方只能把处理不过来的数据存在缓存区里。如果缓存区都满了，发送方还在疯狂发送数据的话，接收方只能把收到的数据包丢掉，这就浪费了网络资源啦。

> ★
>
> TCP 提供一种机制可以**让发送端根据接收端的实际接收能力控制发送的数据量**，这就是**流量控制**。
>
> ”

TCP通过滑动窗口来控制流量，我们看下流量控制的**简要流程**吧：

首先双方三次握手，初始化各自的窗口大小，均为 400 个字节。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-8f6389c120734508b1e3367a8a8c4d9f.png)TCP的流量控制

1. 假如当前发送方给接收方发送了200个字节，那么，发送方的`SND.NXT`会右移200个字节，也就是说当前的可用窗口减少了200 个字节。
2. 接受方收到后，放到缓冲队列里面，REV.WND =400-200=200字节，所以win=200字节返回给发送方。接收方会在 ACK 的报文首部带上缩小后的滑动窗口200字节
3. 发送方又发送200字节过来，200字节到达，继续放到缓冲队列。不过这时候，由于大量负载的原因，接受方处理不了这么多字节，只能处理100字节，剩余的100字节继续放到缓冲队列。这时候，REV.WND = 400-200-100=100字节，即win=100返回发送方。
4. 发送方继续干活，发送100字节过来，这时候，接受窗口win变为0。
5. 发送方停止发送，开启一个定时任务，每隔一段时间，就去询问接受方，直到win大于0，才继续开始发送。

### 43. 说说半连接队列和 SYN Flood攻击的关系

**思路讲解：** 我以前面试的时候，面试官就问我什么是半连接队列、什么是全连接队列，哈哈。我们需要掌握半连接队列、全连接对列是啥，还需要清楚半连接队列和 SYN Flood攻击有什么关系。

**我的答案如下：**

TCP进入三次握手前，服务端会从**CLOSED**状态变为**LISTEN**状态,同时在内部创建了两个队列：半连接队列（SYN队列）和全连接队列（ACCEPT队列）。

什么是**半连接队列（SYN队列）** 呢? 什么是**全连接队列（ACCEPT队列）** 呢？回忆下TCP三次握手的图：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-7038b6e7d52a9cc580b6071a705ad61a.png)

- TCP三次握手时，客户端发送SYN到服务端，服务端收到之后，便回复**ACK和SYN**，状态由**LISTEN变为SYN_RCVD**，此时这个连接就被推入了**SYN队列**，**即半连接队列**。
- 当客户端回复ACK, 服务端接收后，三次握手就完成了。这时连接会等待被具体的应用取走，在被取走之前，它**被推入ACCEPT队列，即全连接队列**。

SYN Flood是一种典型的DDos攻击，它在短时间内，伪造**不存在的IP地址**,向服务器大量发起SYN报文。当服务器回复SYN+ACK报文后，不会收到ACK回应报文，导致服务器上建立大量的半连接半连接队列满了，这就无法处理正常的TCP请求啦。

那么有哪些方案应对呢？主要有 **syn cookie**和**SYN Proxy防火墙**等。

> ★
>
> - **syn cookie**：在收到SYN包后，服务器根据一定的方法，**以数据包的源地址、端口等信息为参数计算出一个cookie值作为自己的SYNACK包的序列号**，回复SYN+ACK后，服务器并不**立即分配资源进行处理**，等收到发送方的ACK包后，重新根据数据包的源地址、端口计算该包中的确认序列号是否正确，如果正确则建立连接，否则丢弃该包。
> - **SYN Proxy防火墙**：服务器防火墙会**对收到的每一个SYN报文进行代理和回应**，并保持半连接。等发送方将ACK包返回后，再重新构造SYN包发到服务器，建立真正的TCP连接。
>
> ”

### 44. 聊聊TCP的滑动窗口

[参考](https://blog.csdn.net/yao5hed/article/details/81046945)

**思路讲解：** TCP滑动窗口是个高频考点，我们需要知道**TCP报文首部有个字段win控制窗口大小**的，同时也需要掌握，滑动窗口是怎么滑的。

TCP 发送一个数据，如果需要收到确认应答，才会发送下一个数据。这样的话就会有个缺点：效率会比较低。

> ★
>
> 这就好像我们面对面在聊天，你说完一句，我应答之后，你才能说下一句。那么，如果我在忙其他事情，没有能够及时回复你呢？你说完一句后，要等到我忙完回复你，你才说下句，这显然不现实，效率太低。
>
> ”

为了解决这个问题，TCP引入了**窗口**，它是操作系统开辟的一个缓存空间。窗口大小值表示无需等待确认应答，而可以继续发送数据的最大值。

TCP头部有个字段叫win，也即那个**16位的窗口大小**，它告诉对方本端的TCP接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度，从而达到**流量控制**的目的。

> ★
>
> 通俗点讲，就是**接受方每次收到数据包，在发送确认报文的时候，同时告诉发送方，自己的缓存区还有多少空余空间**，缓冲区的空余空间，我们就称之为接受窗口大小。这就是win。
>
> ”

TCP 滑动窗口分为两种: 发送窗口和接收窗口。**发送端的滑动窗口**包含四大部分，如下：

- 已发送且已收到ACK确认
- 已发送但未收到ACK确认
- 未发送但可以发送
- 未发送也不可以发送

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-ae709e0823e9e85ae50a98d6254ef491.png)

- 虚线矩形框，就是发送窗口。
- SND.WND: 表示发送窗口的大小,上图虚线框的格子数是14个，即发送窗口大小是14。
- SND.NXT：下一个发送的位置，它指向未发送但可以发送的第一个字节的序列号。
- SND.UNA: 一个绝对指针，它指向的是已发送但未确认的第一个字节的序列号。

接收方的滑动窗口包含三大部分，如下：

- 已成功接收并确认
- 未收到数据但可以接收
- 未收到数据并不可以接收的数据

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-886a4ca16926fd89d193095a8c6ccb5f.png)

- 虚线矩形框，就是接收窗口。
- REV.WND: 表示接收窗口的大小,上图虚线框的格子就是9个。
- REV.NXT:下一个接收的位置，它指向未收到但可以接收的第一个字节的序列号。

### 45. TCP的拥塞控制

**思路讲解：** TCP拥塞机制也是个高频考点，需要掌握**它跟流量控制**的区别，也需要掌握拥塞控制的这几种算法：**慢启动算法、拥塞避免、拥塞发生、快速恢复算法**。

拥塞控制是**作用于网络的，防止过多的数据包注入到网络中，避免出现网络负载过大的情况**。它的目标主要是最大化利用网络上瓶颈链路的带宽。它跟**流量控制**又有什么区别呢？流量控制是**作用于接收者**的，根据**接收端的实际接收能力控制发送速度**，防止分组丢失的。

我们可以把网络链路比喻成一根水管，如果我们想最大化利用网络来传输数据，那就是尽快让水管达到最佳充满状态。

![img](http://mianbaoban-assets.oss-cn-shenzhen.aliyuncs.com/xinyu-images/MBXY-CR-e05ea7c279a7308a5bc041dbab7152f6.png)

发送方维护一个**拥塞窗口cwnd（congestion window）** 的变量，用来估算在一段时间内这条链路（水管）可以承载和运输的数据（水）的数量。它大小代表着网络的拥塞程度，并且是动态变化的，但是为了达到最大的传输效率，我们该如何知道这条水管的运送效率是多少呢？

一个比较简单的方法就是不断增加传输的水量，直到水管快要爆裂为止（对应到网络上就是发生丢包），用 TCP的描述就是：

> ★
>
> 只要网络中没有出现拥塞，拥塞窗口的值就可以再增大一些，以便把更多的数据包发送出去，但只要网络出现拥塞，拥塞窗口的值就应该减小一些，以减少注入到网络中的数据包数。
>
> ”

实际上，拥塞控制主要有这几种常用算法

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

#### 45.1 慢启动算法

**RTT(Round Trip Time)：**一个连接的往返时间，即数据发送时刻到接收到确认的时刻的差值；

**RTO(Retransmission Time Out)：**重传超时时间，即从数据发送时刻算起，超过这个时间便执行重传。

慢启动算法，表面意思就是，别急慢慢来。它表示TCP建立连接完成后，一开始不要发送大量的数据，而是先探测一下网络的拥塞程度。由小到大逐渐增加拥塞窗口的大小，如果没有出现丢包，**每收到一个ACK，就将拥塞窗口cwnd大小就加1（单位是MSS）**。**每轮次**发送窗口增加一倍，呈指数增长，如果出现丢包，拥塞窗口就减半，进入拥塞避免阶段。

- TCP连接完成，初始化cwnd = 1，表明可以传一个MSS单位大小的数据。
- 每当收到一个ACK，cwnd就加一;
- 每当过了一个RTT（transmission round， 传输轮次），cwnd就增加一倍; 呈指数让升

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-8b9d6208fc78c82a044d96f57d5ff787.png)

为了防止cwnd增长过大引起网络拥塞，还需设置一个**慢启动阀值ssthresh**（slow start threshold）状态变量。当`cwnd`到达该阀值后，就好像水管被关小了水龙头一样，减少拥塞状态。即当**cwnd >ssthresh**时，进入了**拥塞避免**算法。

#### 45.2 拥塞避免算法

一般来说，慢启动阀值ssthresh是65535字节，`cwnd`到达**慢启动阀值**后

- 每收到一个ACK时，cwnd = cwnd + 1/cwnd
- 当每过一个RTT时，cwnd = cwnd + 1

显然这是一个线性上升的算法，避免过快导致网络拥塞问题。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-0bce7b07782db19233b98cdd54ed6121.png)

#### 45.3 拥塞发生

当网络拥塞发生**丢包**时，会有两种情况：

- RTO超时重传
- 快速重传

如果是发生了**RTO超时重传**，就会使用拥塞发生算法

- 慢启动阀值sshthresh =  cwnd /2
- cwnd 重置为 1
- 进入新的慢启动过程

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-373e0f394792af9c44face0f2852abff.png)

这真的是**辛辛苦苦几十年，一朝回到解放前**。其实还有更好的处理方式，就是**快速重传**。发送方收到**3个连续重复的ACK**时，就会快速地重传，不必等待**RTO超时**再重传。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-311271a6e184c49c37a2e3b2bfd9af61.png)

慢启动阀值ssthresh 和 cwnd 变化如下：

- 拥塞窗口大小 cwnd = cwnd/2
- 慢启动阀值 ssthresh = cwnd
- 进入快速恢复算法

#### 45.4 快速恢复

**快速重传和快速恢复算法一般同时使用**。快速恢复算法认为，还有**3个重复ACK收到**，说明网络也没那么糟糕，所以没有必要像RTO超时那么强烈。

正如前面所说，进入快速恢复之前，cwnd 和 sshthresh已被更新：

```
- cwnd = cwnd /2
- sshthresh = cwnd
```

然后，真正的快速算法如下：

- cwnd = sshthresh  + 3
- 重传重复的那几个ACK（即丢失的那几个数据包）
- 如果再收到重复的 ACK，那么 cwnd = cwnd +1
- 如果收到新数据的 ACK 后, cwnd = sshthresh。因为收到新数据的 ACK，表明恢复过程已经结束，可以再次进入了拥塞避免的算法了。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-8537366c7623840fd7065dd671ddbe40.png)

### 46.请简述TCP和UDP的区别

**思路：** 这道题，校招的时候，问的概率高点，概念性的东西，**TCP是面向连接，而UDP是无连接**。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-fd1699aa297fdd9d109808150b5a71fa.png)

### 47. 说说TCP是如何确保可靠性的呢？

**思路：** TCP是可靠的连接，为什么具有可靠性呢？记住这些点：**连接和断开的可靠性（三次握手，四次挥手）、有状态（哪些数据发送了，哪些没发）、可控制（超时重传、流量控制、拥塞控制等）**。

- 首先，TCP的连接是基于**三次握手**，而断开则是基于**四次挥手**。确保连接和断开的可靠性。
- 其次，TCP的可靠性，还体现在**有状态**;TCP会记录哪些数据发送了，哪些数据被接收了，哪些没有被接受，并且保证数据包按序到达，保证数据传输不出差错。
- 再次，TCP的可靠性，还体现在**可控制**。它有**数据包校验**、**ACK应答**、**超时重传(发送方)**、**失序数据重传**（接收方）、**丢弃重复数据**、**流量控制**（滑动窗口）和**拥塞控制**等机制。

### TCP 如何保证有序性

发送主机每次发送数据时，TCP就给每个数据包分配一个序列号并且在一个特定的时间内等待接收主机对分配的这个序列号进行确认，

如果发送主机在一个特定时间内没有收到接收主机的确认，则发送主机会重传此数据包。

接收主机利用序列号对接收的数据进行确认，以便检测对方发送的数据是否有丢失或者乱序等，接收主机一旦收到已经顺序化的数据，它就将这些数据按正确的顺序重组成数据流并传递到高层进行处理。

#### 核心算法

- 为了保证数据包的可靠传递，发送方必须把**已发送的数据包**（未确认）保留在缓冲区
- 同时为每个已发送的数据包启动一个**超时定时器**（重传定时器RTO）
- 如在定时器超时之前收到了对方发来的应答信息（可能是对本包的应答，也可以是对本包后续包的应答），则释放该数据包占用的缓冲区
- 否则，**重传该数据包**，直到收到应答或重传次数超过规定的最大次数为止。
- 接收方收到数据包后，**先进行CRC校验**，如果正确则把数据交给上层协议，然后给发送方**发送一个累计应答包**，表明该数据已收到，如果接收方正好也有数据要发给发送方，应答包也可放在数据包中捎带过去。

### 48. 说说TCP报文首部有哪些字段，其作用又分别是什么？

**思路：** 小伙伴们，可以记下这个图。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-cf87753017b42de921f71c11746cc6cf.png)

- **16位端口号**：源端口号，主机该报文段是来自哪里；目标端口号，要传给哪个上层协议或应用程序
- **32位序号**：一次TCP通信（从TCP连接建立到断开）过程中某一个传输方向上的字节流的每个字节的编号。
- **32位确认号**：用作对另一方发送的tcp报文段的响应。其值是收到的TCP报文段的序号值加1。
- **4位头部长度**：表示tcp头部有多少个32bit字（4字节）。因为4位最大能标识15，所以TCP头部最长是60字节。
- **6位标志位**：URG(紧急指针是否有效)，ACk（表示确认号是否有效），PSH（缓冲区尚未填满），RST（表示要求对方重新建立连接），SYN（建立连接消息标志接），FIN（表示告知对方本端要关闭连接了）
- **16位窗口大小**：是TCP流量控制的一个手段。这里说的窗口，指的是接收通告窗口。它告诉对方本端的TCP接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。
- **16位校验和**：由发送端填充，接收端对TCP报文段执行CRC算法以检验TCP报文段在传输过程中是否损坏。注意，这个校验不仅包括TCP头部，也包括数据部分。这也是TCP可靠传输的一个重要保障。
- **16位紧急指针**：一个正的偏移量。它和序号字段的值相加表示最后一个紧急数据的下一字节的序号。因此，确切地说，这个字段是紧急指针相对当前序号的偏移，不妨称之为紧急偏移。TCP的紧急指针是发送端向接收端发送紧急数据的方法。

### 49. Nagle 算法与延迟确认

#### 49.1 Nagle算法

如果发送方疯狂地向接收方发送很小的数据包，比如一次就发送1个字节，那么显然会有问题。

> ★
>
> TCP/IP协议中，无论发送多少数据，总是需要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。为了尽可能的利用网络带宽，TCP总是希望尽可能的发送足够大的数据。**Nagle算法**就是为了尽可能发送大块数据，避免网络中充斥着许多小数据块。
>
> ”

Nagle算法：**任意时刻，最多只能有一个未被确认的小段**。所谓“小段”，指的是小于MSS（Maximum Segment Size，最大报文长度）尺寸的数据块，所谓“未被确认”，是指一个数据块发送出去后，没有收到对方发送的ACK确认该数据已收到。

Nagle算法的实现规则：

> ★
>
> - 如果包长度达到MSS，则允许发送；
> - 如果该包含有FIN，则允许发送；
> - 设置了TCP_NODELAY选项，则允许发送；
> - 未设置TCP_CORK选项时，若所有发出去的小数据包（包长度小于MSS）均被确认，则允许发送；
> - 上述条件都未满足，但发生了超时（一般为200ms），则立即发送。
>
> ”

#### 49.2 延迟确认

**`什么是延迟应答？`**

> 数据传输的时候，发送端给接收端发送数据，接收端给发送端发去确认应答信息，这样比较耗时，效率低下，延迟应答就是接收端收到数据之后，稍微等一会再应答，这样可以提高数据的传输效率，因为发送端发好几次数据，接收端只需要一次来确认应答，这样可以降低网络拥塞的概率

如果接受方刚接收到发送方的数据包，**在很短很短的时间内，又接收到第二个包**。那么请问接收方是一个一个地回复好点，还是合在一起回复好呢？

> ★
>
> 接收方收到数据包后，如果暂时没有数据要发给对端，它可以等一小段时间，再确认（Linux上默认是40ms）。如果这段时间刚好有数据要传给对端，ACK就随着数据传输，而不需要单独发送一次ACK。如果超过时间还没有数据要发送，也发送ACK，避免对端以为丢包。
>
> ”

**`所有的包都可以延迟应答吗？`**

不是

> 数量限制：每隔N个包就必须应答一次
> 时间限制：超过最大延迟时间就必须应答一次

ps：具体的延迟应答数量和超时时间，依操作系统不同也有差异，一般N取2，超时时间取200ms

但是有些场景不能用延迟确认，比如发现了**乱序包**、**接收到了大于一个 frame 的报文，且需要调整窗口大小**等。

一般情况下，**Nagle算法和延迟确认**不能一起使用，Nagle算法意味着延迟发，**延迟确认**意味着延迟接收，酱紫就会造成更大的延迟，会产生性能问题。



#### **捎带应答**

#### **`什么是捎带应答？`**

> 虽然有延迟应答，但是客户端和服务器在应用层还是还是”一发一收”，此时就会导致数据传输效率低下，捎带应答就是接收端在给发送端发送数据的时候，捎带着向发送端发去确认应答，应答的内容是接收端已经收到发送端发送的数据

**使用捎带应答之前：**

> 客户端：你好吗？
> 服务器：我收到你发的消息了（接收端的ACK应答）
> 服务器：我很好

**使用捎带应答时：**

> 客户端：你好吗？
> 服务器：我很好（接收端给发送端发送的数据），我也收到你发的消息了（接收端的ACK应答）

使用捎带应答后，ACK就可以搭顺风车了，在接收端给发送端发送数据的时候，ACK就可以捎带着给发送端发送过去

### 50. 说说TCP的重传机制

**思路讲解：** TCP的重传机制，也是道非常高频的面试题。重传包括**超时重传、快速重传、带选择确认的重传（SACK）、重复SACK四种**。

#### 50.1 超时重传

超时重传，是TCP协议保证数据可靠性的另一个重要机制，其原理是在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的ACK报文，那么就重新发送数据，直到发送成功为止。

这个一定时间内，一般是多少比较合理呢？来看下什么叫**RTT（Round-Trip Time，往返时间）**。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-69f6f681275c215367e3018831d76ac8.png)

RTT就是数据完全发送完，到收到确认信号的时间，即数据包的一次往返时间。超时重传时间，就是RTO（Retransmission Timeout)。

那么，**RTO到底设置多大呢？**

- 如果RTO设置很大，等了很久都没重发，这样肯定就不行。
- 如果RTO设置很小，那很可能数据都没有丢失，就开始重发了，这会导致网络阻塞，从而恶性循环，导致更多的超时出现。

一般来说，**RTO略微大于RTT**，效果是最佳的。其实，RTO有个标准方法的计算公式，也叫**Jacobson / Karels 算法**。一起来看下吧：

**1. 首先计算SRTT（即计算平滑的RTT）**

```
SRTT = (1 - α) * SRTT + α * RTT  //求 SRTT 的加权平均
```

**2. 其次，计算RTTVAR (round-trip time variation)**

```
RTTVAR = (1 - β) * RTTVAR + β * (|RTT - SRTT|) //计算 SRTT 与真实值的差距
```

**3. 最后，得出最终的RTO**

```
RTO = µ * SRTT + ∂ * RTTVAR  =  SRTT + 4·RTTVAR  
```

一般情况，α、β等的参数取值如下：

```
α = 0.125，β = 0.25， μ = 1，∂ = 4
```

别问这些参数是怎么来的，它们是大量实践，调出的最优参数。

超时重传不是十分完美的重传方案，它有这些缺点：

> ★
>
> - 当一个报文丢失时，会等待一定的超时周期，才重传分组，增加了端到端的时延。
> - 当一个报文丢失时，在其等待超时的过程中，可能会出现这种情况：其后的报文段已经被接收端接收但却迟迟得不到确认，发送端会认为也丢失了，从而引起不必要的重传，既浪费资源也浪费时间。
>
> ”

并且，对于TCP，如果发生一次超时重传，时间间隔下次就会加倍。

#### 50.2 快速重传

其实可以使用**快速重传**，来解决超时重发的时间等待问题。它不以时间驱动，而是以数据驱动。它是基于接收端的反馈信息来引发重传的。快速重传流程如下：

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-fc905a42c5917b66a1d9476b7fa1e71b.png)快速重传流程

发送方发送了 1，2，3，4，5,6份数据:

- 第一份 Seq=1 先送到了，于是就 Ack回2；
- 第二份 Seq=2 也送到了，于是ACK回3；
- 第三份 Seq=3 由于网络等某些原因，没送到；
- 第四份 Seq=4 送到了，但是由于Seq=3没收到。因此ACK还是回3；
- 后面的 Seq=5,6的也送到了，ACK还是回复3，因为Seq=3没有收到。
- 发送方连着收到三个重复冗余ACK=3的确认（其实是4个哈，但是因为前面的一个是正常的ACK，后面三个才是重复冗余的），于是知道哪个报文段在传输过程中丢失了；发送方在定时器过期之前，重传该报文段。
- 最后，接收方收到了 Seq=3，此时因为 Seq=4，5，6都收到了，于是它回ACK=7。

但是呢，**快速重传**也可能有问题：ACK只向告知发送方，最大的有序报文段。到底是哪个报文丢失了呢？**并不确定**！那到底该重传多少个包呢？

> ★
>
> 是重传 Seq=3 ？还是重传 Seq=3、Seq=4、Seq=5、Seq=6 呢？因为发送端并不清楚这三个连续的 ACK=3 是谁传回来的。
>
> ”

#### 50.3 带选择确认的重传（SACK）

为了解决：**应该重传多少个包**的问题? TCP提供了**带选择确认的重传**（即SACK，Selective Acknowledgment）。

> ★
>
> **SACK机制**就是，在快速重传的基础上，**接收方返回最近收到报文段的序列号范围**，这样发送方就知道接收方哪些数据包是没收到的。这样就很清楚应该重传哪些数据包啦。
>
> ”

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-2f299dcc517551ba3f3e2da95457e661.png)

如上图中，发送方收到了三次同样的ACK=30的确认报文，于是就会触发快速重发机制，通过SACK信息发现只有`30~39`这段数据丢失，于是重发时，就只选择了这个`30~39`的TCP报文段进行重发。

#### 50.4 重复SACK（D-SACK）

> ★
>
> D-SACK，英文是Duplicate SACK，是在SACK的基础上做了一些扩展，主要用来告诉发送方，有哪些数据包，自己重复接受了。DSACK的目的是帮助发送方判断，是否发生了包失序、ACK丢失、包重复或伪重传。让TCP可以更好的做网络流控。来看个图吧：
>
> ”

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/MBXY-CR-597f59d6bb0af8567da78be599e27ddc.png)

### [连接一个 IP 不存在的主机时，握手过程是怎样的？](https://mp.weixin.qq.com/s/Czv0CxDKqr2QNItO4aNZMA)

连一个 IP 不存在的主机时

- 如果IP在局域网内，会发送N次ARP请求获得目的主机的MAC地址，同时不能发出TCP握手消息。
- 如果IP在局域网外，会将消息通过路由器发出，但因为最终找不到目的地，触发TCP重试流程。

连IP 地址存在但端口号不存在的主机时

- 不管目的IP是回环地址还是局域网内外的IP地址，目的主机的传输层都会在收到握手消息后，发现端口不正确，发出RST消息断开连接。
- 当然如果目的机器设置了防火墙策略，限制他人将消息发到不对外暴露的端口，那么这种情况，发送端就会不断重试第一次握手。

最后留个问题，连一个 **不存在的局域网外IP**的主机时，我们可以看到TCP的重发规律是：开始时，每隔1s重发五次 `TCP SYN`消息，接着2s,4s,8s,16s,32s都重发一次；

对比连一个 **不存在的局域网内IP**的主机时，却是每隔1s重发了4次`ARP请求`，接着过了32s后才再发出一次ARP请求。已知ARP请求是没有重传机制的，它的重试就是TCP重试触发的，但两者规律不一致，是为什么？

### 代码执行send成功后，数据就发出去了吗？可以立马发送吗

#### Socket 缓冲区

##### 什么是 socket 缓冲区

编程的时候，如果要跟某个IP建立连接，我们需要调用操作系统提供的 `socket API`。

**socket 在操作系统层面，可以理解为一个文件**

我们可以对这个文件进行一些**方法操作**。

用`listen`方法，可以让程序作为服务器**监听**其他客户端的连接。

用`connect`，可以作为客户端**连接**服务器。

用`send`或`write`可以**发送**数据，`recv`或`read`可以**接收**数据。

在建立好连接之后，这个 **socket** 文件就像是远端机器的 **"代理人"** 一样。比如，如果我们想给远端服务发点什么东西，那就只需要对这个文件执行写操作就行了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjvRd2M0FiaaT9nWzLibKUflPASdGv9ZLtxEQlBu8zKrUmPbibnC6CGFcVA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那写到了这个文件之后，剩下的发送工作自然就是由操作系统**内核**来完成了。

既然是写给操作系统，那操作系统就需要**提供一个地方给用户写**。同理，接收消息也是一样。

这个地方就是 **socket 缓冲区**。

用户**发送**消息的时候写给 send buffer（发送缓冲区）

用户**接收**消息的时候写给 recv buffer（接收缓冲区）

也就是说**一个socket ，会带有两个缓冲区**，一个用于发送，一个用于接收。因为这是个先进先出的结构，有时候也叫它们**发送、接收队列**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjicVh0e15ib4rx1N0V9ic5eWtXKp8GZAefOYONT4KKfUhcAkqJvph7CNfQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 怎么观察 socket 缓冲区

如果想要查看 socket 缓冲区，可以在linux环境下执行 `netstat -nt` 命令。

```
# netstat -nt
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0     60 172.22.66.69:22         122.14.220.252:59889    ESTABLISHED
```

这上面表明了，这里有一个协议（Proto）类型为 TCP 的连接，同时还有本地（Local Address）和远端（Foreign Address）的IP信息，状态（State）是已连接。

还有**Send-Q 是发送缓冲区**，下面的数字60是指，当前还有60 Byte在发送缓冲区中未发送。而 **Recv-Q 代表接收缓冲区**， 此时是空的，数据都被应用进程接收干净了。

#### TCP部分

我们在使用TCP建立连接之后，一般会使用 send 发送数据。

```
int main(int argc, char *argv[])
{
    // 创建socket
    sockfd=socket(AF_INET,SOCK_STREAM, 0))

    // 建立连接  
    connect(sockfd, 服务器ip信息, sizeof(server))  

    // 执行 send 发送消息
    send(sockfd,str,sizeof(str),0))  

    // 关闭 socket
    close(sockfd);

    return 0;
}
```

上面是一段伪代码，仅用于展示大概逻辑，我们在建立好连接后，一般会在代码中执行 `send` 方法。那么此时，消息就会被立刻发到对端机器吗？

##### 执行 send 发送的字节，会立马发送吗？

答案是不确定！执行 send 之后，数据只是拷贝到了socket 缓冲区。至 什么时候会发数据，发多少数据，全听操作系统安排。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjqxb2RZpP3PEu9PpjaALP2ic37KzVtytibmQ9uRHRxdaseKHHkxxtWDLA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在用户进程中，程序通过操作 socket 会从用户态进入内核态，而 send方法会将数据一路传到传输层。在识别到是 TCP协议后，会调用 tcp_sendmsg 方法。

```
// net/ipv4/tcp.c
// 以下省略了大量逻辑
int tcp_sendmsg()
{  
  // 如果还有可以放数据的空间
  if (skb_availroom(skb) > 0) {
    // 尝试拷贝待发送数据到发送缓冲区
    err = skb_add_data_nocache(sk, skb, from, copy);
  }  
  // 下面是尝试发送的逻辑代码,先省略     
}
```

在 tcp_sendmsg 中， **核心工作就是将待发送的数据组织按照先后顺序放入到发送缓冲区中， 然后根据实际情况（比如拥塞窗口等）判断是否要发数据。如果不发送数据，那么此时直接返回**。

##### 如果缓冲区满了会怎么办

前面提到的情况里是，发送缓冲区有足够的空间，可以用于拷贝待发送数据。

###### 如果发送缓冲区空间不足，或者满了，执行发送，会怎么样？

这里分两种情况。

首先，socket在创建的时候，是可以设置是**阻塞**的还是**非阻塞**的。

```
int s = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, IPPROTO_TCP);
```

比如通过上面的代码，就可以将 `socket` 设置为**非阻塞** （`SOCK_NONBLOCK`）。

当发送缓冲区**满了**，如果还向socket执行send

- 如果此时 socket 是阻塞的，那么程序会在那**干等、死等**，直到释放出新的缓存空间，就继续把数据拷进去，然后**返回**。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjeg5UOCtVEU0TC9eNeSybzlfmKzZDmo3sBBOsOt8ibydIEBQMt3W4QCw/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

- 如果此时 socket 是非阻塞的，程序就会**立刻返回**一个 `EAGAIN` 错误信息，意思是  `Try again` , 现在缓冲区满了，你也别等了，待会再试一次。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjWL1ptVibTnKOsMc7lCQEgWLUhZX3VBzMYHPPvmcrZyAt1RheRhcZCFQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

我们可以简单看下源码是怎么实现的。还是回到刚才的 `tcp_sendmsg` 发送方法中。

```
int tcp_sendmsg()
{  
  if (skb_availroom(skb) > 0) {
    // ..如果有足够缓冲区就执行balabla
  } else {
    // 如果发送缓冲区没空间了，那就等到有空间，至于等的方式，分阻塞和非阻塞
    if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
        goto do_error;
  }   
}        
```

里面提到的  `sk_stream_wait_memory` 会根据`socket`是否阻塞来决定是一直等等一会就返回。

```
int sk_stream_wait_memory(struct sock *sk, long *timeo_p)
{
    while (1) {
    // 非阻塞模式时，会等到超时返回 EAGAIN
        if (等待超时))
            return -EAGAIN;     
     // 阻塞等待时，会等到发送缓冲区有足够的空间了，才跳出
        if (sk_stream_memory_free(sk) && !vm_wait)
            break;
    }
    return err;
}
```

###### 如果接收缓冲区为空，执行 recv 会怎么样？

接收缓冲区也是类似的情况。

当接收缓冲区**为空**，如果还向socket执行 recv

- 如果此时 socket 是阻塞的，那么程序会在那**干等**，直到接收缓冲区有数据，就会把数据从接收缓冲区拷贝到用户缓冲区，然后**返回**。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjSJukYkMGeUviaGDLIaCk6oZ8J3VqPcu61Cq2tq6xiaWxWgtbQwrm7XDg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

- 如果此时 socket 是非阻塞的，程序就会**立刻返回**一个 `EAGAIN` 错误信息。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjNLGjIFwxJuPicnESic7QGicYc7y4Zib4YANHNl7icNzbtyLK9Ja6ElydPwA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

下面用一张图汇总一下，方便大家保存面试的时候用哈哈哈。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsj9hSlrmT2jGibWZW7blcOgJxZQX7BznAAGxU6AX6nn6vDto9rsDvLpEA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### UDP接收缓冲区

**每个UDP socket都有一个接收缓冲区，没有发送缓冲区**，从概念上来说就是只要有数据就发，不管对方是否可以正确接收，所以不缓冲，不需要发送缓冲区。

UDP：当套接口接收缓冲区满时，新来的数据报无法进入接收缓冲区，**此数据报就被丢弃**。UDP是没有流量控制的；快的发送者可以很容易地就淹没慢的接收者，导致接收方的UDP丢弃数据报。

### 如果socket缓冲区还有数据，执行close了，会怎么样？

首先我们要知道，**一般正常情况下，发送缓冲区和接收缓冲区 都应该是空的。**

**如果发送、接收缓冲区长时间非空，说明有数据堆积**，这往往是由于一些网络问题或用户应用层问题，导致数据没有正常处理。

那么正常情况下，**如果 `socket` 缓冲区为空，执行 `close`。就会触发四次挥手**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjSfT17HEmqQ3YnuSic8aL9XJhGOEgdXkB14txCYzPSqjR8NDu0up6nRA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这个也是面试老八股文内容了，**这里我们只需要关注第一次挥手，发的是 `FIN` 就够了**。



#### 如果接收缓冲区有数据时，执行close了，会怎么样？

`socket close` 时，主要的逻辑在 `tcp_close()` 里实现。

先说结论，关闭过程主要有两种情况：

- **如果接收缓冲区还有数据未读，会先把接收缓冲区的数据清空**，然后给对端**发一个RST（重建连接）**。
- 如果接收缓冲区是空的，那么就调用 `tcp_send_fin()` **开始进行四次挥手过程的第一次挥手**。

```
void tcp_close(struct sock *sk, long timeout)
{
  // 如果接收缓冲区有数据，那么清空数据
    while ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {
        u32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq -
              tcp_hdr(skb)->fin;
        data_was_unread += len;
        __kfree_skb(skb);
    }

   if (data_was_unread) {
    // 如果接收缓冲区的数据被清空了，发 RST
        tcp_send_active_reset(sk, sk->sk_allocation);
     } else if (tcp_close_state(sk)) {
    // 正常四次挥手， 发 FIN
        tcp_send_fin(sk);
    }
    // 等待关闭
    sk_stream_wait_close(sk, timeout);
}
```

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjjP4WDVfnF2bmf32kZ6icQHfOa8OibmS5MZG9TFMQ96GNltYibOFDGFTkg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)



#### 如果发送缓冲区有数据时，执行close了，会怎么样？

以前以为，这种情况下，内核会把发送缓冲区数据清空，然后四次挥手。

但是发现源码**并不是这样的**。

```
void tcp_send_fin(struct sock *sk)
{
  // 获得发送缓冲区的最后一块数据
    struct sk_buff *skb, *tskb = tcp_write_queue_tail(sk);
    struct tcp_sock *tp = tcp_sk(sk);

  // 如果发送缓冲区还有数据
    if (tskb && (tcp_send_head(sk) || sk_under_memory_pressure(sk))) {
        TCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN; // 把最后一块数据值为 FIN 
        TCP_SKB_CB(tskb)->end_seq++;
        tp->write_seq++;
    }  else {
    // 发送缓冲区没有数据，就造一个FIN包
  }
  // 发送数据
    __tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);
}
```

此时，**还有些数据没发出去，内核会把发送缓冲区最后一个数据块拿出来。然后置为 FIN**。

`socket` 缓冲区是个**先进先出**的队列，这种情况是指内核会等待TCP层安静把发送缓冲区数据都发完，最后再执行 **四次挥手的第一次挥手（FIN包）**。

有一点需要注意的是，**只有在接收缓冲区为空的前提下，我们才有可能走到 `tcp_send_fin()`** 。而只有在进入了这个方法之后，我们**才有可能考虑发送缓冲区是否为空**的场景。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/FmVWPHrDdnlQB0CrBiboEZGfE8keHGEsjWnK0M8ibXAAxOl5mudeZcVDYZU0icOfn4WcCME5iaHBdXib3oqZH7kyic1g/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

#### 总结

1. 考虑接受缓冲区是否有数据，如果有数据，会先把接收缓冲区的数据清空，然后给对端发一个RST（重建连接），服务端收到RST包后，不会回复ACK，服务器也会把发送队列中的所有数据都丢弃掉。（因为发送完RST报文后，这条tcp连接就关闭了，也就没有必要确认了）
2. 考虑发送缓冲区是否有数据，若存在数据，内核会把发送缓冲区最后一个数据块拿出来。然后置为 FIN。
3. 然后开始第一次挥手

### 服务端没有 listen，客户端发起连接建立，会发生什么？

[参考](https://xiaolincoding.com/network/3_tcp/tcp_no_listen.html#%E6%B2%A1%E6%9C%89-listen-%E8%83%BD%E5%BB%BA%E7%AB%8B-tcp-%E8%BF%9E%E6%8E%A5%E5%90%97)

**服务端如果只 bind 了 IP 地址和端口，而没有调用 listen 的话，然后客户端对服务端发起了连接建立，服务端会回 RST 报文。**

### 没有 listen，能建立 TCP 连接吗？

**是可以的，客户端是可以自己连自己的形成连接（TCP自连接），也可以两个客户端同时向对方发出请求建立连接（TCP同时打开），这两个情况都有个共同点，就是没有服务端参与，也就是没有listen，就能建立连接**。

> 那没有listen，为什么还能建立连接？

我们知道执行 listen 方法时，会创建半连接队列和全连接队列。

三次握手的过程中会在这两个队列中暂存连接信息。

所以形成连接，前提是你得有个地方存放着，方便握手的时候能根据 IP + 端口等信息找到对应的 socket。

> 那么客户端会有半连接队列吗？

显然没有，因为客户端没有执行listen，因为半连接队列和全连接队列都是在执行 listen 方法时，内核自动创建的。

但内核还有个全局 hash 表，可以用于存放 sock 连接的信息。

这个全局 hash 表其实还细分为 ehash，bhash和listen_hash等，但因为过于细节，大家理解成有一个全局 hash 就够了，

**在 TCP 自连接的情况中，客户端在 connect 方法时，最后会将自己的连接信息放入到这个全局 hash 表中，然后将信息发出，消息在经过回环地址重新回到 TCP 传输层的时候，就会根据 IP + 端口信息，再一次从这个全局 hash 中取出信息。于是握手包一来一回，最后成功建立连接**。

TCP 同时打开的情况也类似，只不过从一个客户端变成了两个客户端而已。

### 没有accept，能建立TCP连接吗？accept函数的作用

[参考](https://mp.weixin.qq.com/s/n17NjGRab1u5eXkOCro1gg)

<img src="https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YAnibQeDqpw5Q4DgXX0d1D2DnLD5YYI9DvxlZEj0wy6wv0MDGOZIsEUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

建立连接的过程中根本不需要`accept()` 参与， **执行accept()只是为了从全连接队列里取出一条连接。**

### 服务端**端口未监听**时，客户端尝试去连接

服务端也会回一个RST。这两个情况长一样，所以客户端这时候收到RST之后，其实无法区分到底是**端口未监听**，还是**全连接队列满了**。

<img src="https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1Ya0KGVP0ADmx4ZMjOJDsYSVkiboZy8UYgqn6j6bKcYBjMp8ZxwdcexGw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />



#### 三次握手的细节分析

服务端代码，对socket执行bind方法可以绑定监听端口，然后执行`listen方法`后，就会进入监听（`LISTEN`）状态。内核会为每一个处于`LISTEN`状态的`socket` 分配两个队列，分别叫**半连接队列和全连接队列**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YgGZgiaWDm3uEpypj0w5TzhUicFFHulfibictvibVVAmaqpYiaPeUL252zqog/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 半连接队列、全连接队列是什么

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YAnibQeDqpw5Q4DgXX0d1D2DnLD5YYI9DvxlZEj0wy6wv0MDGOZIsEUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **半连接队列（SYN队列）**，服务端收到**第一次握手**后，会将`sock`加入到这个队列中，队列内的`sock`都处于`SYN_RECV` 状态。
- **全连接队列（ACCEPT队列）**，在服务端收到**第三次握手**后，会将半连接队列的`sock`取出，放到全连接队列中。队列里的`sock`都处于 `ESTABLISHED`状态。这里面的连接，就**等着服务端执行accept()后被取出了。**

建立连接的过程中根本不需要`accept()` 参与， **执行accept()只是为了从全连接队列里取出一条连接。**

**连接队列（icsk_accept_queue）是个链表**，而**半连接队列（syn_table）是个哈希表**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIiakVbS0pYlxaUca5m4AQuG1YzgqRF1t0O2iae5bmy8xHSAIGz1KXuwickPyFHs7lFvyzLSBpYd6QdKag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

总结

- **每一个**`socket`执行`listen`时，内核都会自动创建一个半连接队列和全连接队列。
- 第三次握手前，TCP连接会放在半连接队列中，直到第三次握手到来，才会被放到全连接队列中。
- `accept方法`只是为了从全连接队列中拿出一条连接，本身跟三次握手几乎**毫无关系**。
- 出于效率考虑，虽然都叫队列，但半连接队列其实被设计成了**哈希表**，而全连接队列本质是链表。
- **全连接队列满了，再来第三次握手也会丢弃**，此时如果`tcp_abort_on_overflow=1`，还会**直接发`RST`给客户端**。
- 半连接队列满了，可能是因为受到了`SYN Flood`攻击，可以设置`tcp_syncookies`（在第二次握手中加入cookie），绕开半连接队列。
- 客户端没有半连接队列和全连接队列，但有一个**全局hash**，可以通过它实现自连接或TCP同时打开。

### client走完第三步就发送请求，但server还没有调用accept，会发生什么？

client认为连接建立成功，但是server上这个连接实际没有ready，所以server没有回复，一段时间后client认为丢包了然后重传这个包，一直到超时，client主动发fin包断开该连接。

### TCP 和 UDP 可以同时绑定相同的端口吗？

[参考](https://xiaolincoding.com/network/3_tcp/port.html#tcp-%E5%92%8C-udp-%E5%8F%AF%E4%BB%A5%E5%90%8C%E6%97%B6%E7%BB%91%E5%AE%9A%E7%9B%B8%E5%90%8C%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%90%97)

TCP 和 UDP 服务端网络相似的一个地方，就是会调用 bind 绑定端口。

给大家贴一下 TCP 和 UDP 网络编程的区别就知道了。

**TCP 网络编程**如下，服务端执行 listen() 系统调用就是**监听端口的动作**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B.png" alt="img" style="zoom:50%;" />

**UDP 网络编程**如下，服务端是**没有监听这个动作**的，只有执行 bind() 系统调用来绑定端口的动作。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/udp%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B.png" alt="img" style="zoom:67%;" />

> TCP 和 UDP 可以同时绑定相同的端口吗？

答案：**可以的**。

在数据链路层中，通过 MAC 地址来寻找局域网中的主机。在网际层中，通过 IP 地址来寻找网络中互连的主机或路由器。在传输层中，需要通过端口进行寻址，来识别同一计算机中同时通信的不同应用程序。

所以，传输层的「端口号」的作用，是为了区分同一个主机上不同应用程序的数据包。

传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。

当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/network/port/tcp%E5%92%8Cudp%E6%A8%A1%E5%9D%97.jpeg" alt="img" style="zoom:50%;" />

因此， TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。

###  多个 TCP 服务进程可以绑定同一个端口吗？

**如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”**。

注意，如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。

这是因为 0.0.0.0 地址比较特殊，代表任意地址，意味着绑定了 0.0.0.0 地址，相当于把主机上的所有 IP 地址都绑定了。

### RST介绍

[参考](https://www.jianshu.com/p/3f557a7351a3)、[参考](https://blog.51cto.com/u_15127633/4818444)

RST：**重置连接、复位连接，用来关闭异常的连接**。

1. 发送RST包关闭连接时，不必等缓冲区的包都发出去，直接就丢弃缓冲区中的包，发送RST。
2. 而接收端收到RST包后，也不必发送ACK包来确认。

**使用场景**：在连接建立时、在中间发送数据时、在连接关闭时

- **连接建立时**出现RST回应的情况有：**发送一个不存在的端口**，想一个Listen的套接字或者端口上**发送数据分节**（不是创建连接的分节）
- **发送数据时**出现RST分节的情况有：**数据出现错误、不是按照seq要求来发送数据时**
- **连接终止时**发送RST分节的情况有：如果在关闭的时候设置了套接字选项，l_linger，那么在关闭的时候可能会出现RST分节

#### 二、什么时候发送RST包

1. 建立连接的SYN到达某端口，但是该端口上**没有正在监听的服务**。
2. TCP收到了一个**根本不存在的连接**上的分节。
3. **请求超时**。 使用setsockopt的SO_RCVTIMEO选项设置recv的超时时间。**接收数据超时时，会发送RST包**。

#### 三、尝试手动发送RST包

1. 使用shutdown、close关闭套接字，发送的是FIN，不是RST。
2. 套接字关闭前，使用sleep。对运行的程序Ctrl+C，会发送FIN，不是RST。
3. 套接字关闭前，执行return、exit(0)、exit(1)，会发送FIN、不是RST。

以上几种方法，都不能发送RST包。 发送RST包，需要自己伪造数据包进行发送。

### select，poll和epoll的区别

[参考](https://developer.aliyun.com/article/763247)

select，poll和epoll其实都是操作系统中IO多路复用实现的方法。

#### **select**

**select方法本质其实就是维护了一个文件描述符（fd）数组**，以此为基础，实现IO多路复用的功能。这个fd数组有长度限制，在32位系统中，最大值为1024个，而在64位系统中，最大值为2048个。

select方法被调用，**首先需要将fd_set从用户空间拷贝到内核空间，然后内核用poll机制**（此poll机制非IO多路复用的那个poll方法）**直到有一个fd活跃，或者超时了，方法返回**。

- 如果返回值为-1，表明发生了错误
- 如果返回值为0，表明超时了
- 如果返回值为正数，表明有n个fd准备就绪了

**select方法返回后，需要轮询fd_set，以检查出发生IO事件的fd**。这样一套下来，select方法的缺点就很明显了：

- fd_set在用户空间和内核空间的频繁复制，效率低
- 单个进程可监控的fd数量有限制，无论是1024还是2048，对于很多情景来说都是不够用的。
- 基于轮询来实现，效率低

#### **poll**

poll本质上和select没有区别，依然需要进行数据结构的复制，**依然是基于轮询来实现**，但区别就是，select使用的是fd数组，而**poll则是维护了一个链表**，所以从理论上，poll方法中，单个进程能监听的fd不再有数量限制。但是轮询，复制等select存在的问题，poll依然存在

#### **epoll**

epoll就是对select和poll的改进了。它的核心思想是**基于事件驱动来实现的**，实现起来也并不难，就是**给每个fd注册一个回调函数，当fd对应的设备发生IO事件时，就会调用这个回调函数，将该fd放到一个链表中，然后由客户端从该链表中取出一个个fd，以此达到O（1）的时间复杂度**

epoll操作实际上对应着有三个函数：epoll_create，epoll_ctr，epoll_wait

**epoll_create**

epoll_create相当于在内核中创建一个存放fd的数据结构。在select和poll方法中，内核都没有为fd准备存放其的数据结构，只是简单粗暴地把数组或者链表复制进来；而epoll则不一样，epoll_create会在内核建立一颗专门用来存放fd结点的红黑树，后续如果有新增的fd结点，都会注册到这个epoll红黑树上。

**epoll_ctr**

另一点不一样的是，select和poll会一次性将监听的所有fd都复制到内核中，而epoll不一样，当需要添加一个新的fd时，会调用epoll_ctr，给这个fd注册一个回调函数，然后将该fd结点注册到内核中的红黑树中。当该fd对应的设备活跃时，会调用该fd上的回调函数，将该结点存放在一个就绪链表中。这也解决了在内核空间和用户空间之间进行来回复制的问题。

**epoll_wait**

epoll_wait的做法也很简单，其实直接就是从就绪链表中取结点，这也解决了轮询的问题，时间复杂度变成O(1)

所以综合来说，epoll的优点有：

- 没有最大并发连接的限制，远远比1024或者2048要大。（江湖传言1G的内存上能监听10W个端口）
- 效率变高。epoll是基于事件驱动实现的，不会随着fd数量上升而效率下降
- 减少内存拷贝的次数

**水平触发和边缘触发**

简单理解下

水平触发的意思就是说，只要条件满足，对应的事件就会一直被触发。所以如果条件满足了但未进行处理，那么就会一直被通知

边缘触发的意思就是说，条件满足后，对应的事件只会被触发一次，无论是否被处理，都只会触发一次。

而对于select和poll来说，其触发都是水平触发。而epoll则有两种模式：·EPOLLLT和EPOLLET

- EPOLLLT（默认状态）：也就是水平触发。在该模式下，只要这个fd还有数据可读，那么epoll_wait函数就会返回该fd
- EPOLLET（高速模式）：也就是边缘触发。在该模式下，当被监控的fd上有可读写事件发生时，epoll_wait会通知程序去读写，若本次读写没有读完所有数据，或者甚至没有进行处理，那么下一次调用epoll_wait时，也不会获取到该fd。这种效率比水平触发的要高，系统中不会充斥着大量程序不感兴趣的fd，不感兴趣直接忽视就行，下次不会再触发

#### **总结**

- **select，poll是基于轮询实现的**，将fd_set从用户空间复制到内核空间，然后让内核空间以poll机制来进行轮询，一旦有其中一个fd对应的设备活跃了，那么就把整个fd_set返回给客户端（复制到用户空间），再由客户端来轮询每个fd的，找出发生了IO事件的fd
- **epoll是基于事件驱动实现的**，加入一个新的fd，会调用epoll_ctr函数为该fd注册一个回调函数，然后将该fd结点注册到内核中的epoll红黑树中，当IO事件发生时，就会调用回调函数，将该fd结点放到就绪链表中，epoll_wait函数实际上就是从这个就绪链表中获取这些fd。
- epoll分为EPOLLLT（**水平触发**，默认状态）和EPOLLET（**边缘触发**，效率高）
- 并不是所有的情况中epoll都是最好的，比如当fd数量比较小的时候，epoll不见得就一定比select和poll好

### 断网了，还能ping通 127.0.0.1 吗？为什么？

[参考](https://mp.weixin.qq.com/s?__biz=Mzg5NDY2MDk4Mw==&mid=2247486417&idx=1&sn=c648ca9f2d33f77d69ae3c615c62b77e&scene=21#wechat_redirect)

**能ping通的**

#### 什么是127.0.0.1

首先，这是个 `IPV4` 地址。

`IPV4` 地址有 `32` 位，一个字节有 `8` 位，共 `4` 个字节。

其中**127 开头的都属于回环地址**，也是 `IPV4` 的特殊地址，没什么道理，就是人为规定的。

而`127.0.0.1`是**众多**回环地址中的一个。之所以不是 `127.0.0.2` ，而是 `127.0.0.1`，是因为源码里就是这么定义的，也没什么道理。

```
/* Address to loopback in software to local host.  */
#define    INADDR_LOOPBACK     0x7f000001  /* 127.0.0.1   */
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3al814qA9PQccVHwXclglX7Vm2Ro1aN6rj2Mzom2WOPB9Nh5tOiaZeshg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`IPv4` 的地址是 `32` 位的，2的32次方，大概是`40+亿`。地球光人口就76亿了，40亿IP这点量，**塞牙缝都不够**，实际上**IP也确实用完**了。

所以就有了`IPV6`， `IPv6` 的地址是 `128` 位的，大概是2的128次方≈**10的38次方**。据说地球的沙子数量大概是 **10的23次方**，所以IPV6的IP可以认为用不完。

IPV4以8位一组，每组之间用 **.** 号隔开。

IPV6就以16位为一组，每组之间用 **:** 号隔开。如果全是0，那么可以省略不写。

![图片](https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3aDribNWH3v6rqs4qJgTpg835BwUW1Eg2NwJBjEbNMhQpjeSfdlN1gSjw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在IPV4下的回环地址是 `127.0.0.1`，在`IPV6`下，表达为 `::1` 。中间把**连续的0**给省略了，之所以不是**7个 冒号**，而是**2个冒号:** ， 是因为一个 IPV6 地址中**只允许出现⼀次两个连续的冒号**。

> 多说一句：在IPV4下用的是 **ping 127.0.0.1** 命令。在IPV6下用的是 **ping6  ::1** 命令。

#### 什么是 ping

**ping 是应用层命令**，可以理解为它跟游戏或者聊天软件属于同一层。只不过聊天软件可以收发消息，还能点个赞什么的，有很多复杂的功能。而 ping 作为一个小软件，它的功能比较简单，就是**尝试**发送一个小小的消息到目标机器上，判断目的机器是否**可达**，其实也就是判断目标机器网络是否能连通。

ping应用的底层，用的是网络层的**ICMP协议**。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3abT1NHNLtfz0KV8jBOVDFtaEb7ZqpQ48XEzZ1eAnbXrDXmXPuMelJ6A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

虽然ICMP协议和IP协议**都属于网络层协议**，但其实**ICMP也是利用了IP协议进行消息的传输**。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3anicOgzO4NDtrmN7NTibbR3Ze5DUzjDLDEDStBX0WL4A9iaL0h5gq28Xqw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

所以，大家在这里完全可以简单的理解为 ping 某个IP 就是往某个IP地址发个消息。

#### TCP发数据和ping的区别

一般情况下，我们会使用 TCP 进行网络数据传输，那么我们可以看下它和 ping 的区别。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3aTZVia58LjFhfprOVOg8y4WclVdG4Y7tD8IGhPQia65nPey5TDWARgGNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

ping和其他应用层软件都属于**应用层**。

那么我们横向对比一下，比方说聊天软件，如果用的是TCP的方式去发送消息。

为了发送消息，那就得先知道往哪发。linux里万物皆文件，那你要发消息的目的地，也是个文件，这里就引出了socket 的概念。

要使用 `socket` , 那么首先需要创建它。

在 TCP 传输中创建的方式是  `socket(AF_INET, SOCK_STREAM, 0);`，其中 `AF_INET` 表示将使用 IPV4 里 **host:port** 的方式去解析待会你输入的网络地址。`SOCK_STREAM` 是指使用面向字节流的 TCP 协议，**工作在传输层**。

创建好了 `socket` 之后，就可以愉快的把要传输的数据写到这个文件里。调用 socket 的`sendto`接口的过程中进程会从**用户态进入到内核态**，最后会调用到 `sock_sendmsg` 方法。

然后进入传输层，带上`TCP`头。网络层带上`IP`头，数据链路层带上 `MAC`头等一系列操作后。进入网卡的**发送队列 ring buffer** ，顺着网卡就发出去了。

回到 `ping` ， 整个过程也基本跟 `TCP` 发数据类似，差异的地方主要在于，创建 `socket` 的时候用的是  `socket(AF_INET,SOCK_RAW,IPPROTO_ICMP)`，`SOCK_RAW` 是原始套接字 ，**工作在网络层**， 所以构建`ICMP`（网络层协议）的数据，是再合适不过了。ping 在进入内核态后最后也是调用的  `sock_sendmsg` 方法，进入到网络层后加上**ICMP和IP头**后，数据链路层加上**MAC头**，也是顺着网卡发出。因此 **本质上ping 跟 普通应用发消息 在程序流程上没太大差别**。

这也解释了**为什么当你发现怀疑网络有问题的时候，别人第一时间是问你能ping通吗？**因为可以简单理解为ping就是自己组了个数据包，让系统按着其他软件发送数据的路径往外发一遍，能通的话说明其他软件发的数据也能通。

#### 为什么断网了还能 ping 通 127.0.0.1

前面提到，有网的情况下，ping 最后是**通过网卡**将数据发送出去的。

那么断网的情况下，网卡已经不工作了，ping 回环地址却一切正常，我们可以看下这种情况下的工作原理。

<img src="https://mmbiz.qpic.cn/mmbiz_png/FmVWPHrDdnkr3eLdxxIK0eujAOibyGS3a3njA3Ysh4YWH7owWreIy81oOqpAr5vOfdp5QkPjPIiarZ5Tkg7MZUYQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

从应用层到传输层再到网络层。这段路径跟ping外网的时候是几乎是一样的。到了网络层，系统会根据目的IP，在路由表中获取对应的**路由信息**，而这其中就包含选择**哪个网卡**把消息发出。

当发现**目标IP是外网IP**时，会从"真网卡"发出。

当发现**目标IP是回环地址**时，就会选择**本地网卡**。

本地网卡，其实就是个**"假网卡"**，它不像"真网卡"那样有个`ring buffer`什么的，"假网卡"会把数据推到一个叫 `input_pkt_queue` 的 **链表** 中。这个链表，其实是所有网卡共享的，上面挂着发给本机的各种消息。消息被发送到这个链表后，会再触发一个**软中断**。

专门处理软中断的工具人**"ksoftirqd"** （这是个**内核线程**），它在收到软中断后就会立马去链表里把消息取出，然后顺着数据链路层、网络层等层层往上传递最后给到应用程序。

ping 回环地址和**通过TCP等各种协议发送数据到回环地址**都是走这条路径。整条路径从发到收，都没有经过"真网卡"。**之所以127.0.0.1叫本地回环地址，可以理解为，消息发出到这个地址上的话，就不会出网络，在本机打个转就又回来了。**所以断网，依然能 `ping` 通 `127.0.0.1`。

#### ping回环地址和ping本机地址有什么区别

我们在mac里执行 `ifconfig` 。

```
$ ifconfig
lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> mtu 16384
    inet 127.0.0.1 netmask 0xff000000
    ...
en0: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
    inet 192.168.31.6 netmask 0xffffff00 broadcast 192.168.31.255
    ...
```

能看到 **lo0**，表示本地回环接口，对应的地址，就是我们前面提到的 **127.0.0.1** ，也就是**回环地址**。

和 **eth0**，表示本机第一块网卡，对应的IP地址是**192.168.31.6**，管它叫**本机IP**。

之前一直认为ping本机IP的话会通过"真网卡"出去，然后遇到第一个路由器，再发回来到本机。

可以看到 **ping 本机IP 跟 ping 回环地址一样，相关的网络数据，都是走的  lo0，本地回环接口**，也就是前面提到的**"假网卡"**。

只要走了本地回环接口，那**数据都不会发送到网络**中，在本机网络协议栈中兜一圈，就发回来了。因此 **ping回环地址和ping本机地址没有区别**。

#### 127.0.0.1 和 localhost 以及 0.0.0.0 有区别吗

本质上还是有些区别的。

首先 `localhost` 就不叫 `IP`，它是一个**域名**，就跟 `"baidu.com"`,是一个形式的东西，只不过默认会把它解析为 `127.0.0.1` ，当然这可以在 `/etc/hosts` 文件下进行修改。

所以默认情况下，使用 `localhost` 跟使用  `127.0.0.1` 确实是没区别的。

其次就是 `0.0.0.0`，执行 ping 0.0.0.0  ，是会失败的，因为它在`IPV4`中表示的是无效的**目标地址**。

```
$ ping 0.0.0.0
PING 0.0.0.0 (0.0.0.0): 56 data bytes
ping: sendto: No route to host
ping: sendto: No route to host
```

但它还是很有用处的，回想下，我们启动服务器的时候，一般会 `listen` 一个 IP 和端口，等待客户端的连接。

如果此时 `listen` 的是本机的 `0.0.0.0` , 那么它表示**本机上的所有IPV4地址**。

```
/* Address to accept any incoming messages. */
#define    INADDR_ANY      ((unsigned long int) 0x00000000) /* 0.0.0.0   */
```

举个例子。刚刚提到的 `127.0.0.1` 和 `192.168.31.6` ，都是本机的IPV4地址，如果监听 `0.0.0.0` ，那么用上面两个地址，都能访问到这个服务器。

当然， 客户端 `connect` 时，不能使用 `0.0.0.0` 。必须指明要连接哪个服务器IP。

#### 总结

- `127.0.0.1` 是**回环地址**。`localhost`是**域名**，但默认等于 `127.0.0.1`。
- `ping` 回环地址和 `ping` 本机地址，是一样的，走的是**lo0 "假网卡"**，都会经过网络层和数据链路层等逻辑，最后在快要出网卡前**狠狠拐了个弯**， 将数据插入到一个**链表**后就**软中断**通知 **ksoftirqd** 来进行**收数据**的逻辑，**压根就不出网络**。所以断网了也能 `ping` 通回环地址。
- 如果服务器 `listen` 的是 `0.0.0.0`，那么此时用`127.0.0.1`和本机地址**都可以**访问到服务。

### 能ping通，TCP就一定能连通吗？

[参考](https://mp.weixin.qq.com/s/fb2uUWz5ZjPEfYv_l6e4Zg)

换个问法，**ping和tcp协议走的网络路径是一样的吗？**

**不一定，走的网络路径还是有可能是不同的**

#### 网络路径

我们将数据包从本地网卡发出之后，会经过各种**路由器（或者交换机）**，才能到达目的机器。

这些路由器数量众多，相互之间可以互连，连起来之后就像是一张大网，所以叫**"网络"**可以说是非常的形象。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFk2zMjLK9h1mYtmfyeibIqvWic9oNiaUUfKTYzaWia2sSianlLugBIeQHM1nw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那么现在问题来了，**路由器收到数据后，怎么知道应该走哪条路径，传给哪个路由器？**

##### 路由表的生成

基于**Dijkstra算法**，封装出了一个新的协议，**OSPF协议**（**O**pen **S**hortest **P**ath **F**irst, **开放最短路径优先**）。

有了OSPF，路由器就得到了网络图里自己到其他点之间的**最短距离**，于是就知道了**数据包要到某个点，该走哪条最优路径**。

将这些信息汇成一张表，也就是我们常说的**路由表**。

路由表里记录了到什么IP需要走什么端口，以及走这条路径的成本（`metric`）。

可以通过 `route` 命令查看到。

<img src="https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkXSCRGpiaRMtPEt45VFybiaA3K8icuBuQBXdYxygXq6WzGIyaLJdzJsu0Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

##### 路由表决定数据包路径

数据包在发送的过程中，会在**网络层**加入**目标地址IP**。

路由器会根据这个**IP**跟**路由表**去做匹配。

然后路由表，会告诉路由器，什么样的消息该转发到什么端口。

举个例子。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkQP6WPyyWrCIsHNGDPl3ENibFXC8JTVTps3RB6ibrs9GQRCMfrVo390zA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

假设A要发消息到D。也就是`192.168.0.105/24`要发消息到`192.168.1.11/24`。

那么A会把消息经发到路由器。

路由器已知目的地IP`192.168.1.11/24` ，去跟**路由表**做匹配，发现`192.168.1.0/24`, 就在e2端口，那么就会把消息从e2端口发出，（可能还会经过交换机）最后把消息打到目的机器。

当然，如果路由表里找不到，那就打到**默认网关**吧，也就是从e1口发出，发到IP`192.0.2.1`。**这个路由器的路由表不知道该去哪，说不定其他路由器知道**。

##### 路由表的匹配规则

上面的例子里，是只匹配上了路由表里的**一项**，所以只能是它了。

但是，条条大路通罗马。实际上能到目的地的路径肯定有很多。

**如果路由表里有很多项都被匹配上了，会怎么选？**

如果多个路由项都能到目的地，那就优先选**匹配长度更长**的那个。比如，还是目的地`192.168.1.11`，发现路由表里的**192.168.1**.0/**24** 和 **192.168**.0.0/**16**都能匹配上，但明显**前者匹配长度更长**，所以最后会走 **192.168.1**.0/**24**对应的转发端口。

**但如果两个表项的匹配长度都一样呢？**

那就会看生成这个路由表项的**协议**是啥，选优先级高的，优先级越高也就是所谓的**管理距离**（**AD**，**A**dministrative**D**istance）越小。比如说优先选**手动配**的静态（**static**）路由，次优选**OSPF**动态学习过来的表项。

如果还是相同，就看**度量值metrics**，其实也就是**路径成本cost**，成本越小，越容易被选中。

**路由器能选的路线有很多，但按道理，最优的只有"一条"，所以到这里为止，我们都可以认为，对于同一个目的地，ping和TCP走的路径是相同的。**

但是。

**如果连路径成本都一样呢？**也就是说有多条最优路径呢。

**那就都用**。

这也就是所谓的**等价多路径，ECMP**（**E**qual **C**ost **M**ulti**P**ath）。

我们可以通过`traceroute`看下链路是否存在等价多路径的情况。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFk6PzyshMT7Iiajcfib0y5HZBmpyCFhehGwrgw1Xgfevr8BD2vuNj3kelg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到，中间某几行，有**好几个IP**，也就是说这一跳里同时可以选好几个目的机器，说明这段路径**支持ECMP**。

##### ECMP有什么用

利用等价多路径，我们**可以增加链路带宽**。

举个例子。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkHZicWf61IoTIyFOibVDBiaPysIY6O8Av2ibzL5LTDJcxKn2bFQ6icZibdDGA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从A点到B点，如果这两条路径成本不同，带宽都是`1千兆`。那数据包肯定就选成本低的那条路了，如果这条路出故障了，就走下面那条路。但不管怎么样，**同一时间，只用到了一条路径**。另外一条闲置就有些浪费了，有没有办法可以利用起来呢？

有，将它们两条路径的成本设置成一样，那它们就成了等价路由，然后中间的路由器开启**ECMP**特性，就可以同时利用这两条链路了。带宽就从原来的`1千兆`变成了`2千兆`。数据就可以在两条路径中随意选择了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkVDgU7TUuVlcdrbrLQ1icbMOBEJmf7xItOX9MzSORV3HJu3X3oFwdj8w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

但这也带来了另外一个问题。**加剧了数据包乱序**。

原来我只使用一条网络路径，数据依次发出，如无意外，也是依次到达。

现在两个数据包走两条路径，先发的数据包可能后到。这就乱序了。

那么问题又又来了。

##### 乱序会有什么问题？

对于我们最最最常使用的TCP协议来说，它是个可靠性网络的协议，这里提到的**可靠**，不仅是保证数据要能送到目的地，还要保证**数据顺序**要跟原来发送端的一样。

实现也很简单，**TCP为每个数据包（segment）做上编号**。数据到了接收端后，根据**数据包编号**发现是**乱序数据包**，就会扔到**乱序队列**中对数据包进行排序。如果前面的数据包还没到，哪怕后面的数据包先到了，也得在乱序队列中一直等，到齐后才能被上层拿到。

举个例子，发送端发出三个数据包，`编号1,2,3`，假设在**传输层**`2和3`先到了，`1`还没到。那此时**应用层**是没办法拿到`2和3`的数据包的，必须得等`1`来了之后，**应用层才能一次性拿到这三个包**。因为这三个包原来可能表示的是一个完整的消息，少了1, 那么**消息就不完整**，应用层拿到了也毫无意义。

像这种，由于**前面的数据丢失**导致**后面的数据没办法及时给到应用层**的现象，就是我们常说的**TCP队头阻塞**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkPQjq4kZGqasu6YDn4gXdvaonteo06DS9pavednyW9prSxax1bKib6qw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

乱序发生时`2和3`需要待在乱序队列中，而**乱序队列其实用的也是接收缓冲区的内存**，而**接收缓冲区是有大小限制的**。通过下面的命令可以看到接收缓冲区的大小。

```
# 查看接收缓冲区
$ sysctl net.ipv4.tcp_rmem
net.ipv4.tcp_rmem = 4096(min)    87380(default)  6291456(max)
# 缓冲区会在min和max之间动态调整
```

乱序的情况越多，接收缓冲区的内存就被占用的越多，对应的**接收窗口**就会变小，那正常能收的数据就变少了，**网络吞吐就变差**了，也就是性能变差了。

因此，我们需要尽量保证所有**同一个TCP连接下的所有TCP包都走相同路径，这样才能最大程度避免丢包**。

##### ECMP的路径选择策略

**当初开启ECMP就是为了提升性能，现在反而加重了乱序，降低了TCP传输性能。**

这怎么能忍。

为了解决这个问题，我们需要有一个合理的路径选择策略。为了**避免同一个连接里的数据包乱序，我们需要保证同一个连接里的数据包，都走同样的路径**。

这好办。我们可以通过连接的**五元组**（发送方的**IP**和**端口**，接收方的**IP**和**端口**，以及通信**协议**）信息**定位到唯一一条连接**。

<img src="https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFk212iaXYEXhHRc7k9UKZibue6WbkWEb5wCEGWU5dxH1EKicicxYHLuHibBKA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

然后对五元组信息生成哈希键，让同一个哈希键的数据走同一条路径，问题就完美解决了。

<img src="https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkGD4BibKHGhSaEU13IYhw6rs9D2PpqNoHcfWWdSE4DndmEmJJHzpwxMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

<img src="https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkWPpSXI1jg7mTMVKVk73oSZhKbV7D46F9zhh7yL25KOIA6OlxHlY6ag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />



#### TCP和Ping走的网络路径一样吗

对于同样的发送端和接收端，**TCP和Ping走的网络路径一样吗？**

**不一定一样**，因为**五元组**里的信息里有一项是**通信协议**。ping用的是**ICMP协议**，跟**TCP协议**不同，并且ping不需要用到端口，所以五元组不同，生成的**哈希键不同**，通过ECMP选择到的路径也可能不同。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFklSAwDam5HZZoBeemhHwNqicg0ugEUZFVna477vujTf1dtuQX3EH0p3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



#### 同样都用TCP协议，数据包走的网络路径一样吗

还是同样的发送端和接收端，同样是TCP协议，不同TCP连接走的网络路径是一样的吗？

跟上面的问题一样，其实**还是五元组的问题**，同样都是TCP协议，对于同样的发送端和接收端，他们的IP和接收端的端口肯定是一样的，但**发送方的端口是可以随时变化**的，因此通过ECMP走的路径也可能不同。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkAH1I0qt3rajhofv5fcH3fCa9AbSmMM9fYqQTdgu4dwAybfj5UdqiaKg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

但问题又来了。

**我知道这个有什么用呢？我做业务开发，又没有设置网络路由的权限。**



#### 利用这个知识点排查问题

对于业务开发，这绝对不是个没用的知识点。

如果某天，你发现，你能ping通目的机器，但用TCP去连，却**偶尔连不上**目的机器。而且两端机器都挺空闲，没什么性能上的瓶颈。实在**走投无路**了。

你就可以想想，会不会是网络中用到了`ECMP`，其中一条链路有问题导致的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkcPmDGiaZseVYK59icYy05WdjZJnFhl6xYj1jGXBNALzdAJ4W6wUTSCLA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

排查方法也很简单。

你是知道本机的IP以及目的机器的IP和端口号的，也知道自己用的是TCP连接。

只要你在**报错的时候打印下错误信息，你就知道了发送端的端口号了。**

这样**五元组**是啥你就知道了。

下一步就是**指定发送端的端口号重新发起TCP请求，同样的五元组，走同样的路径，按理说如果链路有问题，就肯定会复现。**

如果不想改自己的代码，你可以用**nc命令指定客户端端口**看下能不能正常建立TCP连接。

```
nc -p 6666 baidu.com 80
```

`-p 6666`是指定发出请求的客户端端口是`6666`，后面跟着的是**连接的域名**和**80端口**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/AnAgeMhDIianA4RVS9XdKXZT5fmcogTFkDSvVCZe4OoUEsiausLeTnIciamJib7c0iaYd2AXjP0WQzokb6CrqttsCNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

假设用了`6666端口`的五元组去连接**总是失败**，改用`6667或其他端口`**却能成功**，你可以带着这个信息去找找负责网络的同事。



#### 总结

- 路由器可以通过OSPF协议生成路由表，利用数据包里的IP地址去跟路由表做匹配，选择最优路径后进行转发。
- 当路由表一个都匹配不上时会走默认网关。当匹配上多个的时候，会先看**匹配长度**，如果一样就看**管理距离**，还一样就看**路径成本**。如果连路径成本都一样，那**等价路径**。如果路由开启了ECMP，那就可以同时利用这几条路径做传输。
- ECMP可以提高链路带宽，同时利用五元组做哈希键进行路径选择，保证了同一条连接的数据包走同一条路径，减少了乱序的情况。
- 可以通过traceroute命令查看到链路上是否有用到ECMP的情况。
- 开启了ECMP的网络链路中，TCP和ping命令可能走的路径不同，甚至同样是TCP，不同连接之间，走的路径也不同，因此出现了连接时好时坏的问题，实在是走投无路了，可以考虑下是不是跟ECMP有关。
- 当然，**遇到问题多怀疑自己，要相信绝大部分时候真的跟ECMP无关**。

### TCP长连接和短链接的区别及应用场景

**长连接的操作步骤是**：

建立连接->数据传输…（保持连接）…数据传输->关闭连接。

**短连接的步骤是：**

建立连接->数据传输->关闭连接…建立连接->数据传输->关闭连接。

**长连接和短链接各自的优缺点：**

1、长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间，但是一直连接对于客户端来说比较耗电。

2、对于频繁请求资源的客户来说，较适用长连接。

3、客户端与服务端之间的连接如果一直不关闭的话，会存在一个问题，

4、随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致服务端服务受损；

5、如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某些的客户端连累后端服务。

6、短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。

7、一次TCP连接和断开需要7个来回，如果客户端请求频繁，将在TCP的建立和关闭操作上浪费大量时间和带宽。

**TCP长/短连接的应用场景：**

1、**长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况**。每个TCP连接都需要三次握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，再次处理时直接发送数据包就OK了，不用建立TCP连接。例如：**数据库的连接用长连接**，如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。

2、而**像WEB网站的http服务一般都用短连接**，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连接好。

### HTTP的KeepAlive和TCP的KeepAlive

[参考](https://cloud.tencent.com/developer/news/696654)

#### 什么是KeepAlive

- KeepAlive可以简单理解为一种状态保持或重用机制，比如当一条连接建立后，我们不想它立刻被关闭，如果实现了KeepAlive机制，就可以通过它来实现连接的保持
- HTTP的KeepAlive在HTTP 1.0版本默认是关闭的，但在HTTP1.1是默认开启的；操作系统里TCP的KeepAlive默认也是关闭，但一般应用都会修改设置来开启。因此网上TCP流量中基于KeepAlive的是主流
- HTTP的KeepAlive和TCP的KeepAlive有一定的依赖关系，名称又一样，因此经常被混淆，但其实是不同的东西

- HTTP的KeepAlive和TCP的KeepAlive是两个完全不同的概念

#### HTTP的KeepAlive和TCP的KeepAlive的关系

- TCP的KeepAlive是**由操作系统内核来控制**，通过 `keep-alive` 报文来防止TCP连接被对端、防火墙或其他中间设备意外中断，和上层应用没有任何关系，**只负责维护单个TCP连接的状态**，其上层应用可以复用该TCP长连接，也可以关闭该TCP长连接
- HTTP的KeepAlive机制则是和自己的业务密切相关的**，浏览器通过头部告知服务器要复用这个TCP连接**，请不要随意关闭。只有到了 `keepalive` 头部规定的 `timeout` 才会关闭该TCP连接，不过这具体依赖应用服务器，应用服务器也可以根据自己的设置在响应后主动关闭这个TCP连接，只要在响应的时候携带 `Connection: Close` 告知对方
- 所以很多时候我们可以把HTTP连接理解为TCP连接，**但HTTP KeepAlive则不能当成TCP的KeepAlive看待**
- 假设我们**不开启TCP长连接而只开启HTTP长连接，是不是HTTP的KeepAlive就不起作用了？并不是的**，此时HTTP的KeepAlive还会正常起作用，TCP连接还会被复用，但被复用的TCP连接出现故障的概率就高很多。由于没有开启TCP的KeepAlive，防火墙或负载转发服务等中间设备可能因为该TCP空闲太长而悄悄关闭该连接，当HTTP从自己的连接池拿出该TCP连接时，可能并不知道该连接被关闭，继续使用就会出现错误
- 为了减少错误，一般来说**开启HTTP的KeepAlive的应用都会开启TCP的KeepAlive**
- 默认的 `net.ipv4.tcp_keepalive_time` 为2个小时，是不是太长了？感觉太长了，2小时监测一次感觉黄花菜都凉了。我们公司F5后面的Nginx服务器配置了30分钟，但应该也是太长了吧，F5维持空闲连接5分钟，那超时监测不应该低于这个值吗 **？？？**，比如[Google Cloud](https://cloud.google.com/compute/docs/troubleshooting/general-tips#communicatewithinternet)说其防火墙允许10分钟空闲连接，因此建议 `net.ipv4.tcp_keepalive_time` 设置为6分钟
- **TCP中的keepalive是用来保鲜、保活的；HTTP中的keep-alive机制主要为了让支撑它的TCP连接活的的更久**，所以通常又叫做：HTTP persistent connection（持久连接） 和 HTTP connection reuse（连接重用）

### [TCP 四次挥手收到乱序的 FIN 包会如何处理？]()

[参考](https://www.1024sou.com/article/3015.html)、[参考](https://www.1024sou.com/article/3015.html)

**因为如果 FIN 报文比数据包先抵达客户端，此时 FIN 报文其实是一个乱序的报文，此时客户端的 TCP 连接并不会从 FIN_WAIT_2 状态转换到 TIME_WAIT 状态。**

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/v2-d684ebd029de80515aebb862737732b1_720w.jpg)

**在 FIN_WAIT_2 状态时，如果收到乱序的 FIN 报文，那么就被会加入到「乱序队列」，并不会进入到 TIME_WAIT 状态。**

等再次收到前面被网络延迟的数据包时，会判断乱序队列有没有数据，然后会检测乱序队列中是否有可用的数据，如果能在乱序队列中找到与当前报文的序列号保持的顺序的报文，就会看该报文是否有 FIN 标志，如果发现有 FIN 标志，这时才会进入 TIME_WAIT 状态。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/v2-5a2bb96c2249575aa4d33d69a79d466e_720w.jpg)

### 如何实现可靠UDP传输

首先，为了保证可靠性，我们需要在发送数据的时候添加**重传定时器**，来保证丢失的数据会被重传。重传的定时器可以定时回调发送重传的数据，也支持将接收到ACK的数据从定时器中取出。

现在有了重传定时器，那每次发送数据的时候，应该给定时器设置多长的超时时间呢？最简单的可以设定一个固定的重传时间，最合理的应该针对每条传输链路的不同设置每个连接的合理时间--rto。为了找到rto时间，我们需要获取到每个数据包发送确认时间，即rtt时间，即数据从发送到接收到ACK确认之间的时间间隔。我们参照TCP的实现策略，可以给每个消息记录一个发送时间，当接收到ACK确认时，将此时的时间减去记录的发送时间就获取到了rtt时间。但这样有一个问题，当发生数据重传时接收到ACK，无法判断这个ACK是对初次发送数据的确认还是对重传数据的确认，此时只能将发生重传数据测量到的rtt时间丢弃。所以又有第二种rtt计算策略，我们可以将发送时间记录在数据头中发送出去，接受端在发送确认ACK时，将这个时间戳抄下来顺着ACK返回，这样发送端接收到ACK确认时，就能准确的知道要确认数据的发送时间，由此来计算rtt时间。有了rtt时间，我们按照TCP的标准方法《CP/TPxian详解卷一, P465》，计算rto时间。

当接受到ACK确认时，我们需要将确认的数据从定时器中移除。

为了提高网络链路利用率，接收端不能每次接收到数据时都立即发送ACK确认，为什么呢？传输的数据量越小，控制头占比越高，而且网络中到处都是只携带一个ACK的包在飞，会造成路由器排队。这里可以接着参考TCP的实现策略。一种是延时ACK，即接收端接收到消息时定制一个pending time，当超时时将这段时间内所有要发送的ACK组合在一起发送，还有一种是捎带ACK，即pending time未到，但恰好也有数据要发送给对端，那么就将ACK捎带在这个数据包中一起发送出去。由于接收端ACK发送都不是瞬时的，所以在上文说到的RTT计算时也需要考虑引起的计算误差。

### **如何将socket设置成非阻塞的，非阻塞socket与阻塞的socket在收发数据上的区别**

1. 生成socket时设置
socket函数创建socket默认是阻塞的，也可以增加选项将socket设置为非阻塞的：

```
int s = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, IPPROTO_TCP);
```

2. 使用fcntl设置
    将socket设置为非阻塞的

  ```
  if ((nFlags = fcntl (nSock, F_GETFL, 0)) < 0)
   return 0;
  nFlags = nFlags | O_NONBLOCK;
  if (fcntl (nSock, F_SETFL, nFlags) < 0)
   return 0;
  ```

  将socket设置为阻塞的

  ```
  if ((nFlags = fcntl (nSock, F_GETFL, 0)) < 0)
   return 0;
  nFlags = nFlags & (~O_NONBLOCK);
  if (fcntl (nSock, F_SETFL, nFlags) < 0)
   return 0;
  ```

#### 非阻塞和阻塞在收发数据时有什么区别

**发送时的区别**

TCP发送(即send函数)

**send函数在阻塞模式下，会等待所有数据都被拷贝到发送缓冲区才会返回**，也就是说，阻塞模式下，send函数返回值必定是参数中发送长度的大小；

**send函数在非阻塞模式下，会立即返回，但是会尽可能的多拷贝数据到缓冲区，但不保证全部拷贝后返回**，因此非阻塞模式下，send函数返回值可能比参数中发送长度小，而如果缓冲区满了的话，就会立即返回；

UDP发送(即sendto函数)

即使在阻塞模式下，sendto也不会阻塞，因为U**DP并没有真正的发送缓冲区**，它所做的只是将应用缓冲区数据拷贝给下层协议栈，加上UDP头、IP头等，实际是不存在阻塞的，非阻塞模式也一样。

**接收时的区别**

TCP接收(即recv函数)

**在阻塞模式下， recv将会阻塞，直到缓冲区里有至少一个字节才返回，当没有数据到来时，recv会一直阻塞或者直到超时，不会返回**；

**在非阻塞模式下， recv不会阻塞，如果缓冲区里有任何一个字节，都会立即返回， 而如果没有数据，则返回错误**WSAEWOULDBLOCK;

UDP接收(即recvfrom函数)

在阻塞模式下，recvfrom将会阻塞，直到缓冲区里有一个完整UDP数据包才会返回；

在非阻塞模式下，recvfrom函数会立即返回， 如果缓冲区有一个完整数据包，就会返回数据报大小，如果没有数据，也是返回错误WSAEWOULDBLOCK;

### [什么是跨域? 出现原因及解决方法](https://segmentfault.com/a/1190000040485198)

**跨域是什么意思？**

**首先一个url是由：协议、域名、端口 三部分组成。（一般端口默认80）**
如：[https://blog.moonlet.cn:80](https://link.segmentfault.com/?enc=v%2BcabMAsIP4VVUlrpR6Vxw%3D%3D.wnNaaVMZLXj9Vt0fn46Iayot6m6uzhtYu0IqugZ33co%3D)

**当一个请求url的`协议`、`域名`、`端口`三者之间的`任意一个`与当前页面url`不同`即为`跨域`。**

![image-20220701212045278](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/image-20220701212045278.png)

**跨域产生原因？**

出于浏览器的`同源策略`限制。

> 同源策略（Same Orgin Policy）是一种约定，它是浏览器核心也最基本的安全功能，它**会阻止一个域的js脚本和另外一个域的内容进行交互**，如果缺少了同源策略，浏览器很容易受到XSS、CSFR等攻击。
>
> 所谓同源（即在同一个域）就是两个页面具有相同的协议（protocol）、主机（host）和端口号（port）。

**非同源会出现的限制**

```mipsasm
无法读取非同源网页的cookie、localstorage等
无法接触非同源网页的DOM和js对象
无法向非同源地址发送Ajax请求
```

**nginx反向代理解决跨域（前端常用）**

`正向代理：`

```css
a-->b访问不了，可以找个中间的服务器c, 先访问c再从c到b,类似曲线救国。
明确访问的目的地，但是用户不知道中间的代理服务器。（忽略中间服务器）
反向代理：a--> c <--b
a明确访问c代理服务器，但是不知道c的内容从哪里来，c反向从别的地方拿来数据。(忽略的是目标地址)
```

浏览器可以访问a,而服务器之间不存在跨域问题，浏览器先访问a的服务器c，让c服务器作为代理去访问b服务器,拿到之后再返回数据给a。

例如：

```awk
nginx是静态服务器，跨域请求放在api下面好管理http://www.baidu.com:80/api/user
可以在nginx下面的config下面的nginx.conf里面配置
从80端口进来的就拦截一下，代理到81端口

server{
        location /api {
                //拦截一下
                proxy_pass  http://www.baidu.com:81;
    }
}
```

**添加响应头解决跨域**

浏览器先询问b,b允许a访问

access-control-allow-origin

access-control-max-age

PHP端修改header：

```awk
header('Access-Control-Allow-Origin:*');//允许所有来源访问
header('Access-Control-Allow-Method:POST,GET');//允许访问的方式
```

**通过jsonp解决跨域（老方法）**

`实现原理：`通常为了减轻web服务器的负载，我们把js、css、图片等静态资源分离到另一台独立域名的服务器上，在html页面中再通过相应的标签从不同域名下加载静态资源，而被浏览器允许。

html中有的标签天然支持跨域，比如<script src="http://www.baidu.com"></script>但是只支持get请求。

**CORS解决跨域** **(`第三方模块`)**

它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。

CORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。

浏览器端：

目前，所有浏览器都支持该功能（IE10以下不行）。整个CORS通信过程，都是浏览器自动完成，不需要用户参与。

服务端：

CORS通信与AJAX没有任何差别，因此你不需要改变以前的业务逻辑。只不过，浏览器会在请求中携带一些头信息，我们需要以此判断是否运行其跨域，然后在响应头中加入一些信息即可。这一般通过过滤器完成即可。

> **优势：**
>
> 在服务端进行控制是否允许跨域，可自定义规则
> 支持各种请求方式
>
> **缺点：**
>
> 会产生额外的请求



### 四层、七层负载均衡的区别

[参考](https://cloud.tencent.com/developer/article/1082047)

负载均衡就是一种计算机网络技术，**用来在多个计算机（计算机集群）、网络连接、CPU、磁碟驱动器或其他资源中分配负载，以达到最佳化资源使用、最大化吞吐率、最小化响应时间、同时避免过载的目的**。那么，这种计算机技术的实现方式有多种。大致可以分为以下几种，其中最常用的是四层和七层负载均衡：

二层负载均衡

负载均衡服务器对外依然**提供一个VIP（虚IP），集群中不同的机器采用相同IP地址，但是机器的MAC地址不一样**。当负载均衡服务器接受到请求之后，**通过改写报文的目标MAC地址的方式将请求转发到目标机器实现负载均衡**。

三层负载均衡

和二层负载均衡类似，负载均衡服务器对外依然**提供一个VIP（虚IP），但是集群中不同的机器采用不同的IP地址**。当负载均衡服务器接受到请求之后，**根据不同的负载均衡算法，通过IP将请求转发至不同的真实服务器**。

四层负载均衡

四层负载均衡工作在OSI模型的传输层，由于在传输层，只有TCP/UDP协议，这两种协议中除了包含源IP、目标IP以外，还包含源端口号及目的端口号。四层负载均衡服务器**在接受到客户端请求后，以后通过修改数据包的地址信息（IP+端口号）将流量转发到应用服务器。**

七层负载均衡

七层负载均衡工作在OSI模型的应用层，应用层协议较多，常用http、radius、dns等。七层负载就可以基于这些协议来负载。这些应用层协议中会包含很多有意义的内容。比如同一个Web服务器的负载均衡，除了**根据IP加端口进行负载外，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡**。

![img](https://raw.githubusercontent.com/Simin-hub/Picture/master/img/xcdkyuw83o.jpeg)

对于一般的应用来说，有了Nginx就够了。Nginx可以用于七层负载均衡。但是对于一些大的网站，一般会采用DNS+四层负载+七层负载的方式进行多层次负载均衡。

![img](https://ask.qcloudimg.com/http-save/yehe-1497738/io18ifc0ev.jpeg)

三、四层、七层负载均衡对比

所谓**四层即运输层，就是基于 IP + 端口的负载均衡**； 七层即应用层，就是**基于 URL 等应用层信息的负载均衡**； 同理，还有**基于 MAC 地址的二层负载均衡**和**基于 IP 地址的三层负载均衡**。

#### 负载均衡算法

**静态负载均衡算法包括：轮询，比率，优先权**

**动态负载均衡算法包括: 最少连接数,最快响应速度，观察方法，预测法，动态性能分配，动态服务器补充，服务质量，服务类型，规则模式。**

**轮询**（Round Robin）：顺序循环将请求一次顺序循环地连接每个服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从顺序循环队列中拿出，不参加下一次的轮询，直到其恢复正常。

**比率**（Ratio）：给每个服务器分配一个加权值为比例，根椐这个比例，把用户的请求分配到每个服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配, 直到其恢复正常。

**优先权**（Priority）：给所有服务器分组,给每个组定义优先权，BIG-IP 用户的请求，分配给优先级最高的服务器组（在同一组内，采用轮询或比率算法，分配用户的请求）；当最高优先级中所有服务器出现故障，BIG-IP 才将请求送给次优先级的服务器组。这种方式，实际为用户提供一种热备份的方式。

**最少的连接方式**（Least Connection）：传递新的连接给那些进行最少连接处理的服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配, 直到其恢复正常。

**最快模式**（Fastest）：传递连接给那些响应最快的服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP 就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。

**观察模式**（Observed）：连接数目和响应时间以这两项的最佳平衡为依据为新的请求选择服务器。当其中某个服务器发生第二到第7 层的故障，BIG-IP就把其从服务器队列中拿出，不参加下一次的用户请求的分配，直到其恢复正常。

**预测模式**（Predictive）：BIG-IP利用收集到的服务器当前的性能指标，进行预测分析，选择一台服务器在下一个时间片内，其性能将达到最佳的服务器相应用户的请求。(被BIG-IP 进行检测)

**动态性能分配**(Dynamic Ratio-APM):BIG-IP 收集到的应用程序和应用服务器的各项性能参数，动态调整流量分配。

**动态服务器补充(**Dynamic Server Act.):当主服务器群中因故障导致数量减少时，动态地将备份服务器补充至主服务器群。

**服务质量**(QoS）:按不同的优先级对数据流进行分配。

**服务类型**(ToS): 按不同的服务类型（在Type of Field中标识）负载均衡对数据流进行分配。

**规则模式**：针对不同的数据流设置导向规则，用户可自行。



